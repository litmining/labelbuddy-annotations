{"text": "Abrams, Daniel Arthur and Padmanabhan, Aarthi and Chen, Tianwen and Odriozola, Paola and Baker, Amanda E and Kochalka, John and Phillips, Jennifer M and Menon, Vinod\neLife, 2019\n\n# Title\n\nImpaired voice processing in reward and salience circuits predicts social communication in children with autism\n\n# Keywords\n\nautism\nauditory\nvoice\nreward\nbrain\nmother\nHuman\n\n\n# Abstract\n \nEngaging with vocal sounds is critical for children\u2019s social-emotional learning, and children with autism spectrum disorder (ASD) often \u2018tune out\u2019 voices in their environment. Little is known regarding the neurobiological basis of voice processing and its link to social impairments in ASD. Here, we perform the first comprehensive brain network analysis of voice processing in children with ASD. We examined neural responses elicited by unfamiliar voices and mother\u2019s voice, a biologically salient voice for social learning, and identified a striking relationship between social communication abilities in children with ASD and activation in key structures of reward and salience processing regions. Functional connectivity between voice-selective and reward regions during voice processing predicted social communication in children with ASD and distinguished them from typically developing children. Results support the Social Motivation Theory of ASD by showing reward system deficits associated with the processing of a critical social stimulus, mother\u2019s voice, in children with ASD. \n\n Editorial note:   This article has been through an editorial process in which the authors decide how to respond to the issues raised during peer review. The Reviewing Editor's assessment is that minor issues remain unresolved ( ). \n \n\n# Body\n \n## Introduction \n  \nThe human voice is a critical social stimulus in children\u2019s environment, and engaging with vocal sounds is important for language ( ;  ) and social-emotional learning ( ) during typical development. However, children with autism spectrum disorder (ASD) are often not responsive to voices ( ;  ), and it has been hypothesized that voice processing deficits contribute to pronounced social communication difficulties in ASD ( ;  ;  ). A special case of voice processing impairments in children with ASD is a deficit in processing mother\u2019s voice ( ), a biologically\u00a0salient and implicitly rewarding sound for typically developing (TD) children ( ;  ), which is closely associated with cognitive ( ;  ) and social development ( ;  ). Compared to studies of visual face processing ( ;  ;  ;  ;  ), very little is known regarding the neurobiology of voice processing networks in children with ASD, which is fundamental to human communication. \n\nIt remains unknown why children with ASD often do not engage with the voices in their environment. Specifically, it is not known which aspects of voice processing are impaired in children with ASD. One possibility is that sensory deficits negatively affect voice processing and contribute to social communication deficits ( ;  ;  ;  ;  ;  ). A second possibility relates to the motivation to engage with socially\u00a0relevant stimuli ( ;  ;  ;  ). The social motivation theory of ASD posits that impairments in representing the reward value of human vocal sounds impedes individuals with ASD from engaging with these stimuli, and contributes to social interaction difficulties ( ;  ). While this is a prominent model for considering social communication function in ASD, there has been a dearth of compelling experimental evidence showing aberrant reward processing in response to clinically meaningful social stimuli ( ). \n\nAn important approach for testing theories of ASD is the use of human brain imaging methods and functional circuit analyses. Behavioral studies are limited in their ability to provide details regarding the neural mechanisms underlying distinct aspects of social information processing, and systems neuroscience analyses can uncover important aspects of social information processing that may be impaired in individuals with ASD. For example, the social motivation theory posits that individuals with ASD show reduced engagement and connectivity in the mesolimbic reward system, including the ventral tegmental area (VTA), nucleus accumbens (NAc), orbitofrontal cortex (OFC), and ventromedial prefrontal cortex (vmPFC), and structures of the salience and affective processing systems, instantiated in the anterior insula and amygdala, during social processing ( ). \n\nPrevious brain imaging research of voice processing in adults with ASD has supported the sensory deficit model by showing reduced regional activity in voice-selective superior temporal sulcus (STS) ( ;  ), a core region associated with structural analysis of the human voice ( ). However, several factors have precluded thorough tests of prominent ASD theories in the context of the neurobiology of voice processing. First, there have been few studies examining voice processing in ASD, particularly when compared to the extensive face processing literature ( ;  ;  ;  ;  ;  ). Second, previous studies have not employed biologically\u00a0salient voices (e.g. mother/caregiver), which are thought to be implicitly rewarding ( ), to probe brain circuit function in children with ASD. For example, a recent study in TD children showed that, compared to unfamiliar voices, mother\u2019s voice elicits activation within voice-selective, mesolimbic reward, affective, and salience, and face-processing brain regions, and connectivity between these regions predicts social communication abilities ( ). Third, previous studies of voice processing have focused on group differences in brain activity between individuals with ASD and matched controls but have not examined how individual variation in social communication abilities are associated with social brain circuit function in ASD. Finally, although autism has been conceptualized as a disorder of brain connectivity ( ;  ), previous brain imaging studies of human voice processing in ASD have focused on regional activation profiles in voice-selective cortex ( ;  ) and have not employed a brain networks perspective. Importantly, a brain networks approach goes beyond describing activation in circumscribed brain regions and accounts for the coordinated activity in distributed brain systems during social information processing, and would provide considerable insight into aberrancies in several critical brain systems in ASD ( ;  ;  ). For example, a previous resting state fMRI study investigated intrinsic connectivity of voice-selective cortex and showed that children with ASD have reduced connectivity between voice-selective STS and key structures of the mesolimbic reward system, anterior insula, and amygdala ( ). Moreover, the strength of intrinsic connectivity in this network predicted social communication abilities in children with ASD. While intrinsic network findings support the social motivation theory of ASD, a critical question remains: do results from intrinsic connectivity reflect an epiphenomenon, or is aberrant brain connectivity in voice and reward brain systems during the processing of biologically\u00a0salient and clinically\u00a0relevant voices a signature of social communication deficits in children with ASD? \n\nHere, we examine social information processing in children with ASD by probing brain circuit function and connectivity in response to human vocal sounds. We examined two aspects of voice processing: (1) unfamiliar voice processing compared to non-social auditory processing (i.e. environmental sounds) and (2) mother\u2019s voice compared to unfamiliar voice processing ( ). The rationale for this approach is that these two levels of social information processing may reflect distinct neural signatures in voice-selective, salience, and reward processing brain systems in children with ASD. A key aspect of our analysis was to investigate whether brain activity and connectivity in response to these vocal contrasts reflects individual differences in social communication abilities in children with ASD ( ). A second aspect of the analysis was to build on results from a previous intrinsic connectivity study of the voice processing network in children with ASD ( ) to examine whether stimulus-evoked connectivity patterns within this network during unfamiliar and mother\u2019s voice processing can reliably distinguish children with ASD from TD children and predict social communication abilities in children with ASD. \n   fMRI Experimental design, acoustical analysis, and behavioral results.  \n(  A  ) Randomized, rapid event-related design: During fMRI data collection, three auditory nonsense words, produced by three different speakers, were presented to the child participants at a comfortable listening level. The three speakers consisted of each child\u2019s mother and two control voices. Non-speech environmental sounds were also presented to enable baseline comparisons for the speech contrasts of interest. All auditory stimuli were 956 ms in duration and were equated for RMS amplitude. (  B  ) Acoustical analyses show that vocal samples produced by the participants\u2019 mothers were comparable between TD (yellow) and ASD groups (magenta) and were similar to the control samples (cyan) for individual acoustical measures (p>0.10 for all acoustical measures; see Appendix,   Acoustical analysis of mother\u2019s voice samples  ). (  C  ) All TD children and the majority of children with ASD were able to identify their mother\u2019s voice with high levels of accuracy, however five children with ASD performed below chance on this measure (see Appendix,   Identification of Mother\u2019s Voice  ). The horizontal line represents chance level for the mother\u2019s voice identification task. \n  \n\n## Results \n  \n### TD vs. ASD activation differences in response to unfamiliar voices \n  \nDirect group comparisons between TD children and children with ASD in response to unfamiliar female voices show that children with ASD have reduced activity in a relatively small set of brain regions confined to lateral temporal cortex ( ; see   for effect sizes and   for within-group results). Specifically, children with ASD show reduced activity in right hemisphere planum polare (PP), an area of auditory association cortex within the superior temporal gyrus. Within-group signal level analysis showed that TD children have greater activity for unfamiliar female voices, compared to environmental sounds, in this brain region (i.e. positive \u03b2s; see  ) while children with ASD show weaker activity for this same contrast (i.e. negative \u03b2s for unfamiliar voices compared to environmental sounds). No brain regions showed greater activity for unfamiliar female voices in the ASD, compared to the TD, group. \n   Brain activity difference in TD children compared to children with ASD in response to vocal stimuli.  \n(  A  ) Group comparisons indicate that TD children show greater activity compared to children with ASD in right-hemisphere auditory association cortex (planum polare (PP)) in response to the unfamiliar female voices\u00a0>\u00a0non-vocal environmental sound contrast. No regions showed greater activity in children with ASD compared to TD children for the unfamiliar female voice contrast. (  B  ) Group comparisons indicate that TD children show greater activity in several visual processing regions, including bilateral intercalcarine cortex, lingual gyrus, and fusiform cortex, as well as right-hemisphere posterior hippocampus and superior parietal regions, in response to the mother\u2019s voice\u00a0>\u00a0unfamiliar female voices contrast. No regions showed greater activity in children with ASD compared to TD children for the mother\u2019s voice contrast. \n  \n\n### TD vs. ASD activation differences in response to mother\u2019s voice \n  \nDirect group comparisons between brain responses measured from TD children and children with ASD in response to mother\u2019s voice relative to unfamiliar female voices revealed that children with ASD have reduced activity in several visual processing regions as well as key structures of the medial temporal lobe memory system ( ; see   for effect sizes and   for within-group results). Specifically, whole-brain analysis revealed that TD children had greater activation compared to children with ASD for mother\u2019s voice in bilateral intercalcarine cortex extending into lingual gyrus. Moreover, children with ASD showed reduced activity compared to TD children in a broad extent of fusiform gyrus bilaterally, including both left-hemisphere occipital regions of fusiform as well as temporal occipital regions in the right-hemisphere. Children with ASD also showed less activity for mother\u2019s voice in right-hemisphere posterior hippocampus, a critical region for learning and memory, as well as precuneus cortex of the default mode network. Signal level analysis shows that TD children have greater activity for mother\u2019s voice compared to unfamiliar female voices in these brain regions (i.e. positive \u03b2s; see  ) while children with ASD show weaker activity for mother\u2019s voice (i.e. negative \u03b2s). No brain structures showed greater activity for mother\u2019s voice in the ASD, compared to the TD, group. Moreover, fMRI activation profiles in children with ASD were not related to mother\u2019s voice identification accuracy (see Appendix,   fMRI activation and connectivity profiles in children with ASD are not related to mother\u2019s voice identification accuracy  ). \n\n\n### Brain activity and social\u00a0communication abilities \n  \nIdentifying sources of variance in key symptom domains represents an important question for autism research. We performed a whole-brain linear regression analysis using individual social\u00a0communication scores as a predictor of brain activation. We first examined this relation in the context of general vocal processing using the unfamiliar female voices minus environmental sounds contrast. Results from this analysis show a striking pattern: the strength of activity in a variety of brain systems serving auditory, reward, and salience detection is correlated with social\u00a0communication\u00a0abilities in children with ASD ( ; see   for effect sizes). Specifically, this pattern was apparent in auditory association cortex of the superior temporal plane, including the PP, but also in the nucleus accumbens of the reward pathway, and anterior insula of the salience network. Scatterplots show that brain activity and social\u00a0communication abilities vary across a range of values and greater social function, reflected by lower social\u00a0communication scores, is associated with greater brain activity in these auditory, reward, and salience processing regions. Support vector regression (SVR) analysis ( ;  ) showed that the strength of activity in these regions was a reliable predictor of social\u00a0communication function in these children (  R  \u00a0\u2265\u00a00.49; p\u00a0\u2264\u00a00.011 for all regions). \n   Activity in response to vocal stimuli and social\u00a0communication abilities in children with ASD.  \n(  A  ) In children with ASD, the whole-brain covariate map shows that social\u00a0communication scores are correlated with activity strength during unfamiliar female voice processing in auditory association cortex, the NAc of the reward system, and AI of the salience network. Scatterplots show the distributions and covariation of activity strength in response to unfamiliar female voices and standardized scores of social\u00a0communication abilities in these children. Greater social\u00a0communication abilities, reflected by smaller social\u00a0communication scores, are associated with greater brain activity in these regions. (  B  ) The whole-brain covariate map shows that social\u00a0communication scores are correlated with activity strength during mother\u2019s voice processing in primary auditory and association cortex, voice-selective STS, vmPFC of the reward system, AI and rACC of the salience network, and SMA. \n  \nWe next examined the question of heterogeneity in the context of mother\u2019s voice processing, and results show a similar pattern: children with ASD with greater social\u00a0communication abilities showed greater activation for mother\u2019s voice in a wide extent of primary auditory, auditory association, and voice-selective cortex as well as mesolimbic reward, salience detection, and motor regions ( ). Specifically, this brain-behavior relationship was evident in auditory regions of superior temporal cortex, including medial aspects of bilateral Heschl\u2019s gyrus, which contains primary auditory cortex, right-hemisphere PP of the superior temporal plane, as well as bilateral voice-selective mSTS. This relationship was also observed in regions of the salience network, including dorsal aspects of AI bilaterally and right-hemisphere rostral ACC (rACC), as well as vmPFC of the reward network. SVR results indicated that the strength of activity in these particular brain regions during mother\u2019s voice processing was a reliable predictor of social\u00a0communication function in these children (  R  \u00a0\u2265\u00a00.50; p\u00a0\u2264\u00a00.009 for all regions). \n\n\n### Connectivity patterns predict group membership \n  \nFunctional connectivity was examined using a generalized psychophysiological interaction (gPPI) model within an extended voice processing brain network defined   a priori   from intrinsic connectivity results described in a previous study in children with ASD ( ) ( ). This approach allows us to systematically build upon our previous findings while preempting task and sample-related biases in region-of-interest (ROI) selection. This extended voice processing network included ROIs in voice-selective STS, structures of the reward and salience networks, amygdala, hippocampus, and fusiform cortex (see   for details of this network). There were no univariate group differences in individual links during either unfamiliar voice ( ) or mother\u2019s voice processing ( ) after correcting for multiple comparisons (FDR,   q  \u00a0<\u00a00.05). Support vector classification (SVC) results showed that multivariate connectivity patterns during unfamiliar voice processing were unable to predict group membership above chance (SVC Accuracy\u00a0=\u00a051.6%, p\u00a0=\u00a00.31); however, multivariate connectivity patterns during mother\u2019s voice processing accurately predicted TD vs. ASD group membership (SVC Accuracy\u00a0=\u00a070.4%, p\u00a0=\u00a00.001). We performed a confirmatory analysis using a different logistic regression classifier (GLMnet, generalized linear model via penalized maximum likelihood) and results were similar to the SVC results (unfamiliar voice processing: 54.8%, p\u00a0=\u00a00.78 (not significant); mother\u2019s voice: 80.9%, p\u00a0=\u00a00.010). These SVC results held even after accounting for group differences in mother\u2019s voice identification accuracy (see Appendix,   fMRI activation and connectivity profiles in children with ASD are not related to mother\u2019s voice identification accuracy  ). Results show that patterns of brain connectivity during biologically-salient voice processing, but not unfamiliar voice processing, can distinguish children with ASD from TD children. \n   Functional connectivity in the extended voice-selective network and TD vs.\u00a0ASD group membership.  \n(  A  ) The brain network used in connectivity analyses, which includes voice-selective, reward, salience, affective, and face-processing regions, was defined   a priori   from intrinsic connectivity results described in a previous study of children with ASD ( ). (  B-C  ) Group difference connectivity matrices shows differences in connectivity between TD children and children with ASD for all node combinations during (  B  ) unfamiliar female voice processing and (  C  ) mother\u2019s voice processing. Results from multivariate connectivity analysis show that connectivity patterns during mother\u2019s voice processing can accurately predict TD vs. ASD group membership; however, connectivity patterns during unfamiliar female voice processing are unable to accurately predict group membership. \n  \n\n### Connectivity patterns predict social\u00a0communication abilities \n  \nWe next examined the relation between connectivity beta weights in each cell of the connectivity matrix and social\u00a0communication scores in children with ASD. There were no significant univariate correlations between the strength of brain connectivity during either unfamiliar ( ) or mother\u2019s voice processing ( ) and social\u00a0communication abilities. We then performed support vector regression (SVR) to examine whether multivariate patterns of connectivity during voice processing accurately predict social\u00a0communication abilities in these children. Given that brain activation results showed that both unfamiliar ( ) and mother\u2019s voice processing ( ) explained variance in social\u00a0communication abilities, we used a combination of connectivity features from both vocal conditions for this analysis. SVR results showed that multivariate connectivity patterns during unfamiliar and mother\u2019s voice processing accurately predict social\u00a0communication scores in children with ASD (  R  \u00a0=\u00a00.42, p\u00a0=\u00a00.015). We performed a confirmatory analysis using GLMnet and results were similar to the SVR results (social\u00a0communication prediction:   R  \u00a0=\u00a00.76, p\u00a0<\u00a00.001). Furthermore, when children with below chance accuracy on the mother\u2019s voice identification accuracy were removed from the analysis, this result held and connectivity patterns were still predictive of social\u00a0communication scores (see Appendix,   fMRI activation and connectivity profiles in children with ASD are not related to mother\u2019s voice identification accuracy  ). \n   Functional connectivity in the extended voice-selective network and social\u00a0communication abilities in children with ASD.  \n(  A  ) The brain network used in connectivity analyses, which includes voice-selective, reward, salience, affective, and face-processing regions, was defined   a priori   from intrinsic connectivity results described in a previous study of children with ASD\u00a0( ). (  B-C  ) Correlation matrices show Pearson\u2019s correlations between social\u00a0communication scores and connectivity for each pairwise node combination in response to (  B  ) unfamiliar female voice processing and (  C  ) mother\u2019s voice processing in children with ASD. Results from multivariate connectivity analysis show that using a combination of connectivity features from both unfamiliar female and mother\u2019s voice processing can accurately predict social\u00a0communication scores in children with ASD. \n  \n\n\n## Discussion \n  \nIt is unknown why children with ASD often \u2018tune out\u2019 from the voices of social partners in their environment ( ), including personal relations such as family members and caregivers ( ). Here, we identify a striking relationship between individuals\u2019 social communication abilities and the strength of activation in reward and salience processing brain regions, notably NAc and AI, during human voice processing in children with ASD. Multivariate connectivity patterns within an extended voice processing network distinguished children with ASD from their TD peers and predicted social communication abilities in children with ASD. These findings suggest that dysfunction of the brain\u2019s reward system provides a stable brain signature of ASD that contributes to aberrant processing of salient vocal information ( ). \n\n### Regional and network features associated with voice processing predict individual differences in social function in children with ASD \n  \nIndividuals with ASD present with a complex behavioral profile, which includes an array of sensory ( ), cognitive ( ), and affective processing differences ( ) compared to TD individuals. Consensus on the specific factors that most contribute to pronounced social communication difficulties in this population has remained elusive. Our findings showed that both regional and network features associated with voice processing, encompassing voice-selective cortex in the STS and extended voice-processing network that includes auditory, reward, and salience regions, predicted social function in children with ASD. The diversity of this network reflects the complexities of social communication itself, which involves the ability to integrate sensory, affective, mnemonic, and reward information. Importantly, our results unify several important characteristics of ASD in the extant literature, including regional functional aberrancies within specific brain systems and their association with social abilities ( ;  ;  ;  ;  ), network level dysfunction ( ;  ;  ;  ), and heterogeneity of social communication abilities ( ;  ). We suggest that social communication function \u2013 human\u2019s ability to interact with and relate to others \u2013 is a unifying factor for explaining regional activation profiles and large-scale connectivity patterns linking key elements of the social brain. \n\n\n### A voice-related brain network approach for understanding social information processing in autism \n  \nBrain network analyses represent an important approach for understanding brain function in autism ( ;  ;  ;  ), and psychopathology more broadly ( ). These methods, which are typically applied to resting-state brain imaging data, have yielded considerable knowledge regarding network connectivity patterns in ASD and their links to behavior ( ). A central assumption of this approach is that aberrant task-evoked circuit function is associated with clinical symptoms and behavior; however, empirical studies examining these associations have been lacking from the ASD literature. Our study addresses this gap by probing task-evoked function within a network defined   a priori   from a previous study of intrinsic connectivity of voice-selective networks in an independent group of children with ASD. We show that voice-related network function during the processing of a clinically and biologically meaningful social stimulus predicts both ASD group membership as well as social communication abilities in these children. Our findings bridge a critical gap between the integrity of the intrinsic architecture of the voice-processing network in children with ASD and network signatures of aberrant social information processing in these individuals. \n\n\n### Biologically-salient vocal stimuli for investigating the social brain in autism spectrum disorders \n  \nOur results demonstrate that brief samples of a biologically\u00a0salient voice, mother\u2019s voice, elicit a distinct neural signature in children with ASD. Our findings have important implications for the development of social skills in children with ASD. Specifically, typically developing children prefer biologically\u00a0salient voices such as a mother\u2019s voice which provide critical cues for social ( ) and language learning ( ). In contrast, both anecdotal ( ) and experimental accounts ( ) indicate that children with ASD do not show a preference for these sounds. We suggest that aberrant function within the extended voice processing network may underlie insensitivity to biologically\u00a0salient voices in children with ASD, which may subsequently affect key developmental processes associated with social and pragmatic language learning. \n\n\n### The social motivation theory and reward circuitry in children with ASD \n  \nThe social motivation theory of ASD provides an important framework for considering pervasive social deficits in affected individuals ( ;  ). The theory posits that social skills emerge in young children from an initial attraction to social cues in their environment. For example, TD infants are highly attentive to speech despite having no understanding of words\u2019 meanings, and this early attraction to vocal cues may be a critical step in a developmental process that includes speech sound discrimination, mimicry, and, ultimately, language learning and verbal communication ( ). In contrast, children with ASD often do not engage with the speech in their environment ( ), and a central hypothesis of the social motivation theory is that weak reward attribution to vocal sounds during early childhood disrupts important developmental processes supporting social communication. \n\nOur findings provide support for the social motivation theory by showing a link between social communication abilities in children with ASD and the strength of activity in reward and salience detection systems in response to unfamiliar and mother\u2019s voice. Specifically, children with ASD who have the most severe social communication deficits have the weakest responses in reward and salience detection brain regions to both of these vocal sources. Moreover, network connectivity of an extended voice-selective network, which includes nodes of the salience and reward networks, distinguished ASD and TD children and predicted social communication abilities in children with ASD. These results are the first to show that aberrant function of reward circuitry during voice processing is a distinguishing feature of childhood autism, and may limit the ability of children with ASD to experience vocal sounds as rewarding or salient. Our findings add to a growing literature suggesting that functional connectivity between voice-selective STS and reward and salience processing regions is an important predictor of social skill development in children ( ;  ). \n\nOur results highlighting the role of reward and salience in the context of voice processing have implications for clinical treatment of social communication deficits in children with ASD. An important direction for treatment of children with ASD involves the use of teaching strategies ( ;  ) that focus on motivating children to engage in verbal interactions to improve social communication skills ( ;  ). Findings suggest that clinical efforts to increase the reward value of vocal interactions in children with ASD may be key to remediating social communication deficits in these individuals. Furthermore, neural activity and connectivity measures may represent a quantitative metric for assessing response to clinical treatments focused on verbal interactions. \n\n\n### Limitations \n  \nThere are limitations to the current work that warrant consideration. First, the sample size is relatively modest compared to recent task-based brain imaging studies of neurotypical adult populations and resting-state fMRI or structural MRI studies in individuals with ASD, however these types of studies do not face the same data collection challenges as task-based studies in clinical pediatric populations ( ). Importantly, resting-state and structural imaging studies are unable to address specific questions related to social information processing in ASD, such as biologically\u00a0salient voice processing, which are critical for understanding the brain bases of social dysfunction in affected children. Indeed, our sample size is larger than, or comparable to, the majority of task-fMRI studies in children with ASD published since 2017, and have more stringent individual-level sampling compared to these studies. This is an important consideration given that the replicability of task fMRI data is not solely contingent on a large sample size but also depends on the amount of individual-level sampling. A recent report examining this question showed that modest sample sizes, comparable to those described in our submitted manuscript, yield highly replicable results with only four runs of task data with a similar number of trials per run as our study ( ). In comparison, we required that each child participant had at least seven functional imaging runs of our event-related fMRI task that met our strict head movement criteria. A final limitation of this work is that, consistent with the vast majority of brain imaging studies in children with ASD, we were unable to include lower functioning children with ASD since the scanner environment is ill-suited for these children ( ). Further studies with larger samples are needed both to capture the full range of heterogeneity of ASD and to ensure the broader generalizability of the findings reported here. \n\n\n### Conclusion \n  \nWe identified neural features underlying voice processing impairments in children with ASD, which are thought to contribute to pervasive social communication difficulties in affected individuals. Results show that activity profiles and network connectivity patterns within voice-selective and reward regions, measured during unfamiliar and mother\u2019s voice processing, distinguish children with ASD from TD peers and predict their social communication abilities. These findings are consistent with the social motivation theory of ASD by linking human voice processing to dysfunction in the brain\u2019s reward centers, and have implications for the treatment of social communication deficits in children with ASD. For example, parent training has emerged as a powerful and cost-effective approach for increasing treatment intensity ( ): treatment delivery in the child\u2019s natural environment promotes functional communication ( ), generalization ( ), and maintenance of skills over time ( ;  ). Findings from the current study, which demonstrate a link between social communication function and neural processing of mother\u2019s voice, support the importance of parent training by suggesting that a child\u2019s ability to focus on, and direct neural resources to, these critical communication partners may be a key to improving social function in affected children. \n\n\n\n## Materials and methods \n  \n### Participants \n  \nThe Stanford University Institutional Review Board approved the study protocol. Parental consent and the child\u2019s assent were obtained for all evaluation procedures, and children were paid for their participation in the study. \n\nA total of 57 children were recruited from around the San Francisco Bay Area for this study. All children were required to be right-handed and have a full-scale IQ\u00a0>\u00a080, as measured by the Wechsler Abbreviated Scale of Intelligence (WASI)\u00a0( ). 28 children met ASD criteria based on an algorithm ( ) that combines information from both the module 3 of the ADOS-2 (47) and the ADI\u2013Revised ( ). Specifically, these children showed mild to more severe social communication deficits, particularly in the areas of social-emotional reciprocity and verbal and non-verbal communication, and repetitive and restricted behaviors and interests ( ). Five children with ASD were excluded because of excessive movement in the fMRI scanner, one child was excluded because of a metal retainer interfering with their brain images, and one child was excluded because their biological mother was not available to do a voice recording. Importantly, children in the ASD sample are considered \u2018high-functioning\u2019 and had fluent language skills and above-average reading skills ( ). Nevertheless, these children are generally characterized as having communication impairments, especially in the area of reciprocal conversation. \n   Demographic and IQ measures      \nTD children and had no history of neurological, psychiatric, or learning disorders, personal and family history (first degree) of developmental cognitive disorders and heritable neuropsychiatric disorders, evidence of significant difficulty during pregnancy, labor, delivery, or immediate neonatal period, or abnormal developmental milestones as determined by neurologic history and examination. Three TD children were excluded because of excessive movement in the fMRI scanner, one was excluded because of scores in the \u2018severe\u2019 range on standardized measures of social function, and four female TD children were excluded to provide a similar ratio of males to females relative to the ASD participants. The final TD and ASD groups that were included in the analysis consisted of 21 children in each group who were matched for full-scale IQ, age, sex, and head motion during the fMRI scan ( ). All participants are the biological offspring of the mothers whose voices were used in this study (i.e. none of our participants were adopted, and therefore none of the mother\u2019s voices are from an adoptive mother), and all participants were raised in homes that included their mothers. Participants\u2019 neuropsychological characteristics are provided in  . \n\n\n### Data acquisition parameters \n  \nAll fMRI data were acquired at the Richard M. Lucas Center for Imaging at Stanford University. Functional images were acquired on a 3 T Signa scanner (General Electric) using a custom-built head coil. Participants were instructed to stay as still as possible during scanning, and head movement was further minimized by placing memory-foam pillows around the participant\u2019s head. A total of 29 axial slices (4.0 mm thickness, 0.5 mm skip) parallel to the anterior/posterior commissure line and covering the whole brain were imaged by using a T2*-weighted gradient-echo spiral in-out pulse sequence ( ) with the following parameters: repetition time\u00a0=\u00a03576 ms; echo time\u00a0=\u00a030 ms; flip angle\u00a0=\u00a080\u00b0; one interleave. The 3576 msec TR can be calculated as the sum of: (1) the stimulus duration of 956 msec; (2) a 300 ms silent interval buffering the beginning and end of each stimulus presentation (600 ms total of silent buffers) to avoid backward and forward masking effects; (3) the 2000 ms volume acquisition time; and (4) an additional 20 ms silent interval, which helped the stimulus computer maintain precise and accurate timing during stimulus presentation. The field of view was 20 cm, and the matrix size was 64\u00a0\u00d7\u00a064, providing an in- plane spatial resolution of 3.125 mm. Reduction of blurring and signal loss arising from field inhomogeneities was accomplished by the use of an automated high-order shimming method before data acquisition. \n\n\n### fMRI Task \n  \nAuditory stimuli were presented in 10 separate runs, each lasting 4 min. One run consisted of 56 trials of mother\u2019s voice, unfamiliar female voices, environmental sounds and catch trials, which were pseudo-randomly ordered within each run. Stimulus presentation order was the same for each subject. Each stimulus lasted 956 msec in duration. Prior to each run, child participants were instructed to play the \u2018kitty cat game\u2019 during the fMRI scan. While laying down in the scanner, children were first shown a brief video of a cat and were told that the goal of the cat game was to listen to a variety of sounds, including \u2018voices that may be familiar,\u2019 and to push a button on a button box only when they heard kitty cat meows (catch trials). The function of the \u2018catch trials\u2019 was to keep the children alert and engaged during stimulus presentation. During each run, four or five exemplars of each stimulus type (i.e. nonsense words samples of mother\u2019s and unfamiliar female voices, environmental sounds), as well as three catch trials, were presented. At the end of each run, the children were shown another engaging video of a cat. Although the button box failed to register responses during data collection in four children with ASD and nine TD children, data analysis of the catch trails for 17 children with ASD and 12 TD children showed similar catch trial accuracies between TD (accuracy\u00a0=\u00a091%) and ASD groups (accuracy\u00a0=\u00a089%; two-sample   t  -test results:   t  (2) = 0.35, p\u00a0=\u00a00.73). Across the ten runs, a total of 48 exemplars of each stimulus condition were presented to each subject (i.e. 144 total exemplars produced by each of the three vocal sources, including the child\u2019s mother, unfamiliar female voice #1, and unfamiliar female voice #2). Vocal\u00a0stimuli were presented to participants in the scanner using Eprime V1.0 (Psychological Software Tools, 2002). Participants wore custom-built headphones designed to reduce the background scanner noise to \u223c70 dBA ( ;  ). Headphone sound levels were calibrated prior to each data collection session, and all stimuli were presented at a sound level of 75 dBA. Participants were scanned using an event-related design. Auditory stimuli were presented during silent intervals between volume acquisitions to eliminate the effects of scanner noise on auditory discrimination. One stimulus was presented every 3576 ms, and the silent period duration was not jittered. The total silent period between stimulus presentations was 2620 ms, and consisted of a 300 ms silent period, 2000 ms for a volume acquisition, another 300 ms of silence, and a 20 ms silent interval that helped the stimulus computer maintain precise and accurate timing during stimulus presentation. \n\n\n### Functional MRI preprocessing \n  \nfMRI data collected in each of the 10 functional runs were subject to the following preprocessing procedures. The first five volumes were not analyzed to allow for signal equilibration. A linear shim correction was applied separately for each slice during reconstruction by using a magnetic field map acquired automatically by the pulse sequence at the beginning of the scan. Translational movement in millimeters (x, y, z) was calculated based on the SPM8 parameters for motion correction of the functional images in each subject. To correct for deviant volumes resulting from spikes in movement, we used a de-spiking procedure. Volumes with movement exceeding 0.5\u00a0voxels (1.562 mm) or spikes in global signal exceeding 5% were interpolated using adjacent scans. The majority of volumes repaired occurred in isolation. After the interpolation procedure, images were spatially normalized to standard Montreal Neurological Institute (MNI) space, resampled to 2 mm isotropic voxels, and smoothed with a 6 mm full-width at half maximum Gaussian kernel. \n\n\n### Movement criteria for inclusion in fMRI analysis \n  \nFor inclusion in the fMRI analysis, we required that each functional run had a maximum scan-to-scan movement of\u00a0<\u00a06 mm and no more than 15% of volumes were corrected in the de-spiking procedure. Moreover, we required that all individual subject data included in the analysis consisted of at least seven functional runs that met our criteria for scan-to-scan movement and percentage of volumes corrected; subjects who had fewer than seven functional runs that met our movement criteria were not included in the data analysis. All 42 participants included in the analysis had at least seven functional runs that met our movement criteria, and the total number of runs included for TD and ASD groups were similar (TD\u00a0=\u00a0192 runs; ASD\u00a0=\u00a0188 runs). \n\n\n### Voxel-wise analysis of fMRI activation \n  \nThe goal of this analysis was to identify brain regions that showed differential activity levels in response to mother\u2019s voice, unfamiliar voices, and environmental sounds. Brain activation related to each vocal\u00a0task condition was first modeled at the individual subject level using boxcar functions with a canonical hemodynamic response function and a temporal derivative to account for voxel-wise latency differences in hemodynamic response. Environmental sounds were not modeled to avoid collinearity, and this stimulus served as the baseline condition. Low-frequency drifts at each voxel were removed using a high-pass filter (0.5 cycles/min) and serial correlations were accounted for by modeling the fMRI time series as a first-degree autoregressive process ( ). We performed whole-brain ANOVAs to separately investigate unfamiliar and mother\u2019s voice processing: (1) the unfamiliar voice analysis used the factors group (TD and ASD) and auditory condition (unfamiliar voices and environmental sounds) and (2) the mother\u2019s voice analysis used the factors group (TD and ASD) and voice condition (mother's\u00a0voice and unfamiliar voices). These ANOVAs were designed to test specific hypotheses described in the Introduction. Group-level activation was determined using individual subject contrast images and a second-level analysis of variance. The main contrasts of interest were [mother\u2019s voice \u2013 unfamiliar female voices] and [unfamiliar female voices \u2013 environmental\u00a0sounds]. Significant clusters of activation were determined using a voxel-wise statistical height threshold of p\u00a0<\u00a00.005, with family-wise error corrections for multiple spatial comparisons (p\u00a0<\u00a00.05; 67 voxels) determined using Monte Carlo simulations ( ;  ) using a custom Matlab script (see Source Code). To examine GLM results in the inferior colliculus and NAc, small subcortical brain structures, we used a small volume correction at p<0.05 with a voxel-wise statistical height threshold of p\u00a0<\u00a00.005. To determine the robustness of our findings, group comparisons were also performed using more stringent height and extent thresholds ( \u2013 ). To provide estimates of effect sizes within specific regions displayed in  ,   t  -scores from the whole-brain TD vs. ASD group GLM analysis were averaged within each significant cluster. Effect sizes were then computed as Cohen\u2019s   d   according to   below, where   t   is the mean   t  -score within a cluster and   N   is the sample size: \n\nTo define specific cortical regions, we used the Harvard\u2013Oxford probabilistic structural atlas ( ) with a probability threshold of 25%. \n\n\n### Brain-behavior analysis \n  \nRegression analysis was used to examine the relationship between brain responses to unfamiliar and mother\u2019s voice and social\u00a0communication abilities in children with ASD. Social\u00a0communication function was assessed using the Social Affect subscore of the ADOS-2 (47). Brain-behavior relationships were examined using analysis of activation levels. A whole-brain, voxel-wise regression analysis was performed in which the relation between fMRI activity and social\u00a0communication scores was examined using images contrasting [unfamiliar female voices\u00a0>\u00a0environmental sounds] and [mother\u2019s vs. unfamiliar female voices]. Significant clusters were determined using a voxel-wise statistical height threshold of p\u00a0<\u00a00.005, with family-wise error corrections for multiple spatial comparisons (p\u00a0<\u00a00.05; 67 voxels) determined using Monte Carlo simulations ( ;  ). To determine the robustness of our findings, brain-behavior relations were also examined using more stringent height and extent thresholds ( \u2013 ). To provide estimates of effect sizes within regions displayed in  ,   t  -scores from the whole-brain ASD Social Communication covariate analysis were averaged within each cluster identified in the GLM analysis. Effect sizes were then computed as Cohen\u2019s   f   according to   below, where   t   is the mean   t  -score within a cluster and   N   is the sample size: \n\n\n### Brain activity levels and prediction of social function \n  \nTo examine the robustness and reliability of brain activity levels for predicting social\u00a0communication scores, we used support vector regression (SVR) to perform a confirmatory cross-validation analysis that employs a machine-learning approach with balanced fourfold cross-validation (CV) combined with linear regression ( ). In this analysis, we extracted individual subject activation beta values taken from the [unfamiliar female voices\u00a0>\u00a0environmental sounds] and [mother\u2019s voice\u00a0>\u00a0unfamiliar female voices] GLM contrasts. For the [unfamiliar female voices\u00a0>\u00a0environmental sounds] GLM contrast, GLM betas were extracted from right-hemisphere PP and AI as well as left-hemisphere NAc. For the [mother\u2019s voice\u00a0>\u00a0unfamiliar female voices] GLM contrast, GLM betas were extracted from left-hemisphere HG, PP, and AI as well as right-hemisphere mSTS, vmPFC, rACC, and SMA. These values were entered as independent variables in a linear regression analysis with ADOS-2 Social Affect subscores as the dependent variable. r  , a measure of how well the independent variable predicts the dependent variable, was first estimated using a balanced fourfold CV procedure. Data were divided into four folds so that the distributions of dependent and independent variables were balanced across folds. Data were randomly assigned to four folds and the independent and dependent variables tested in one-way ANOVAs, repeating as necessary until both ANOVAs were insignificant in order to guarantee balance across the folds. A linear regression model was built using three folds leaving out the fourth, and this model was then used to predict the data in the left-out fold. This procedure was repeated four times to compute a final r  representing the correlation between the data predicted by the regression model and the observed data. Finally, the statistical significance of the model was assessed using a nonparametric testing approach. The empirical null distribution of r   was estimated by generating 1000 surrogate datasets under the null hypothesis that there was no association between changes in ADOS social\u00a0communication subscore and brain activity levels. \n\n\n### Functional connectivity analysis \n  \nWe examined functional connectivity between ROIs using the generalized psychophysiological interaction (gPPI) model ( ), with the goal of identifying connectivity between ROIs in response to each task condition as well differences between task conditions (mother\u2019s voice, other voice, environmental sounds). We used the SPM gPPI toolbox for this analysis. gPPI is more sensitive than standard PPI to task context-dependent differences in connectivity ( ). Unlike dynamical causal modeling (DCM), gPPI does not use a temporal precedence model (x(t\u00a0+\u00a01)~x(t)) and therefore makes no claims of causality. The gPPI model is summarized in   below: \n\nBriefly, in each participant, the regional timeseries from a seed ROI was deconvolved to uncover quasi-neuronal activity and then multiplied with the task design waveform for each task condition to form condition-specific gPPI interaction terms. These interaction terms are then convolved with the hemodynamic response function (HRF) to form gPPI regressors for each task condition. The final step is a standard general linear model predicting target ROI response after regressing out any direct effects of the activity in the seed ROI. In the equation above,  \u00a0and  \u00a0are the time series in the two brain regions, and  \u00a0contains three columns corresponding to each task condition.\u00a0The goal of this analysis was to examine connectivity patterns within an extended voice-selective network identified in a previous study of children with ASD ( ). This study showed weak intrinsic connectivity between bilateral voice-selective STS and regions implicated in reward, salience, memory, and affective processing. The rationale for the use of an   a priori   network is it is an established method of network identification that preempts task and sample-related biases in region-of-interest (ROI) selection. This approach therefore allows for a more generalizable set of results compared to a network defined based on nodes identified using the current sample of children and task conditions. The network used in all connectivity analyses consisted of 16 regions. All cortical ROIs were constructed as 5 mm spheres centered on the coordinates listed in  , while subcortical ROIs were constructed as 2 mm spheres. \n\n\n### Functional connectivity, group classification, and prediction of social function \n  \nSupport vector classification (SVC) and regression (SVR) were used to examine whether patterns of connectivity within the extended voice processing network could predict TD vs. ASD group membership and social\u00a0communication abilities in children with ASD, respectively. First, to examine TD vs. ASD group membership, a linear support vector machine algorithm (C\u00a0=\u00a01) from the open-source library LIBSVM (  http://www.csie.ntu.edu.tw/~cjlin/libsvm/  ) was used to build classifiers to distinguish children with ASD from TD children during unfamiliar voice processing. Individual subject connectivity matrices (16 \u00d7 16 ROIs) taken from the [unfamiliar female voices\u00a0>\u00a0environmental sounds] gPPI contrast were used as features to train classifiers in each dataset. Classifier performance was evaluated using a four-fold cross-validation procedure. Specifically, a dataset was randomly partitioned into four folds. Three folds of data (training set) were used to train a classifier, which was then applied to the remaining fold (test set) to predict whether each sample in the test set should be classified as ASD or TD. This procedure was repeated four times with each of the four folds used exactly once as a test set. The average classification accuracy across the four folds (cross-validation accuracy) was used to evaluate the classifier\u2019s performance. To further account for variation due to random data partition, we repeated the same cross-validation procedure 100 times with different random data partitions. Finally, the mean cross-validation accuracies from 100 iterations was reported, and its statistical significance was evaluated using permutation testing (1000 times) by randomly permuting subjects\u2019 labels and repeating the same above procedures. The same SVC methods were used to examine whether connectivity features during mothers voice processing could accurately predict TD vs. ASD group membership, however in this analysis individual subject connectivity matrices (16 \u00d7 16 ROIs) taken from the [mother\u2019s voice\u00a0>\u00a0unfamiliar female voices] gPPI contrast were used as features to train\u00a0the classifier. \n\nFinally, SVR was used to examine whether connectivity patterns during unfamiliar female and mother\u2019s voice processing could predict social\u00a0communication scores in children with ASD. SVR methods are the same as those described in   Brain Activity Levels and Prediction of Social Function  ; however, features in this analysis include multivariate connectivity patterns across the extended voice-selective network (16 ROIs). Given that brain activation results showed that both unfamiliar ( ) and mother\u2019s voice processing ( ) explained variance in social\u00a0communication abilities, we used a combination of connectivity features from both vocal conditions for this analysis. Specifically, connectivity features from both the [unfamiliar female voices\u00a0>\u00a0environmental sounds] and [mother\u2019s voice\u00a0>\u00a0unfamiliar female voices] gPPI contrasts were entered as independent variables in a linear regression analysis with ADOS-2 Social Affect subscores as the dependent variable. \n\nAs a confirmatory analysis, and to examine the robustness of SVC and SVR results, we used GLMnet (  http://www-stat.stanford.edu/~tibs/glmnet-matlab  ), a logistic regression classifier that includes regularization and exploits sparsity in the input matrix, on the same 16 \u00d7 16 connectivity matrices described for the SVC and SVR analyses above. \n\n\n### Stimulus design considerations \n  \nPrevious studies investigating the processing ( ;  ) and neural bases ( ;  ) of mother\u2019s voice processing have used a design in which one mother\u2019s voice serves as a control voice for another participant. However, due to an important practical limitation, the current study used a design in which all participants heard the same two control voices. While we make every effort to recruit children from a variety of communities in the San Francisco Bay Area, some level of recruitment occurs through contact with specific schools, and in other instances our participants refer their friends to our lab for inclusion in our studies. In these cases, it is a reasonable possibility that our participants may have known other mothers involved in the study, and therefore may be familiar with these mothers\u2019 voices, which would limit the control we were seeking in our control voices. Importantly, HIPPA guidelines are explicit that participant information is confidential, and therefore there would be no way to probe whether a child knows any of the other families involved in the study. Given this practical consideration, we concluded that it would be best to use the same two control voices, which we knew were unfamiliar to the participants, for all participants\u2019 data collection. \n\n\n### Stimulus recording \n  \nRecordings of each mother were made individually while their child was undergoing neuropsychological testing. Mother\u2019s voice stimuli and control voices were recorded in a quiet conference room using a Shure PG27-USB condenser microphone connected to a MacBook Air laptop. The audio signal was digitized at a sampling rate of 44.1 kHz and A/D converted with 16-bit resolution. Mothers were positioned in the conference room to avoid early sound wave reflections from contaminating the recordings. To provide a natural speech context for the recording of each nonsense word, mothers were instructed to repeat three sentences, each of which contained one of the nonsense words, during the recording. The first word of each of these sentence was their child\u2019s name, which was followed by the words \u2018that is a,\u2019 followed by one of the three nonsense words. A hypothetical example of a sentence spoken by a mother for the recording was \u2018Johnny, that is a keebudieshawlt.\u2019 Prior to beginning the recording, mothers were instructed on how to produce these nonsense words by repeating them to the experimenter until the mothers had reached proficiency. Importantly, mothers were instructed to say these sentences using the tone of voice they would use when speaking with their child during an engaging and enjoyable shared learning experience (e.g. if their child asked them to identify an item at a museum). The vocal recording session resulted in digitized recordings of the mothers repeating each of the three sentences approximately 30 times to ensure multiple high-quality samples of each nonsense word for each mother. \n\n\n### Stimulus post-processing \n  \nThe goal of stimulus post-processing was to isolate the three nonsense words from the sentences that each mother spoke during the recording session and normalize them for duration and RMS amplitude for inclusion in the fMRI stimulus presentation protocol and the mother\u2019s voice identification task. First, a digital sound editor (Audacity:   http://audacity.sourceforge.net/  ) was used to isolate each utterance of the three nonsense words from the sentences spoken by each mother. The three best versions of each nonsense word were then selected based on the audio and vocal quality of the utterances (i.e. eliminating versions that were mispronounced, included vocal creak, or were otherwise not ideal exemplars of the nonsense words). These nine nonsense words were then normalized for duration to 956 ms, the mean duration of the nonsense words produced by the unfamiliar female voices, using Praat software similar to previous studies ( ;  ). A 10 msec linear fade (ramp and damp) was then performed on each stimulus to prevent click-like sounds at the beginning and end of the stimuli, and then stimuli were equated for RMS amplitude. These final stimuli were then evaluated for audibility and clarity to ensure that post-processing manipulations had\u00a0not introduced any artifacts into the samples. The same process was performed on the control voices and environmental sounds to ensure that all stimuli presented in the fMRI experiment were the same duration and RMS amplitude. \n\n\n### Post-scan mother\u2019s voice identification task \n  \nAll participants who participated in the fMRI experiment completed an auditory behavioral test following the fMRI scan. The goal of the Mother\u2019s Voice Identification Task was to determine if the participants could reliably discriminate their mother\u2019s voice from unfamiliar female voices. Participants were seated in a quiet room in front of a laptop computer, and headphones were placed over their ears. In each trial, participants were presented with a recording of a multisyllabic nonsense word spoken by either the participant\u2019s mother or a control mother, and the task was to indicate whether or not their mother spoke the word. The multisyllabic nonsense words used in the behavioral task were the exact same samples used in the fMRI task. Each participant was presented with 54 randomly ordered nonsense words: 18 produced by the subject\u2019s mother and the remaining 36 produced by unfamiliar female voices. \n\n\n### Signal level analysis \n  \nGroup mean activation differences for key brain regions identified in the whole-brain univariate analysis were calculated to examine the basis for TD\u00a0>\u00a0ASD group differences for both [unfamiliar female voices\u00a0>\u00a0environmental sounds] ( ) and [mother\u2019s voice\u00a0>\u00a0unfamiliar female voices] contrasts ( ). The reason for this analysis is that stimulus differences can result from a number of different factors. For example, both mother\u2019s voice and unfamiliar female voices could elicit reduced activity relative to baseline and significant stimulus differences could be driven by greater negative activation in response to unfamiliar female voices. Significant stimulus differences were inherent to this ROI analysis as they are based on results from the whole-brain GLM analysis ( ); however, results provide important information regarding the magnitude and sign of results in response to both stimulus conditions. Baseline for this analysis was calculated as the brain response to environmental sounds. The coordinates for the ROIs used in the signal level analysis were based on peaks in TD\u00a0>\u00a0ASD group maps for the [unfamiliar female voices\u00a0>\u00a0environmental sounds] and [mother\u2019s voice\u00a0>\u00a0unfamiliar female voices] contrasts. Cortical ROIs were defined as 5 mm spheres, and subcortical ROIs were 2 mm spheres, centered at the peaks in the TD\u00a0>\u00a0ASD group maps for the [unfamiliar female voices\u00a0>\u00a0environmental sounds] or [mother\u2019s voice\u00a0>unfamiliar female voices] contrasts. Signal level was calculated by extracting the \u03b2-value from individual subjects\u2019 contrast maps for the [unfamiliar female voices\u00a0>\u00a0environmental sounds] and [mother\u2019s voice\u00a0>environmental sounds] comparisons. The mean \u03b2-value within each ROI was computed for both contrasts in all subjects. The group mean \u03b2 and its standard error for each ROI are plotted in  . \n\n\n ## Data availability\n\nAll fMRI activation maps reported in the manuscript will be made available at NeuroVault (https://neurovault.org/collections/4815/). Full single subject raw data will be made public on the NIH NDAR repository, as per NIH rules (procedure is ongoing). https://neurovault.org/collections/4815/ The following dataset was generated: DanielArthur AbramsAarthiPadmanabhan2019fMRI activation maps reported in 'Impaired voice processing in reward and salience circuits predicts social communication in children with autism'NeuroVault4815 \n\n# Table(s)\n\n## ID: table1\n\n### Label: Table 1.\n\n                                                                                                        Unnamed: 0                                                                                  ASD (n\u00a0=\u00a021)                                         TD (n\u00a0=\u00a021)                 p-value\n0                                                                                                     Gender ratio                                                                                     18 M: 3 F                                           17 M: 4 F                   0.69\u2020\n1                                                                                                      Age (years)                                                                                  10.75\u00a0\u00b1\u00a01.48                                        10.32\u00a0\u00b1\u00a01.42                    0.34\n2                                                                                                   Full-scale IQ*                                                                                113.75\u00a0\u00b1\u00a015.04                                      117.45\u00a0\u00b1\u00a010.83                    0.38\n3                                                                                                             VIQ*                                                                                112.25\u00a0\u00b1\u00a016.13                                      118.55\u00a0\u00b1\u00a012.13                    0.17\n4  PIQ ADOS social ADI-A social ADI-B communication ADI- C repetitive behaviors Word reading Reading comprehension  111.52\u00a0\u00b1\u00a014.30 9.52\u00a0\u00b1\u00a02.54 6.81\u00a0\u00b1\u00a04.52 7.43\u00a0\u00b1\u00a05.01 4.10\u00a0\u00b1\u00a02.66 112.24\u00a0\u00b1\u00a011.34 108.29\u00a0\u00b1\u00a011.81  113.14\u00a0\u00b1\u00a013.46 - - - - 114.38\u00a0\u00b1\u00a08.96 115.38\u00a0\u00b1\u00a09.09  0.71 - - - - 0.50 0.35\n5                                                                                                 Max. Motion (mm)                                                                                   1.99\u00a0\u00b1\u00a00.93                                         1.73\u00a0\u00b1\u00a00.93                    0.36\n6                                                                                       Mother's voice ID accuracy                                                                                   0.88\u00a0\u00b1\u00a00.21                                         0.98\u00a0\u00b1\u00a00.04                    0.04\n\n### Caption\n\nDemographic and IQ measures\n\n### Footer\n\nDemographic and mean IQ scores are shown for the sample.M, Male; F, Female; WASI, Wechsler Abbreviated Scale of Intelligence.\u2020Chi-squared test.*Score missing for one participant in TD and ASD groups.\n\n\n\n## ID: app1table1\n\n### Label: Appendix 1\u2014table 1.\n\n                                         Contrast                                    Brain region  Effect size\n0  [Unfamiliar Voices minus Environmental Sounds]             Right-hemisphere Planum Polare (PP)         0.70\n1        [Mother\u2019s Voice minus Unfamiliar Voices]                 Right-hemisphere Intercalcarine         0.65\n2                                             NaN                        Right-hemisphere Lingual         0.68\n3                                             NaN                       Right-hemisphere Fusiform         0.66\n4                                             NaN                        Left-hemisphere Fusiform         0.67\n5                                             NaN                    Right-hemisphere Hippocampus         0.66\n6                                             NaN  Left-hemisphere Superior Parietal Lobule (SPL)         0.69\n7                                             NaN                     Right -hemisphere Precuneus         0.69\n\n### Caption\n\nEffect sizes for GLM results: TD vs.\u00a0ASD Group Analysis.The overall effect size measured across all brain clusters identified in the TD vs. ASD Group Analyses is 0.68.\n\n### Footer\n\nNone\n\n\n\n## ID: app1table2\n\n### Label: Appendix 1\u2014table 2.\n\n                                         Contrast                                               Brain region  Effect size\n0  [Unfamiliar Voices minus Environmental Sounds]                        Right-hemisphere Planum Polare (PP)         0.84\n1                                             NaN                    Left-hemisphere Nucleus Accumbens (NAc)         0.69\n2                                             NaN                      Right-hemisphere Anterior Insula (AI)         0.84\n3        [Mother\u2019s Voice minus Unfamiliar Voices]                        Left-hemisphere Heschl\u2019s Gyrus (HG)         0.77\n4                                             NaN                         Left-hemisphere Planum Polare (PP)         0.77\n5                                             NaN           Right-hemisphere Superior Temporal Sulcus (mSTS)         0.74\n6                                             NaN    Right-hemisphere Ventromedial prefrontal cortex (vmPFC)         0.73\n7                                             NaN                       Left-hemisphere Anterior Insula (AI)         0.77\n8                                             NaN  Right-hemisphere Rostral Antreior Cingulate Cortex (rACC)         0.73\n9                                             NaN            Right-hemisphere Supplementary Motor Area (SMA)         0.76\n\n### Caption\n\nEffect sizes for GLM results: Social Communication Covariate Analysis.The overall effect size measured across all brain clusters identified in the Social Communication Covariate Analysis is 0.76.\n\n### Footer\n\nNone\n\n\n\n## ID: app1table3\n\n### Label: Appendix 1\u2014table 3.\n\n                       Brain region   Coordinates\n0              Left-hemisphere pSTS    [\u221263\u201342 9]\n1             Right-hemisphere pSTS    [57 -31 5]\n2             Left-hemisphere vmPFC    [\u22126 32\u201314]\n3            Right-hemisphere vmPFC     [6 54 -4]\n4   Left-hemisphere Anterior Insula   [\u221228 18\u201310]\n5              Right-hemisphere VTA   [2 -22 -20]\n6               Left-hemisphere NAc    [\u221212 18\u20138]\n7              Right-hemisphere NAc    [14 18 -8]\n8               Left-hemisphere OFC   [\u221236 24\u201314]\n9           Left-hemisphere Putamen    [\u221224 14\u20138]\n10         Right-hemisphere Putamen   [16 14 -10]\n11          Left-hemisphere Caudate    [\u221218 4 20]\n12         Right-hemisphere Caudate    [14 22 -6]\n13        Right-hemisphere Amygdala   [30 -4 -24]\n14     Right-hemisphere Hippocampus   [28 -6 -26]\n15        Right-hemisphere Fusiform  [36 -28 -22]\n\n### Caption\n\nBrain regions used in functional connectivity analyses.\n\n### Footer\n\nNone\n\n\n\n## ID: app1table4\n\n### Label: Appendix 1\u2014table 4.\n\n  Brain Region Activation Height: p<0.005Extent: p<0.05 Height: p<0.005Extent: p<0.01 Height: p<0.001Extent: p<0.05 Height: p<0.001Extent: p<0.01\n0                     NaN                     67 Voxels                     87 Voxels                     30 Voxels                     41 Voxels\n1  Auditory Assoc. Cx, PP                           Yes                           Yes                           Yes                           Yes\n\n### Caption\n\nGLM Threshold Analysis: TD vs.\u00a0ASD Group Analysis [Unfamiliar Voices minus Environmental Sounds] fMRI Contrast.\n\n### Footer\n\nNone\n\n\n\n## ID: app1table5\n\n### Label: Appendix 1\u2014table 5.\n\n             Brain Region Activation Height: p<0.005Extent: p<0.05 Height: p<0.005Extent: p<0.01 Height: p<0.001Extent: p<0.05 Height: p<0.001Extent: p<0.01\n0                                NaN                     67 Voxels                     87 Voxels                     30 Voxels                     41 Voxels\n1           Occipital Fusiform Gyrus                           Yes                           Yes                           Yes                            No\n2  Temporal Occipital Fusiform Gyrus                           Yes                           Yes                            No                            No\n3                  Post. Hippocampus                           Yes                           Yes                            No                            No\n4                      Lingual Gyrus                           Yes                           Yes                           Yes                           Yes\n5                  Superior Parietal                           Yes                           Yes                           Yes                           Yes\n6                          Precuneus                           Yes                           Yes                           Yes                           Yes\n\n### Caption\n\nGLM Threshold Analysis: TD vs.\u00a0ASD Group Analysis [Mother\u2019s Voice minus Unfamiliar Voices] contrast.\n\n### Footer\n\nNone\n\n\n\n## ID: app1table6\n\n### Label: Appendix 1\u2014table 6.\n\n  Brain Region Activation Height: p<0.005Extent: p<0.05 Height: p<0.005Extent: p<0.01 Height: p<0.001Extent: p<0.05 Height: p<0.001Extent: p<0.01\n0                     NaN                     67 Voxels                     87 Voxels                     30 Voxels                     41 Voxels\n1     Auditory Assoc., PP                           Yes                           Yes                           Yes                           Yes\n2    Voice Selective, STG                           Yes                           Yes                           Yes                           Yes\n3  Mesolimbic Reward, NAc                     Yes (SVC)                            No                            No                            No\n4            Salience, AI                           Yes                           Yes                           Yes                           Yes\n\n### Caption\n\nGLM Threshold Analysis: Social Communication Covariate Analysis, [Unfamiliar Voices minus Environmental Sounds] fMRI Contrast.\n\n### Footer\n\nNone\n\n\n\n## ID: app1table7\n\n### Label: Appendix 1\u2014table 7.\n\n    Brain Region Activation Height: p<0.005Extent: p<0.05 Height: p<0.005Extent: p<0.01 Height: p<0.001Extent: p<0.05 Height: p<0.001Extent: p<0.01\n0                       NaN                     67 Voxels                     87 Voxels                     30 Voxels                     41 Voxels\n1      Primary Auditory, HG                           Yes                           Yes                           Yes                           Yes\n2  Voice-selective, STG/STS                           Yes                           Yes                            No                            No\n3  Mesolimbic Reward, vmPFC                           Yes                           Yes                           Yes                           Yes\n4              Salience, AI                           Yes                           Yes                           Yes                           Yes\n5            Salience, rACC                           Yes                           Yes                            No                            No\n6                Motor, SMA                           Yes                           Yes                           Yes                           Yes\n\n### Caption\n\nGLM Threshold Analysis: Social Communication Covariate Analysis, [Mother\u2019s Voice minus Unfamiliar Voices] fMRI Contrast.\n\n### Footer\n\nNone\n\n", "metadata": {"pmcid": 6391069, "text_md5": "87227c8340caa4450172af82a93dacd4", "field_positions": {"authors": [0, 165], "journal": [166, 171], "publication_year": [173, 177], "title": [188, 299], "keywords": [313, 361], "abstract": [374, 1701], "body": [1710, 62094], "tables": [62108, 73107]}, "batch": 1, "pmid": 30806350, "doi": "10.7554/eLife.39906", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6391069", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6391069"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6391069\">6391069</a>", "list_title": "PMC6391069  Impaired voice processing in reward and salience circuits predicts social communication in children with autism"}
