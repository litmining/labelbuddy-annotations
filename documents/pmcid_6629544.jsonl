{"text": "Korn, Christoph W. and Bach, Dominik R.\nNat Hum Behav, 2019\n\n# Title\n\nMinimizing threat via heuristic and optimal policies recruits\nhippocampus and medial prefrontal cortex\n\n# Keywords\n\n\n\n# Abstract\n \nJointly minimizing multiple threats over extended time horizons enhances\nsurvival. Consequently, many tests of approach-avoidance conflict incorporate\nmultiple threats for probing corollaries of animal and human anxiety. To\nfacilitate computations necessary for threat minimization, the human brain may\nconcurrently harness multiple decision policies and associated neural\ncontrollers\u2014but it is unclear which. We combine a task that mimics\nforaging under predation with behavioural modelling and functional neuroimaging.\nHuman choices rely on immediate predator probability\u2014a myopic heuristic\npolicy\u2014and on the optimal policy, which integrates all relevant\nvariables. Predator probability relates positively and the associated choice\nuncertainty relates negatively to activations in anterior hippocampus, amygdala,\nand dorsolateral prefrontal cortex. The optimal policy is positively associated\nwith dorsomedial prefrontal cortex activity. We thus provide a\ndecision-theoretic outlook on the role of the human hippocampus, amygdala, and\nprefrontal cortex in resolving approach-avoidance conflicts relevant for anxiety\nand integral for survival. \n \n\n# Body\n \n## Introduction \n  \nIn order to survive animals must minimize threats such as dying from\nstarvation or predation. This often entails conflicts between approaching food and\navoiding predators. Thus, decisions fundamental for survival necessitate challenging\ncalculations to balance competing goals, such as simultaneously maintaining energy\nhomeostasis and physical integrity  . In\nprinciple, this can be achieved in a normative manner by jointly minimizing the\nexpected impact of different threats such as starvation and predation. To do so,\ndecision-makers should consider extended sequences of risky decisions, in which\nthreat occurrences depend probabilistically on both current and future decisions\n , .\nBut computing the optimal policy for such multi-step decision situations requires\ntaxing evaluations of all possible future states, which might be too complicated to\nexecute under threat  \u2013 . Therefore, decision-makers may take advantage\nof simplifying, heuristic policies that approximate optimal solutions, for example\nby minimizing the most prominent immediate threat and disregarding future time\npoints. Consequently, it has been proposed that the human brain comprises multiple\nneural controllers which implement a variety of decision policies spanning different\nlevels of sophistication and efficacy  , , .\nSeveral of these policies and neural controllers may be concurrently invoked when\nseeking a solution to a complex situation that threatens survival  , \u2013 . \n\nA particularly relevant example for such decision problems is foraging for\nfood under the risk of predation, which involves an approach-avoidance conflict\n(i.e., approaching food while avoiding predators). Laboratory tasks that mimic this\nscenario are widely used as animal anxiety tests, in which an animal can explore the\nenvironment or obtain food amid simultaneous physical threat  \u2013 . Some of\nthese paradigms have been successfully reverse-translated to human computer games\n \u2013 . Importantly, a long-standing tradition of lesion studies in rodents\n  and an emerging field of lesion and\nfunctional imaging studies in humans  \u2013 , , ,  converge on implicating\nsimilar neural structures\u2014in particular the anterior hippocampus and the\namygdala. These findings suggest an at least partly homologous neural implementation\nof approach-avoidance conflicts. \n\nDespite progress in understanding the neural circuits required for\ndecision-making in approach-avoidance conflicts  , , , the algorithms used in such scenarios, and their neural\nimplementation\u2014especially with respect to the hippocampus\u2014remain\nelusive. Most paradigms do not separate individual decisions (such as the elevated\nplus maze, the open field test, and their human analogues) and several others reduce\nthe task to a single decision with relatively low complexity (such as operant\nconflict tests in rodents and humans). In these tasks, it is thus unclear what goal\nthe agent pursues and whether momentary behaviour is part of an extended action\nplan. Furthermore, previous research has rarely assessed the agent\u2019s\nevaluation of different risky outcomes in approach-avoidance conflicts (see the\nstudy on macaques by   for a notable\nexception). This leads to a disconnection between neurobiological research on\napproach-avoidance conflicts, which has implicated the hippocampus, and decision\nneuroscience, which has mostly linked (medial) prefrontal regions to flexible action\nselection  \u2013 . \n\nHere, we apply a decision-theoretic outlook on choice sequences in\napproach-avoidance conflict by employing a mathematically specified computer game\nthat mimics foraging under predation. We first address the cognitive strategies by\nwhich human decision-makers resolve approach-avoidance conflicts, and then\ninvestigate the neural representation of the associated decision variables (DV),\nspecifically of a threat-related DV in hippocampus and the amygdala. This neural\nhypothesis elaborates our behavioural hypothesis that humans base their decisions\nprimarily on a threat-related policy. The design of our task was inspired by several\nrecent studies using virtual foraging tasks in humans and non-human primates  , , \u2013  and thus links approach-avoidance conflicts to the burgeoning\nliterature on complex decision behaviour in ecological scenarios  , \u2013 , . \n\n\n## Results \n  \n### An approach-avoidance task afforded the computation of an optimal\npolicy \n  \nOur approach-avoidance conflict task was framed as virtual foraging under\nthe dual threats of predation and starvation (see   for a task outline and   for a list explaining all relevant variables). In brief,\nparticipants made sequential decisions on up to five trials, called\n\u201cdays,\u201d in 240 mini-blocks, called \u201cforests\u201d\n(resulting in total of 400 trials, i.e., days, per participant). Participants\nwere monetarily incentivized to (a) keep their \u201cenergy points\u201d\nabove zero across the possible maximum number of five days within a given forest\nand to (b) attain as often as possible the maximum number of five energy points.\nOtherwise, the exact number of energy points accrued in a forest was not\ntranslated into financial payoffs. On each day within a forest, participants had\nto make a decision between a certain option, called \u201cwaiting,\u201d and\na risky option, labelled \u201chunting\u201d which we refer to here as\n\u201cforaging.\u201d Waiting entailed a sure loss of one energy point.\nForaging had several possible outcomes: first, participants could be\n\u201cattacked by a predator\u201d with known probability, and lose all\ntheir energy points on the current day. If they were not attacked, the known\n\u201cprobability of foraging success\u201d determined whether they would\ngain a variable number of energy points, or lose a fixed number of two energy\npoints. Thus, participants could reach zero energy points due to\n\u201cpredation\u201d or due to \u201cstarvation.\u201d On each day\nwithin a forest, one of two combinations of probabilities of predator attack and\nforaging success was randomly presented, called \u201cweather type.\u201d\nConditions, in terms of probabilities of predator attack and foraging success,\nappeared in randomized order. Participants were fully and explicitly informed\nabout all variables of the current forest. They knew that they would stay within\neach forest for a maximum of five days, and that they would be rewarded for a\nfull five-day sequence. The number of days past was not depicted on the\nscreen. \n\nThe task was mathematically specified as a Markov decision process. This\nallowed us to calculate the   a priori   optimal policy that\nmaximizes participants\u2019 monetary rewards by jointly minimizing the\nthreats of dying due to predation and starvation for a fixed time horizon of\nfive days (as well as maximizing the number of times reaching the maximum energy\nlevel). This optimal policy combines the probabilities of predator attack and\nforaging success as well as further task variables in a mostly non-linear\nfashion ( ). We use the word \u201coptimal\u201d here in the sense of\n\u201coptimal under task instructions\u201d (i.e., an optimal choice implies\nchoosing the action that maximizes the weighted sum across the relevant Markov\nbranches). The optimal policy   per se   makes prescriptions for\nchoice on the basis of the value difference between the two choice options of\nforaging and waiting. That is, the optimal policy   per se  \nprescribes foraging if the value difference exceeds zero, it prescribes waiting\nif the value difference falls below zero, and it is indifferent if the\ndifference is exactly zero. We related participants\u2019 choices, reaction\ntimes (RTs), and functional magnetic resonance imaging (fMRI) data to the\ncontinuous range of value differences according to the optimal policy. In the\nfollowing, we therefore use the term \u201coptimal policy\u201d as shorthand\nto refer to this continuous range of value differences between foraging and\nwaiting. \n\n\n### A large set of decision variables could potentially explain\nparticipants\u2019 behaviour \n  \nTo explain participants\u2019 choices, we considered an extensive set\nof potential DVs: (1) the optimal policy and (2-16) fifteen possible DVs that\nare not optimal and are therefore considered as heuristics. In general, we\nconsidered heuristics for three reasons: because they constituted components of\nour sequential decision-making task, because they were imperfect variants of the\noptimal policy, or because they captured influences of past trials (see   for detailed descriptions). \n\nSpecifically, three heuristic were features of the current foraging\noption: (2) the predator probability, (3) the probability of foraging gain, and\n(4) the gain magnitude. Four variables took the current energy state into\naccount: the current energy state as a (5) continuous variable ranging from one\nto five and as a (6) binary variable distinguishing the energy state one, in\nwhich waiting leads to sure death, from higher energy states. The two variables\ncalled (7) \u201cexpected energy\u201d and (8) \u201cexpected energy\nchange\u201d correspond to the expected value of foraging at the current\nenergy state, and the resulting difference in energy state. Another DV relied on\n(9) the number of days past. We refer to the following three heuristics as\n\u201cpseudo-optimal policies\u201d because they would be optimal in\nalternative scenarios: (10) one short-sighted policy would be optimal if\nparticipants remained in a forest for the current day only (i.e., time horizon\nof one day); two pseudo-optimal policies would be optimal if participants had to\nsolely minimize the threats of either (11) starvation of (12) predation. Four\nfurther DVs were included to address suggestions by anonymous reviewers: (13)\none of these DVs was a pseudo-optimal policy that weighted the time horizon of\nthe optimal policy exactly according to the distribution of the number of days\nthat participants actually remained in a forest (i.e., an average of 2.5 days,\nwhich was implemented to enhance fMRI design efficiency). Three DVs addressed\nthe influence of the preceding trial or forest on the current trial: (14) the\nenergy change from the last to the current trial, (15)\n\u201cwin-stay-lose-shift,\u201d and (16) a DV indicating whether\nparticipants had died in the preceding forest. \n\n\n### Participants\u2019 choices relied on predator probability and optimal\npolicy \n  \nWe then greedily searched for a single DV that best explained\nparticipant's choices. Bayesian model comparison identified predator\nprobability as the best single predictor of participants\u2019 decisions out\nof the 16 candidate variables in the fMRI sample (final n=24; see   for metrics of Bayesian Information\nCriterion (BIC) and   for BIC and protected exceedance\nprobabilities). Searching for the predictor that best explained the remaining\nvariance revealed that participants additionally relied on the optimal policy\n( ). The model including both\npredator probability and the optimal policy outperformed the simpler model that\nonly included predator probability and also more complex models that\nadditionally included the interactions between different task variables ( ),\nas well as all 66 possible combinations of two DVs from the set of the twelve\nDVs motivated   a priori   (the additional four DVs suggested in\nthe review process were not included in this comparison because they had already\nperformed quite badly in the initial analysis;  ). \n\nCrucially, the winning model robustly predicted empirical choices. This\nis illustrated in posterior predictive checks splitting data according to the\ntwo variables incorporated in the model (predator probability,  , optimal policy,  ) and in posterior predictive checks\nsplitting data according to the variables not included in the model ( ).\nConversely, models with the other DVs did not capture the pattern of empirically\nobserved choice ( ). Individual differences in the best fitting models are\nillustrated in  . \n\nWe did not find evidence that the best-fitting DVs changed over the time\ncourse of the experiment. Specifically, we fitted models separately to data from\nthe first and second halves of the experiment. In both halves, a combination of\npredator probability and optimal policy emerged as the best model ( ). \n\nAll of the above results were replicated in an independent behavioural\nsample acquired during the revision process (n=23; see   and  ). That is, predator probability\ndecisively emerged as the single best predictor. Results according to BIC, which\nwas our primary analysis approach, favoured a model combining predator\nprobability and optimal policy (protected exceedance probability favoured a\ncombination of predator probability and \u201cbinary energy;\u201d  ). \n\nThe two samples only differed when searching for a third DV that might\nexplain variance on top of predator probability and optimal policy. In the fMRI\nsample, the comparison of models with three DVs suggested that the probability\nof foraging gain might best capture remaining variance ( ).\nIn the behavioural sample, \u201cbinary energy\u201d emerged as the DV\ncapturing most remaining variance ( ). Due to the difference between the\ntwo samples we refrain from drawing conclusions about the identity of a\npotential third DV. \n\n\n### Behavioural models were distinguishable \n  \nThe large set of DVs tested here entailed that some of these DVs were\nrelated to each other by design. For example, the optimal policy shared on\naverage variance > 50% with two of the pseudo-optimal policies and with\n\u201cexpected energy change\u201d (see   for\nmean shared variances). Predator probability shared variance > 50% with\ntwo of the pseudo-optimal policies and with \u201cexpected energy.\u201d\nStill, we argue that the DVs that emerged as relevant in our comparison were\nreasonably dissociable or explained variance on top of one another, as was the\ncase for the optimal policy and predator probability. To illustrate the features\nof the optimal policy, we plotted the relations between the 15 heuristics and\nthe optimal policy ( ). \n\nMore importantly, confusion analyses with 2000 simulations per model\nshowed that models with one of the 16 considered DVs could be almost perfectly\nrecovered from simulated data. This means it is unlikely that our winning model\nwould have wrongly been selected if another model were the true model. Among the\nfirst 13 DVs of our list, only one simulated model was misclassified ( ).\nLess than 10% misclassifications occurred for the last three DVs of our list\nthat captured the influence of preceding trials or forests and that performed\noverall worst in the model comparisons ( ). Confusion analyses on the models\nwith predator probability plus one of the 15 other considered DVs indicated very\ngood recovery of the respective models ( ). Less than 5% misclassifications\nemerged for the initially considered DVs (but again misclassifications occurred\nmore often for the last three DVs in our list that did not explain\nparticipants\u2019 behaviour;  ). \n\nTo explore how participants\u2019 behaviour in the task was related to\ntheir subjective assessments, we administered a post-experiment questionnaire\nthat assessed how much participants relied on different components of the task\n( ). Numerically, importance ratings were highest for predator\nprobability, which may suggest that heuristic identified in the model\ncomparisons corresponded to participants\u2019 consciously accessible\ndecisions. \n\n\n### Reaction times scaled with optimal policy and choice uncertainty of predator\nprobability \n  \nWe predicted that participants\u2019 use of the predator probability\nand of the optimal policy should be reflected in their RTs. Specifically, the\ntwo metrics themselves and/or their corresponding choice uncertainties should be\nassociated with RTs. This was indeed the case as shown by analyses of RTs in\nlinear mixed effects (see   for\nstatistics of the fMRI sample and the behavioural sample). In both samples, the\noptimal policy was directly related to RTs such that higher values for foraging\nversus waiting were related to faster decisions (fMRI sample: n=24,\nt(30.67)=-3.04, p=0.005; log-likelihood difference, LLD=-4.0, 95%-confidence\ninterval, CI=[-0.10, -0.02]; behavioural sample: n=23, t(35.64)=-3.03, p=0.005,\nLLD=-4.1, CI=[-0.09, -0.02]). Conversely, higher choice uncertainty of the\npredator probability was associated with slower decisions (fMRI sample: n=24,\nt(25.17)=3.43, p=0.002, LLD=-4.8, CI=[0.22, 0.87]; behavioural sample: n=23,\nt(28.31)=2.69, p=0.012, LLD=-3.2, CI=[0.07, 0.54]). Additionally, the\ndiscrepancy in choice probabilities according to the two metrics scaled with\nlonger RTs (fMRI sample: n=24, t(28.85)=5.53, p<0.001, LLD=-10.1,\nCI=[0.13, 0.27]; behavioural sample: n=23, t(22.34)=3.56, p=0.002, LLD=-4.9,\nCI=[0.05, 0.20]). This discrepancy quantifies how much the prescriptions for\nchoice differ between using predator policy and optimal policy. In sum, RTs\ncorroborate that the predator probability and the optimal policy exert a joint\ninfluence on participants\u2019 decision process on the same trials. \n\n\n### FMRI data could be related to decision variables derived from the winning\nmodel \n  \nWe aimed at identifying trial-by-trial associations of the behaviourally\nrelevant variables with fMRI data during the choice phase. Specifically, we\nincluded the following variables as parametric modulators of the choice phase in\nour primary general linear model (GLM): (1) the predator probability, (2) the DV\nunder the optimal policy, (3&4) the associated choice uncertainties of\nthe former two metrics, (5) the discrepancy between these two metrics, and (6)\nlog-transformed RTs. In our primary GLM, parametric modulators were not\northogonalized but we obtained the same results (except for a few minute\ndifferences due to rounding) in separate GLMs when varying the order of the\nrespective orthogonalized parametric modulators such that the relevant\nparametric modulators were entered last (see  \nfor shared variances between the included variables). \n\n\n### FMRI analyses related hippocampus, amygdala, and DLPFC activity positively to\npredator probability and negatively to the associated choice uncertainty \n  \nIn line with our expectations derived from studies on related types of\nhuman approach-avoidance conflicts tests  , , , predator probability was positively related to BOLD\nsignals in a cluster in the right anterior hippocampus extending into\nneighbouring amygdala, as well as bilateral clusters in the dorsolateral\nprefrontal cortex (DLPFC;   and\n ). Notably, a cluster in the\nanterior hippocampus and neighbouring amygdala showed stronger BOLD responses\nwith lower choice uncertainty of the predator probability ( ). This cluster overlapped with the cluster\nidentified for predator probability   per se   (see   for the overlap of the two\nfunctional clusters and for information on the overlap with anatomical\nhippocampus and amygdala masks, see   or relation of parameter estimates to predator probability,).\nUncertainty of the predator probability was also negatively associated with BOLD\nsignal in bilateral DLPFC and in right lateral inferior frontal gyrus (IFG) as\nwell as other regions. Similar to the pattern in the hippocampus and amygdala,\nactivity maps showed overlaps between the clusters identified for predator\nprobability   per se   and the choice uncertainty of the predator\nprobability ( ). \n\nTaken together, both the predator probability and its associated choice\nuncertainty scaled on a trial-by-trial basis with BOLD response in the anterior\nhippocampus and the amygdala as well as DLPFC regions. \n\n\n### FMRI analyses related DMPFC activity positively to both the optimal policy\nand the associated choice uncertainty \n  \nThe optimal policy was positively related to BOLD signals in the\nposterior dorsomedial prefrontal cortex (DMPFC;   and  ),\nextending into the supplementary motor area. The same parametric contrast\nrevealed a positive relation of the optimal policy with anterior cingulate\ncortex, bilateral IFG (extending into insula), and bilateral thalamus. The\nchoice uncertainty of the optimal policy scaled positively with BOLD signal in\nDMPFC and DLPFC regions as well as IFG and insula ( ). The same metric scaled negatively with activity in the\nposterior cingulate cortex. In the DMPFC and IFG (extending into insula), we\nfound overlaps between clusters elicited by the optimal policy per se and by the\nchoice uncertainty of the optimal policy ( ).\nFurthermore, BOLD signal in the thalamus showed a positive association with the\ndiscrepancy in choice probabilities between the predator probability and the\noptimal policy. \n\n\n### FMRI analyses related activity in striatum and medial regions to\noutcomes \n  \nDuring the time point when participants saw the outcome of their choice,\nthe change in energy state (i.e., energy state at outcome minus energy state at\nchoice) was robustly associated with neural activity in a well-established\nreward network (i.e., bilateral striatum, ventral medial prefrontal cortex, and\nposterior cingulate cortex;   and  ). This activation pattern was\nexpected since participants were monetarily rewarded for avoiding an energy\nstate of zero and for reaching the maximal energy state of five points. \n\n\n### Follow-up and exploratory fMRI analyses strengthen and illustrate the\nrelationships between BOLD signal and behavioural variables \n  \nTo exclude that the above-mentioned fMRI results were unduly driven by\nparticipants\u2019 choices themselves, we ran a secondary model, in which we\nadditionally included choices as a binary parametric modulator. The clusters\nobtained with this secondary model generally replicated the results reported\nabove (see  ). \n\nIn the same vein, we also obtained similar clusters in a tertiary GLM,\nin which the choice uncertainties and the discrepancy were calculated from the\nindependent behavioural sample (see  ; the main difference was that the\ncluster in the left hippocampus and amygdala, which was positively related to\nthe predator probability, survived small-volume correction for an anatomical\nmask of bilateral hippocampus but failed to reach whole-brain FWE correction at\np<0.05). \n\nTo explore inter-individual variability, we included the parameter\nestimates from the behavioural models linking participants\u2019 choices to\npredator probability and optimal policy as covariates into the respective\ncontrasts. Two clusters in bilateral striatum co-varied negatively with\nindividual parameter estimates capturing the degree to which predator\nprobability influenced participants\u2019 choices ( \nand  ). No significant clusters emerged for individual parameter\nestimates related to the optimal policy. \n\nAs noted in the previous sections, we observed that in some regions\nactivity related to predator probability or optimal policy overlapped with\nactivity related to the choice uncertainty of that DV ( ).\nFollowing the suggestions of anonymous reviewers, we visualized these\nrelationships using functionally defined regions of interest (ROIs) based on the\nanalyses described for our primary GLM. These analyses are not independent and\nserve illustrative purposes. Visual inspection shows that in several ROIs the\nrelationships were not completely linear across the four quartiles of the\nparametric modulators, which likely suggests that these regions were influenced\nby more than one behavioural variable ( ). To explore this pattern\nfurther, we conducted non-independent and post-hoc tests within each of the\nfunctionally defined ROIs to assess the specificity of these ROIs for the\nrespective parametric modulators (at p < 0.001). As could be expected\nfrom the overlap maps (  and  ),\nthe clusters in the hippocampus and the amygdala as well as the clusters in\nbilateral DLPFC were positively related to predator probability   per\nse   and negatively related to the associated choice uncertainty\n( ). Similarly, DMPFC and right IFG (extending into insula) were\npositively related to both the optimal policy   per se   and the\nchoice uncertainty of the optimal policy. We also tested in a similar post hoc\nfashion whether BOLD signals in the identified functional ROIs were related to\nactivity elicited when including the three next-best DVs from the behavioural\nmodel comparison into separate GLMs (i.e., the probability of foraging gain,\ncontinuous energy, and expected energy change; see  ). Only clusters in the medial occipital cortex\nshowed relationships ( ). Furthermore, supplementary GLMs that\nincluded the interactions between each policy and its associated choice\nuncertainty revealed clusters in the occipital lobe for these interaction\ncontrasts ( ). \n\n\n\n## Discussion \n  \nWe demonstrate that humans employ two decision policies of varying\ncomplexity in a virtual approach-avoidance conflict task, which translates the\noften-evoked biological example of foraging under predation  ,  into a mathematical\nframework amenable to decision-theoretic analyses. Participants primarily based\ntheir choices on the probability of predator attack\u2014a myopic but\neasy-to-compute heuristic policy. Beyond that, they relied on the normatively\noptimal policy, which entails sophisticated integration of various task components\nas indicated by analyses of choice and RT data. These two policies were reflected in\nmacroscopically different brain regions, which corroborates the theoretical notion\nthat multiple neural controllers take care of different survival-relevant threats\n , .\nCrucially, our results identify the neural controller of the heuristic policy with\nstructures often implicated in approach-avoidance conflicts in both rodents and\nhumans  \u2013 , , . That is, the anterior hippocampus and the\namygdala related to predator probability as well as to the uncertainty of using this\npolicy during choice. The optimal policy, and also the choice uncertainty thereof,\nwere associated with parts of the DMPFC, which dovetails with the general roles of\nthis region in decision-making  , . \n\nPredator probability emerged as the primary policy employed by participants\namong a variety of potential alternatives. This result resonates with previous\ndemonstrations of the same metric modulating behaviour in approach-avoidance\nconflict tasks with spatial layouts  , , .\nNotably, participants used the predator probability as a primary heuristic and not\nthe probability of foraging gain, which constituted the winning policy in an\nanalogous virtual foraging task that did not include predation  . This may indicate that participants are able to select a\nparticularly appropriate heuristic for the task at hand, i.e., a heuristic that\ncombines computational simplicity with near-optimal approximations. Given several\ntheoretical accounts from multiple fields arguing for adaptive heuristic\ndecision-making  \u2013 , it is an interesting avenue for future\nresearch to investigate how participants select and switch between different\nheuristics under varying biologically inspired decision tasks  , . \n\nNeurally, different heuristics are likely to be implemented by different\ncontrollers  . Here, the heuristic of using\npredator probability related to the anterior hippocampus and the amygdala, which\ndemonstrates that in humans these structures are implicated in computing a decision\npolicy central for a rather abstract type of approach-avoidance conflict without\nphysical threats in contrast to most rodent tasks  \u2013 , . Our fMRI results cannot delineate\nsubtleties in the roles of hippocampus and amygdala but we would like to highlight\nthat work on rodents has identified dense monosynaptic connections between the two\nstructures as well as reciprocal electrophysiological interactions during\napproach-avoidance conflict  . While these\nconnections appear to be crucial for avoidance of acquired threat predictors (as in\nthe present study), they may be less relevant in avoiding innate threat predictors\nsuch as in the elevated plus maze  . \n\nWith respect to work on humans, our findings relating anterior hippocampus\nand also the amygdala to predator probability corroborate recent studies implicating\nthese structures in different types of approach-avoidance conflict  \u2013 , , , . In contrast to\nseveral of these studies, our task did not entail spatial layouts, directional\nmovements, or mnemonic demands, which have been argued to possibly impact\nhippocampal activity  . The relation between\npredator probability and hippocampus in our rather strategic task fits particularly\nwell with the recently demonstrated involvement of the hippocampus involvement\nduring strategic decisions to escape from a slow-attacking virtual predator  . Overall, the findings linking the\nhippocampus and amygdala with risks metrics related to virtual predators may thus\nsuggest a more generic role of these regions in risky decision-making than usually\nacknowledged in the field of decision neuroscience. Future studies will be necessary\nto delineate whether these two structures are specifically involved when risks are\nframed in terms of threats to virtual survival. Possibly, participants in our tasks\nmight have interpreted predator probability as some form of \u201cdefensive\ndistance\u201d since we visually depicted increasing predator probability as\ntriangles of increasing sizes. Thus, our work accords with previous studies on\ndefensive distance in rodents and recent efforts to reverse-translate defensive\npatterns from rodents to humans  . \n\nInterestingly, we found that the anterior hippocampus and the amygdala, in\naddition to tracking predator probability, also related to the choice uncertainty\nassociated with using predator probability as a heuristic. Previous research has\nlinked activity in the hippocampus to metrics of outcome uncertainty when humans\nmake value-based decisions   or inferences\nabout abstract variables   and sensory\nstimuli  , . Here, we were not interested in outcome uncertainty but rather in\nchoice uncertainty, i.e. the uncertainty associated with using a particular decision\nstrategy. Potentially, outcome and choice uncertainty share common neural substrates\nalthough this is not necessarily the case for conceptually different types of\nuncertainty  . \n\nAlthough we focused particularly on the hippocampus and the amygdala given\ntheir prominent role in animal studies on approach-avoidance conflicts, we note that\nin our task we observed similar effects for the DLPFC (particularly in the right\nhemisphere). DLPFC involvement is consistent with the well-described fMRI activity\nin this region for processing risks during choice  . \n\nThe optimal policy also guided participants\u2019 decisions as shown by\nbehavioural model comparisons that pitted the optimal policy against a number of\nrather close-by competitors such as metrics related to immediate changes in expected\nvalue and policies that would have been optimal in slightly different tasks. We\nconjecture that humans may often fail to compute the optimal policy in more\nchallenging real-life situations that entail larger action repertoires, stricter\ntime pressure, and the need to learn environmental states and contingencies  . Previous research has not been able to\naddress whether humans are capable of using elaborate decision policy for\nsurmounting approach-avoidance conflicts. We could do so because our sequential\ndecision-making task was formulated in the precise mathematical framework of a\nMarkov decision process (which specifies criteria for optimality without committing\nto particular utility functions or risk preferences). \n\nThe optimal policy was related to a part of the DMPFC and the anterior\ncingulate cortex as well as IFG. The DMPFC and the IFG additionally tracked the\nchoice uncertainty of the optimal policy. In general, these prefrontal regions are\nimplicated in various types of decision-making tasks  \u2013 . The implication of\nthe DMPFC and the anterior cingulate cortex might also point to further converging\nneural processes across species since medial prefrontal regions are also implicated\nin rodent approach-avoidance tasks  . A\nclose-up comparison indicates commonalities and differences in activation patterns\nwith our previous study that used a foraging task without predation\u2014and thus\nwithout approach-avoidance conflict   ( ). In\nboth studies, choice according to the optimal policy was related to activity\nclusters in the anterior cingulate cortex (extending into the midcingulate cortex)\nwhich\u2014although not overlapping\u2014were located in close proximity. The\nchoice uncertainty of the optimal policy was related to slightly more anterior DMPFC\nactivity in the previous compared with the current study. In our view, the biggest\ndifference between the studies concerns the role of the DMPFC extending into the\ndorsal part of the anterior cingulate cortex: In the previous study, activity in\nthis regions robustly scaled with the discrepancy between using the\nheuristic\u2014in that case the probability of foraging gain\u2014and using the\noptimal policy. In the current study, the same region scaled with the optimal policy\nitself (and its choice uncertainty). This informal comparison between our studies\nmay suggest the exciting possibility that the approach-avoidance conflict inherent\nin the choice options of the current task shifted the balance between the metrics\nencoded in the DMPFC\u2014a conjecture to be tested in future experiments. \n\nFrom a process perspective, we deem it intriguing that overlapping regions\ntracked the decision variable itself as well as its choice uncertainty. This seemed\nto be the case in the hippocampus, amygdala, DLPFC, DMPFC, and IFG. Choice\nuncertainties were calculated as the derivatives of the logistic functions linking\nthe decision variables to participants\u2019 choices, which implies that choice\nuncertainties are highest at the inflection points of the logistic functions and\nlowest at the outer ranges when decision variables clearly prescribe one of the two\nactions. We speculate that overlapping representations of a decision variable and\ntheir choice uncertainty might aid in adjusting the degree to which these decision\nvariables are used for choice, which hints potentially at local and distributed\nneural controllers for arbitrating between the heuristic and optimal policies\n(similar to what has been shown for model-free versus model-based learning  , ). \n\nUsing a approach-avoidance conflict task with a precise decision-theoretic\nframework, we demonstrate that human DMPFC computes a sophisticated optimal policy\non top of a predator probability heuristic that relates to the hippocampus and the\namygdala\u2014along with the associated choice uncertainty. These findings argue\nagainst a monolithic view of approach-avoidance conflicts and provide evidence for\nan interplay of two algorithms implemented by multiple controllers. Our study\ndovetails both on a conceptual and on a neural level with work on survival circuits\nand risk assessment in rodents and humans  , , , , . In particular, we link mathematically\ndefined decision policies of varying complexity to flexible, higher-order cortical\nregions. Thereby, our study opens new avenues for translational research on the role\nof approach-avoidance conflicts for anxiety. \n\n\n## Methods \n  \n### Participants \n  \n#### fMRI sample in Zurich \n  \nWe recruited 29 participants via mailing lists of local\nuniversities. Five participants were excluded: One due to head motion\n> 4 mm during MRI, one due to an incidental medical finding revealed\nby MRI, and three who behaved almost deterministically, i.e., they selected\none of the two choice options in more than 0.85 of the retained trials. (We\nselected 0.85 as cut-off during the analysis process. The proportion of\nchoosing the foraging in remaining sample was 0.59 \u00b1 0.10, mean\n\u00b1 SD; see  ). The final sample comprised 24\nparticipants (11 female; age = 25.0 \u00b1 3.9 years). Due to equipment\nmalfunction, two participants only performed nine out of ten sessions.\nParticipants received a show-up fee of CHF 50 plus a variable amount (see\n ). \n\n\n#### Behavioural sample in Hamburg \n  \nWe recruited 26 participants via a local online platform. Three\nparticipants were excluded: One due to equipment malfunction and two who\nselected one of the two choice options in more than 0.85 of the retained\ntrials (value in remaining sample: 0.55 \u00b1 0.18, mean \u00b1 SD, for\nthe foraging option). The final sample comprised 23 participants (15 female;\nage = 25.9 \u00b1 3.5 years). Due to time constraints, one participant\nonly performed eight out of ten sessions. Participants received a show-up\nfee of EU 12 plus a variable amount. \n\nThe study was conducted in accord with the Declaration of Helsinki\nand approved by the governmental research ethics committees (fMRI sample:\nKantonale Ethikkommission Z\u00fcrich, KEK-ZH-Nr. 2013-0328; behavioural\nsample: Ethikkommission der \u00c4rztekammer Hamburg, PV5746). All\nparticipants gave written informed consent using a form approved by the\nethics committees. \n\n\n\n### Sequential decision-making task \n  \n#### Instructions and task \n  \nSee   for an overview of\nthe task setup. Participants received detailed written and oral step-by-step\ninstructions (see  : Task instructions), which presented the\ntask as virtual foraging under the dual threats of predation and of\nstarvation. To familiarize themselves with the task, participants performed\ntwo training sessions: A first short training session of four forests with\nfive days each (after which participants could ask questions) and a second\nlonger training session of 24 forests with five days each. The behavioural\nsample additionally received a questionnaire testing for task understating\nafter the first training session (see  :\nComprehension questionnaire,  ). In our task,\n\u201cforest\u201d refers to a mini-block of \u201cdays\u201d with\neach \u201cday\u201d being one trial. For the main behavioural task\nduring fMRI scanning, forests always lasted five days, but to maximize fMRI\nefficiency these five days could be interrupted, i.e. the first few days\nwere played in the scanner, and the remaining days afterwards. Participants\nknew about this feature but did not know at which point a given forest would\nbe interrupted. Participants performed ten sessions of the main behavioural\ntask in the MR scanner (fMRI sample) or on a desktop computer (behavioural\nsample). The number of days per forest played in the scanner followed an\nexponential distribution with a mean of 2.5 resulting in 40 days in 24\nforests per session. After fMRI scanning, participants completed one\nrandomly selected forest per session, and were rewarded according to\nperformance in these ten forests. For each forest in which they survived\n(regardless of the final energy state) participants received one additional\nreward point. On top of that, they received one reward point for each time\nwithin a forest that their energy level reached five points. Each point\ncorresponded to CHF 1.50 (fMRI sample) or EUR 1 (behavioural sample). The\ntask was presented using the MATLAB toolbox Cogent ( ). After the experiment, all\nparticipants received a questionnaire asking for specific strategies and for\nratings of task components (see  : Post-experiment questionnaire,\n ). For exploratory analyses, participants in the behavioural\nsample additionally filled in short forms of the State-Trait Anxiety\nInventory (STAI)   and of the Need\nfor Cognition (NFC) scale  . \n\n\n#### Mathematical framework and optimal policy \n  \nWe modelled the task as a Markov decision process (MDP)  . MDPs are specified by (1) the\npossible states, (2) the action repertoire, (3) the transition matrix\nbetween these states, (4) the rewards associated with transitions, and (5)\nthe temporal horizon. In the following, we list these components:    \nStates: 12 states per forest, i.e., 6 energy states (0-5\nenergy points) x 2 (weather types). \n  \nActions: foraging or waiting. \n  \nTransition matrix: this is constructed from the\nprobabilities of predator attack, the probabilities of foraging\ngain, gain and loss magnitudes, and the transition probability\nbetween the two weather types (which is always 0.5 and\nindependent of the chosen action). Zero energy states are\nabsorbing. Waiting leads to a sure loss of one point. Foraging\ncan lead to an attack of the predator, which results in a\ntransition to a zero energy state. If the predator does not\nattack, transitions depend on whether a foraging gain occurs or\nnot. That is, the energy state is either increased by the gain\nmagnitude (with total energy being capped at five) or reduced by\ntwo points. \n  \nRewards: all transitions to zero energy states are\nassociated with a reward of -1, all transitions to five energy\npoints are associated with a reward of +1, and all other\ntransitions with a reward of 0. That is, the rewards in the MDP\nreflect the monetary incentives participants had when performing\nthe task. \n  \nTemporal horizon: our task imposes a finite time horizon\nof five steps from the start of each forest. Time steps are\ncalled days. \n  \n\nThe optimal policy specifies actions that maximize obtained rewards.\nThat is, the optimal policy depends on the choice at the current time step\nand the remaining\u2014up to four\u2014time steps. To derive optimal\npolicies in our finite-horizon scenario, we used backward induction:\nSpecifically, we started from the final time step (i.e., day five) and\ncalculated the values of the two choice options (i.e., foraging or waiting)\nfor each state. These values depend on the possible transitions from the\nrespective states. If the value of foraging is higher than the value of\nwaiting, foraging is the deterministically better option in that state and\nat that time step\u2014or vice versa. If both choice options have the same\nvalue, the optimal choice is indifferent between the two options and this\nvalue is used to calculate the values for the second-to-last time step. If\nthe two choice options differ in value, the value of the better choice\noption (i.e., the maximum over the values for foraging and waiting) is used\nto determine the optimal choice and to calculate the values for the\nsecond-to-last time step and. This procedure was repeated until arriving at\nthe first time step (i.e., day one). \n\nThe optimal policy   per se   specifies the action to\nchoose and thus does not allow for variability in the decision process\n(i.e., in some cases waiting and foraging entail large value differences\nwhereas in other cases the two choice options have quite similar values). We\ntherefore used the continuous value differences between the two choice\noptions as predictors of participants\u2019 choices, RTs, and fMRI data.\nFor brevity, we often use the term \u201coptimal policy\u201d to refer\nto the value differences between the foraging and waiting under the optimal\npolicy. \n\nThe optimal policy applies to the scenario as instructed and\nincentivized in our experiment. It is a possibility that participants may\nactually have tried to solve different scenarios. We therefore tested four\npolicies that are optimal under different scenarios. We refer to these\npolicies as \u201cpseudo-optimal.\u201d One pseudo-optimal policy just\nconsiders a finite time horizon of one time step. A second pseudo-optimal\npolicy neglects the transitions according to the predator probability (i.e.,\nthis probability is set to zero for calculating the corresponding\npseudo-optimal policy). A third pseudo-optimal policy neglects the\ntransitions according to the probability of foraging gain. For a fourth\npseudo-optimal policy, which was added during the revision process, optimal\npolicies were averaged for one to five days according to the number of times\nthese horizons actually occurred in the main experiment (i.e., an average of\n2.5 days, which was implemented to enhance fMRI design efficiency). \n\nAll calculations were carried out in MATLAB. \n\n\n#### Choice uncertainties and discrepancy \n  \nWe conjectured that choice uncertainties of the employed DVs and\npossibly the discrepancy between the DVs may be reflected in RT and fMRI\ndata. We quantified uncertainties on the basis of the derivative of the mean\nacross participants of the fitted logistic functions ( ). These derivatives capture the intuition\nthat a small deviation in the DV (e.g., due to perceptual or computational\nerror) has a large impact on the resulting choice at the inflection point of\nthe logistic function (i.e., at the point at which participants are\nindifferent between the two choice options) than at the ranges of the\ndecision variable where the values of the logistic function are close to\nzero (prescribing waiting) or close to one (prescribing foraging).\nAdditionally, to quantify the discrepancy in the prescriptions of the two\nemployed DVs, we took the absolute differences in the mean of the two fitted\nlogistic functions across participants. \n\n\n\n### Analyses and models of choice and reaction time data \n  \nParticipants received a total of 400 trials, i.e., days (except for two\nparticipants who only received 360 trials due to equipment malfunction). On\naverage, participants were \u201calive\u201d\u2014and were thus able to\nmake decisions\u2014in 354.4 \u00b1 16.1 trials, mean \u00b1 SD, (fMRI\nsample) and 354.4 \u00b1 21.9 trials (behavioural sample). Of these, they\nfailed to answer in 8.4 \u00b1 8.8 trials (fMRI sample) and 5.4 \u00b1 8.3\ntrials (behavioural sample), which left 346.0 \u00b1 17.6 trials (fMRI sample)\nand 349.0 \u00b1 27.6 trials (behavioural sample) for analyses. \n\nTo explain participants\u2019 choices, i.e., their probability of\nchoosing the foraging option, p , we used logistic regression\nmodels (implemented in the MATLAB function   mnrfit  ) of the\nfollowing generic form:   with the following form of the decision variable\n(DV):  \n\nAs   predictor  , we first considered the 16 variables\nlisted in  . In the same vein, we\ntested models including two predictors:   We also tested interaction models:  \n\nFor each model we approximated model evidence by calculating the\nBayesian Information Criterion (BIC), which penalizes model complexity:\n  where SDR is the sum of deviance residuals, i.e.,\ntwice the difference between the maximum achievable log likelihood and that\nattained under the fitted model as given by   mnrfit  , k is the\ntotal number of predictors including the intercept, and n is the number of data\npoints per participant. \n\nWe performed both fixed-effects and random-effects analyses. The latter\nassume that different participants may use different models. We used the\nBayesian model selection procedure implemented in SPM12 ( ) to calculate protected\nexceedance probabilities, which measure the likelihood that any given model is\nmore frequent than all other models in the comparison set  . For the Bayesian model selection procedure, BIC values\nwere multiplied by -0.5 to scale values with respect to the appropriate\nconventions  , . \n\nWe analysed log-transformed RTs using linear mixed effects models as\nimplemented in the R package lmer ( )  . Random effects for participants\nincluded a random intercept and random slopes for all variables. Since models of\nchoice data did not provide evidence for the suitability of interaction terms,\nwe did not include any interaction terms as fixed- or random-effects in the\nmodels of RT data. P-values and degrees of freedom were derived using the R\npackage lmerTest ( ).\nLog-likelihood differences were calculated between the models including all\nfixed effects relative to the models without the respective fixed effect (but\nwith the same random-effects structure). \n\n\n### FMRI data acquisition \n  \nData were acquired on a 3 T (Philips Achieva, Best, The Netherlands) MR\nscanner using a 32-channel head coil. Functional images were recorded using a\nT2*-weighted echo-planar imaging (EPI) sequence (TR 2.1 s; TE 30 ms; flip angle\n80\u00b0). A total of 37 axial slices were sampled for whole brain coverage\n(matrix size 96 \u00d7 96; in-plane resolution 2.5 \u00d7 2.5\nmm ; slice thickness 2.8 mm; 0.5 mm gap between slices; slice tilt\n0\u00b0). Functional images were collected in ten sessions of 170 volumes\neach. To obtain steady-state longitudinal magnetization, the first five volumes\nof each session were discarded. Field maps were acquired with a double echo\ngradient echo field map sequence, using 32 slices covering the whole head (TR\n349.11 ms; TE 4.099 and 7.099 ms; matrix size, 80 \u00d7 80; in-plane\nresolution 3 \u00d7 3 mm ; slice thickness 3 mm; 0.5 mm gap between\nslices; slice tilt 0\u00b0). Anatomical images were acquired using a\nT1-weighted scan (FoV 255 \u00d7 255 \u00d7 180 mm; voxel size 1 \u00d7 1\n\u00d7 1 mm ). \n\n\n### FMRI data analyses \n  \nAll fMRI analyses were performed in SPM12. The FieldMap toolbox was used\nto correct for geometric distortions caused by susceptibility-induced field\ninhomogeneities  . Preprocessing of EPI\ndata included rigid-body realignment to correct for head movement, unwarping,\nand slice time correction. EPI images were then coregistered to the\nindividual\u2019s T1 weighted image using a 12-parameter affine transformation\nand normalized to the Montreal Neurological Institute (MNI) T1 reference brain\ntemplate using the extended unified segmentation algorithm in SPM12  . Normalized images were smoothed with an\nisotropic 8 mm full width at half-maximum Gaussian kernel. The six motion\ncorrection parameters estimated from the realignment procedure were entered as\ncovariates of no interest. Regressors were convolved with the canonical HRF and\nlow frequency drifts were excluded using a high-pass filter with a 128 s\ncut-off. \n\nIn the general linear model (GLM) the three distinct phases of the task\n(forest, choice, and outcome phases; see  ) were entered as events with a duration of 0 s (i.e., as stick\nfunctions). Choice and outcome phases in which participants had starved or for\nwhich they did not reply were not explicitly modelled. We were mostly interested\nin the choice phase and ran a primary GLM with a combination of variables that\nemerged in our analyses of behavioural and RT data (see   and  ). That is, our primary GLM included the six parametric modulators of\nthe choice phase: (1) the predator probability, (2) the DV under the optimal\npolicy, (3&4) the associated choice uncertainties of the former two\nmetrics, (5) the discrepancy between these two metrics, and (6) log-transformed\nRTs. We report analyses in which parametric modulators competed for variance\n(i.e., without serial orthogonalization). We also ran five separate GLMs with\nserial orthogonalization such that each of the five parametric modulators was\nentered last in one GLM. These GLMs revealed the same clusters and same values\nas our primary GLM except for some numerical differences due to rounding. Due to\ncollinearities between parametric modulators, there may be some BOLD activity\nthat could be explained by several parametric modulators, which is not reported\nhere, since we were interested in the specific effects of the respective\nparametric modulators. The forest phase was parametrically modulated by the\ncurrent energy state. The outcome phase was parametrically modulated by the\nchange in energy state (i.e., energy state at outcome minus energy state at\nchoice). \n\nIn a secondary GLM, participants\u2019 choices were included as an\nadditional parametric modulator (i.e., as a binary modulator coding for waiting\nor foraging; see  ). In a tertiary GLM, choice uncertainties and the\ndiscrepancy were calculated from the independent behavioural sample (see  ).\nFor follow-up region of interest (ROI) analyses, we set up a GLM with four\nseparate onset regressors for the four levels of predator probability ( ) and three GLMs on the basis of the\nprimary GLM, in which we additionally entered one of the three next-best DVs\nfrom the behavioural model comparison as parametric modulators (i.e., the\nprobability of foraging gain, continuous energy, and expected energy change; see\n  for the respective results). In two further GLMs, we added either\nthe interaction of predator probability with the choice uncertainty of predator\nprobability or the interaction of the optimal policy with the choice uncertainty\nof the optimal policy as parametric modulators ( ). \n\nWe performed a second-level one-sample t-test (one-sided) on contrast\nimages from all participants. All reported clusters are family-wise error (FWE)\ncorrected for multiple comparisons at p < 0.05 using the SPM random field\ntheory based approach. The cluster-defining threshold was p < 0.001. At\nthis voxel-inclusion threshold the random-field theory approach in SPM correctly\ncontrols the false positive rate  . ROI\nanalyses were conducted using the toolboxes rfxplot and marsbar. \n\n\n\n## Supplementary Material \n  \n \n", "metadata": {"pmcid": 6629544, "text_md5": "08bf8eff6d60e1811d1def6e5fe39764", "field_positions": {"authors": [0, 39], "journal": [40, 53], "publication_year": [55, 59], "title": [70, 172], "keywords": [186, 186], "abstract": [199, 1348], "body": [1357, 53553]}, "batch": 1, "pmid": 31110338, "doi": "10.1038/s41562-019-0603-9", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6629544", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6629544"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6629544\">6629544</a>", "list_title": "PMC6629544  Minimizing threat via heuristic and optimal policies recruits\nhippocampus and medial prefrontal cortex"}
