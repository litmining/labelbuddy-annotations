{"text": "Phillipou, Andrea and Abel, Larry A. and Castle, David J. and Hughes, Matthew E. and Gurvich, Caroline and Nibbs, Richard G. and Rossell, Susan L.\nFront Psychol, 2015\n\n# Title\n\nSelf perception and facial emotion perception of others in anorexia nervosa\n\n# Keywords\n\neating disorders\nfMRI\neyetracking\neye movements\naffect\n\n\n# Abstract\n \n Background:   Whether individuals with anorexia nervosa (AN) are able to accurately perceive emotions from faces of others is unclear. Furthermore, whether individuals with AN process images of their own face differently to healthy individuals has thus far not been investigated. Therefore, the aim of this study was to investigate facial affect processing and the processing of one\u2019s own face through measures of emotion identification, functional magnetic resonance imaging (fMRI) and eyetracking. \n\n Methods:   Twenty-four females with AN and 25 matched healthy control participants were presented with an implicit emotion processing task during fMRI and eyetracking, followed by an explicit emotion identification task. \n\n Results:   The AN group were found to \u2018hyperscan\u2019 stimuli and avoided visually attending to salient features of their own face images. Results of the fMRI revealed increased activity to own face stimuli in AN in the right inferior and middle temporal gyri, and right lingual gyrus. AN participants were not found to display emotion identification deficits to the standard emotional face stimuli. \n\n Discussion:   The findings are discussed in terms of increased anxiety to disorder-relevant stimuli in AN. Potential clinical implications are discussed in relation to the use of eyetracking techniques to improve the perception of self in AN. \n \n\n# Body\n \n## Introduction \n  \nAnorexia nervosa (AN) is a psychiatric condition characterized by a significantly low body weight, a fear of weight gain and a disturbance in the experience of one\u2019s own body weight or shape ( ). A common pathognomonic psychological factor in AN is the disturbance of body image ( ). AN also significantly overlaps with anxiety disorders in terms of symptoms and phenomenology, particularly obsessive compulsive disorder ( ;  ). Additionally, AN has long been associated with deficits in the perception of emotion; in her pioneering work,   observed a marked deficiency in the description of feelings and emotional responses in patients with AN. Later research has linked these deficiencies to the construct of alexithymia, which is defined as a difficulty in identifying and describing subjective feelings, a difficulty in distinguishing between feelings and the bodily sensations of emotional arousal, an externally oriented cognitive style, and a lack of imaginal capacity and fantasy ( ). Increased rates of alexithymia have been consistently reported in AN ( ;  ;  ), though whether individuals with AN have difficulty in the processing of other people\u2019s emotions has not been significantly elucidated. \n\nA number of studies have reported that AN patients perform poorly on tasks probing human face emotion identification (also referred to as \u2018facial affect processing\u2019). Some authors have reported that this deficit was not specific to any particular emotion ( ), while others have found deficits that are specific to the identification of surprise ( ;  ), sadness ( ;  ), disgust ( ), and fear ( ). In only one study of which we are aware, the authors did not find any emotion identification deficit in AN ( ). The consistency of these findings has prompted researchers to explore the neurobiological basis of this impairment in AN using neuroimaging techniques. \n\nTo date, only two published functional magnetic resonance imaging (fMRI) studies have investigated facial affect processing in AN.   found that AN patients exhibited increased blood-oxygen level dependent (BOLD) activation in the right fusiform gyrus in response to mildly happy, prototypically happy, and neutral human face expressions compared to controls. In contrast,   did not find any group differences with their paradigm that used fearful and happy human face expressions. However, the findings of these studies have limited utility as neither contrasted the affect images with neutral affect images to control for differential between group face processing   per se  ;   contrasted the emotions to a baseline (fixation cross) and   contrasted fearful and happy conditions with one another. \n\nAnother related set of studies have analyzed visual scanpaths and have revealed critical information for understanding face emotion processing in a range of psychiatric conditions. During face stimulus processing, healthy individuals focus on salient features such as the eyes, nose, and mouth ( ), whereas patients with psychiatric conditions such as schizophrenia and autism spectrum disorder, show reduced visual attention to these features ( ;  ). Furthermore, although poorer attention to salient features has been found in psychiatric populations such as schizophrenia during implicit emotion processing tasks, typical attention to salient features has been found when explicit task instructions are given ( ). Individuals with psychiatric conditions also show different scanpath strategies: those with anxiety disorders such as social anxiety disorder \u2018hyperscan\u2019 (increased scanpath lengths with fixations of shorter duration) face stimuli ( ), whereas those with schizophrenia show a restricted scanpath of fewer fixations of longer duration, and reduced scanpath lengths ( ). Whether individuals with AN also exhibit discrepant visual scanpath behavior when viewing face stimuli relative to healthy individuals, has thus far not been thoroughly investigated. During a free-viewing task where participants were presented with whole body stimuli and face stimuli,   reported reduced attention to the eye region of face stimuli, and less time visually attending face regions when whole bodies were presented in AN. However, the findings of that study are limited as no attempt was made to standardize the stimuli acquired from a dating website.  , on the other hand, presented participants images of their own bodies photographed in a black leotard and reported that while healthy controls spent relatively the same amount of time focusing on four interest areas (face, chest, abdomen, and legs), individuals with AN spent more time looking at the their legs and abdomen and less time looking at the face, suggesting an avoidance of fixating one\u2019s own face in AN. \n\nThe aim of the current study was to investigate the processing of emotional faces of others, and faces of self, in AN. Participants performed an implicit emotion processing task that involved gender identification of stimuli while undergoing fMRI and eyetracking, and an explicit emotion identification task outside the scanner during eyetracking. Functional imaging studies of emotional face perception are typically performed as implicit tasks, such as gender decision tasks, rather than explicit tasks as they are less cognitively demanding and do not interfere with emotional processing. As explicit tasks have a higher cognitive demand, more frontal areas are involved and it is more difficult to observe the areas involved in emotion processing ( ;  ). Performing the task both implicitly and explicitly also allowed for the investigation of emotion perception and scanpaths under different conditions. As scanning behaviors have not previously been specifically investigated in AN, individuals with AN were expected to exhibit similar scanning behaviors to related conditions such as anxiety disorders, namely, hyperscanning of face stimuli during both tasks (i.e., increased fixations of shorter duration). Similarly to other psychiatric conditions, we further hypothesized that AN participants would show an avoidance of salient features of the emotional face stimuli during the implicit task. As typical attention to salient features has been found when explicit task instructions are given in populations that demonstrate poor attention to salient features during implicit tasks ( ), groups were not expected to differ in areas of attentional focus during the explicit task. In relation to participants\u2019 own face stimuli and given the avoidance of looking at one\u2019s own face as reported by  , the AN group were hypothesized to show less attention to salient features during both tasks. Individuals with AN were also expected to show emotion identification deficits to face stimuli displaying negative emotion, but were not expected to show emotion recognition difficulties to their own face when they were asked to display a neutral expression. Related to this hypothesis, the AN group were expected to manifest reduced activity in limbic areas of the brain to negative affect stimuli, relative to neutral control faces; and to show reduced activity to stimuli of their own face in frontoparietal brain areas which are involved in the processing of ones\u2019 own face ( ). \n\n\n## Materials and Methods \n  \nThis study was approved by the human research ethics departments at The University of Melbourne, Swinburne University of Technology, The Melbourne Clinic, The Austin Hospital, and St Vincent\u2019s Hospital; all in Melbourne, VIC, Australia. Informed written consent was obtained from all participants. All procedures contributing to this work comply with the ethical standards of the relevant national and institutional committees on human experimentation and with the Helsinki Declaration of 1975, as revised in 2008. \n\n### Participants \n  \nTwenty-four right-handed individuals with AN and 25 healthy control (HC) individuals were recruited for the study. One participant in the AN group declined to have her own face included among the stimuli and her data were not included in the analysis. A programming error resulted in one HC participant being presented with too few own face stimuli and her data were excluded. Technical difficulties encountered with use of the eyetracking equipment in the MRI resulted in the data of three AN participants and three HC participants being excluded, allowing eyetracking analyses to be conducted on 20 AN and 21 HC participants. All 23 AN and 24 HC participants\u2019 data were included in the fMRI and emotion identification analyses. \n\nHCs were recruited through public advertisements, whereas AN participants were recruited through public advertisements, the Body Image and Eating Disorders Treatment and Recovery Service at the Austin and St Vincent\u2019s Hospitals, and The Melbourne Clinic (all located in Melbourne, VIC, Australia). All participants were English speaking, had no history of significant brain injury or neurological condition, no significant ocular pathology and normal (or corrected to normal) visual acuity. Controls were required to have no history of an eating disorder or other mental illness; they were also required to not be taking any medications apart from hormonal contraceptives (11 HC participants were taking this medication). AN participants were instructed to continue with their normal medications, which were: selective serotonin reuptake inhibitors (SSRIs) (10), atypical antipsychotics (10), benzodiazepines (5), serotonin-noradrenaline reuptake inhibitors (SNRIs) (3), hormonal contraceptives (3), melatonergic antidepressants (2), noradrenergic and specific serotonergic antidepressant (NaSSA) (1), and cyclopyrrolones (1). Medications that patients were taking, such as benzodiazepines and atypical antipsychotics have been found to moderately reduce saccadic peak velocity ( ), but do not affect scanpaths to the best of our knowledge. \n\nThe Mini International Neuropsychiatric Interview, 5.0.0 (MINI;  ) was used to screen participants for major Axis I psychiatric disorders according to the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV). It was also used to confirm diagnoses of AN, with the exception of the amenorrhea criterion which is not included in DSM-5 criteria. AN was required to be the primary diagnosis of the AN group; participants with comorbid psychiatric conditions, other than psychotic conditions, were not excluded as this would not have represented a typical AN sample. \n\nPremorbid intelligence was estimated using the Wechsler Test of Adult Reading (WTAR;  ). Eating disorder symptomatology was investigated with the Eating Disorders Examination Questionnaire (EDE-Q;  ) and alexithymia with the Toronto Alexithymia Scale (TAS-20;  ) (  Table   ). \n  \nParticipant information. \n    \n\n### Task \n  \nParticipants were presented with face stimuli from a standard set of black-and-white images, the Pictures of Facial Affect ( ). The stimuli consist of male and female images displaying the seven basic emotions: anger, disgust, fear, happiness, sadness, surprise, and neutral. The stimuli chosen were those with the highest inter-rater agreement for all seven emotions. Four male and four female models displaying each emotion were presented. Participants were first presented with an implicit task while undergoing fMRI and eyetracking, followed by the explicit task which involved emotion identification and eyetracking. An implicit task was undertaken during fMRI as explicit tasks have a higher cognitive demand and more frontal areas involvement, making it more difficult to observe activity in areas involved in emotion processing ( ;  ). In the implicit task, participants were presented with each stimulus twice in a pseudorandom fashion, resulting in a total of 16 presentations of each emotion over two runs. In the explicit task, participants were shown each stimulus once, resulting in eight presentations per emotion over one run. Due to the extended length of the task in the MRI, participants were not presented with surprised faces in the implicit task as it is the most ambiguous emotion, having neither a positive or negative emotional valence. Participants were also pseudorandomly presented with a black-and-white image of their own face with a neutral expression, with 16 presentations during the implicit task and eight during the explicit task. Photographs of participants were taken by the researcher while participants were instructed to look straight ahead with a relaxed expression, similarly to a passport photograph. Participants\u2019 own face photographs were edited to match the properties of the Ekman face stimuli in terms of size and resolution, and were made black-and-white. \n\nIn the implicit task, faces were displayed pseudorandomly across emotions for 8000 ms on a white background, followed by a black 1\u00b0 fixation cross for 3000\u20134300 ms. Each photograph was 336 \u00d7 640 pixels, equalling 17 cm \u00d7 27.5 cm or 18 \u00d7 13\u00b0 at the eye. Participants were instructed simply to look at the face while it was on screen and to make a gender response with their right hand by clicking one of two buttons only when the fixation cross appeared following the face presentation. Long periods of fixation, between 10200 and 11400 ms were presented pseudorandomly throughout the task to increase BOLD signal variance by allowing the signal to return to baseline. Each of the two runs also began with a long period of fixation for 15000 ms. \n\nIn the explicit task, faces were again displayed for 8000 ms on a white background, equalling to 8 \u00d7 13\u00b0 to the eye. Prior to the presentation of each stimulus, a 1\u00b0 fixation cross appeared in the center of the screen. Following the presentation of each face stimulus, a forced-choice screen appeared on the monitor asking participants to identify the emotion displayed in the previous face from a list containing all of the emotions. The participants were given as much time as they required to make a response. \n\n\n### Data Acquisition and Analysis \n  \n#### Eyetracking \n  \nStimuli were presented through SR Research\u2019s Experiment Builder program, and eyetracking was recorded using a remote view eyetracker, the EyeLink1000 (SR Research, Mississauga, ON, Canada), monocularly at 500 Hz. The recorded data were analyzed with SR Research\u2019s analysis program, DataViewer. Areas of interest (AOIs) were defined as the eyes, nose and mouth. To investigate the proportion of fixations and fixation durations to salient features and non-salient features, two spatial-temporal parameters were calculated: the feature fixation index (FFI) and the feature duration index (FDI;  ). The FFI is derived by dividing the number of fixations to salient features minus the number of fixations to non-salient features by the total number of fixations. The FDI is derived in the same manner. Indices range from -1 to +1, with positive values indicating more fixations or longer durations to salient features, and negative values indicating more visual attention to non-salient features. \n\n\n#### Functional Magnetic Resonance Imaging \n  \nMagnetic resonance imaging scans were undertaken with the Siemens Tim Trio 3 tesla system with a 32 channel head coil at Swinburne University of Technology (Melbourne, VIC, Australia). During each functional run of active task performance, 1080 T2 -weighted images were acquired axially parallel to the AC\u2013PC line using an interleaved multiband sequence (multiband acceleration factor = 4, bandwidth = 25988 Hz/Px, repetition time (TR) = 710 ms, echo-time (TE) = 30 ms, echo-spacing = 0.51 ms, flip-angle = 52\u00b0, field of view = 222 mm, voxel resolution = 3 mm \u00d7 3 mm \u00d7 3 mm, slice thickness = 3 mm, number of slices = 44). Multiband acquisition sequences were derived from the Human Connectome Project ( ). A T1-weighted image was acquired sagitally for anatomical reference (bandwidth = 170 Hz/Px, TR = 1900 ms, TE = 2.52 ms, echo spacing = 7.5 ms, flip angle = 9\u00b0, field-of-view = 350 mm \u00d7 263 mm \u00d7 350 mm, voxel resolution = 1 mm \u00d7 1 mm \u00d7 1 mm, slice thickness = 1 mm). \n\nMagnetic resonance imaging data pre-processing and statistical analyses were performed using SPM8, through Matlab R2014a (Mathworks, Natick, MA, USA). Image pre-processing included image realignment, then coregistration of the T1 image to a mean realigned functional image created during realignment. The co-registered T1 image was normalized to the T1 template supplied with SPM8 Montreal Neuroimaging Institute (MNI), then the parameters of this transformation were applied to realigned functional images. The normalized functional images were spatially smoothed with a Gaussian kernel of 8 mm \u00d7 8 mm \u00d7 8 mm. \n\n\n#### Statistical Analyses \n  \nFirst-level modeling was performed by fitting a convolved hemodynamic response function (HRF) and its temporal derivative separately to the onset times of angry, disgusted, fearful, happy, sad, neutral, and own faces (seven regressors plus their temporal derivative). After parameter estimation, each emotion parameter and the participants\u2019 own face parameter was contrasted with the neutral face parameter producing six contrast images (angry > neutral, sad > neutral, etc.). At the group level, these contrast images were first entered into one-way analysis of variance (ANOVA) models for AN and HC groups separately to investigate within-group effects (results are presented in Supplementary Material). Group differences were interrogated with a mixed-effects ANOVA model using the flexible factorial option in SPM8. This model included a between-subjects   group   factor (two levels: patients vs. controls), a within subjects   condition   factor (six levels: angry > neutral, sad > neutral, etc.) and a   subject  s factor (number of levels equals the number of participants) that controlled for within-subject variability ( ). \n\n T  -statistic images were corrected for multiple comparisons using the random-field theory approach at the voxel and cluster levels (  p   < 0.05, FWE-corrected). The mixed-design analysis involved the investigation of a group by condition interaction, followed by simple effects comparing each condition between groups. \n\nFollowing the mixed-design analysis, clusters which resulted in significant differences between groups were defined as different regions of interests (ROIs) with the MarsBar toolbox ( ) run under Matlab R2014a. Contrast estimates for each ROI were correlated with eyetracking and behavioral data, and scores on the EDE-Q and TAS-20. \n\nPerformance on eyetracking components and emotion identification (rate of emotion identification errors) were compared with mixed design ANOVAs, following normality checking and the removal of outliers. Percentage data underwent an arcsine transformation prior to inclusion in ANOVAs. Violations of sphericity were corrected with a Greenhouse\u2013Geiser correction. For conditions in which groups significantly differed in emotion identification error rate, Mann\u2013Whitney   U   tests were carried out to identify which emotions were incorrectly reported as the data were not normally distributed. Pearson\u2019s correlation analyses were also performed between eyetracking data and behavioral data, and scores on the EDE-Q and TAS-20. For brevity, only significant interactions with group will be commented on in detail. Detailed results of eyetracking and fMRI analyses unrelated to group interactions are available as Supplementary Material. \n\n\n\n\n## Results \n  \n### Behavioral \n  \nFor rate of emotion identification errors, a 2 (group) \u00d7 8 (condition) mixed design ANOVA was undertaken (see Supplementary Table  ). The analysis revealed a significant main effect of condition [  F  (2.5,113.5) = 7.4,   p   < 0.001]. A significant main effect of group [  F  (1,46) = 5.2,   p   \u2264 0.05] and a significant interaction between group and condition were also found [  F  (2.5,113.5) = 4.6,   p   \u2264 0.01]. Within subjects contrasts revealed that groups did not significantly differ in emotion identification errors to any individual emotion, but AN participants made significantly more emotion identification errors to their own face [  F  (1,46) = 5.1,   p   \u2264 0.05]. When errors were made to own face emotion, analyses revealed that AN participants were more likely than controls to report their own face as sad [  U  (46) = 200.0, Z = -2.9,   p   \u2264 0.01], whereas control participants were more likely to correctly report their own neutral face as portraying a neutral expression [  U  (46) = 209.0,   Z   = -2.3,   p   \u2264 0.05] (see Supplementary Table  ). \n\n\n### Eyetracking \n  \nAverage fixation count, fixation duration, and saccade amplitude were analyzed in separate 2 (group) \u00d7 7 (condition) \u00d7 2 (task) mixed design ANOVAs. As the first run of the fMRI task consisted of the same number of trials as the behavioral task, these two tasks were included in the analysis. Furthermore, as the behavioral task also consisted of surprised faces which were not included in the fMRI task, these trials were excluded from the analysis. Means and SD for fixation count, fixation duration and saccade amplitude are presented in see Supplementary Table  . \n\nFor fixation count, a significant main effect of condition [  F  (4.1,159.2) = 3.0,   p   \u2264 0.05] was found with a greater number of fixations made to participants\u2019 own faces and faces depicting anger and fear. A significant main effect was also found for group [  F  (1,39) = 5.3,   p   \u2264 0.05], with AN participants making more fixations than controls. A significant interaction between condition and task was also found [  F  (3.4,131.9) = 7.61,   p   < 0.001] with a decreased number of fixations between implicit and explicit tasks for participants\u2019 own faces and faces depicting fear. There was, however, no significant main effect of task, and no significant interaction between condition and group, or task and group. There was also no interaction between condition, group and task. Analyses conducted on fixation duration revealed a significant main effect of group [  F  (1,38) = 8.5,   p   \u2264 0.01] with AN participants making fixations of shorter duration than heathy individuals. No other significant main effects or interactions were found. Analyses conducted on saccade amplitude resulted in no significant main effects or interactions. \n\nA 2 (group) \u00d7 7 (condition) \u00d7 2 (task) mixed design ANOVA conducted on the FDI revealed a significant main effect of condition [  F  (2.3,82.0) = 16.4,   p   < 0.001], and a significant interaction between condition and task [  F  (4.0,144.9) = 3.0,   p   \u2264 0.05] with greater attention to salient features of one\u2019s own face during the implicit compared to explicit task. Analyses conducted on the FFI revealed significant main effects of condition [  F  (2.2,72.5) = 19.7,   p   < 0.001] and task [  F  (1,33) = 10.1,   p   \u2264 0.01] with greater attention to salient facial features during the explicit task compared to the implicit task. Significant interactions were also found between condition and task [  F  (3.8,125.9) = 3.1,   p   \u2264 0.05] and condition and group [  F  (2.2,72.5) = 3.2,   p   \u2264 0.05]. Within-subjects contrasts did not reveal any significant differences between groups for any emotion, but a significant difference for own faces between AN and control participants [  F  (1,33) = 5.9,   p   \u2264 0.05]. Further 2 (group) \u00d7 2 (task) mixed design ANOVAs were also conducted on the FFI and FDI to participants\u2019 own face. A significant main effect of task, and an interaction between task and group were not found for either the FFI or FDI. A significant main effect of group was, however, found for both the FFI [  F  (1,36) = 7.6,   p   > 0.01] and FDI [  F  (1,36) = 6.8,   p   \u2264 0.05] (see Supplementary Table  ). Control participants showed more visual attention to salient features of their own face, whereas the attention shown to salient and non-salient features of their own face in AN was roughly equal. \n\n\n### Functional Magnetic Resonance Imaging \n  \n#### Mixed Design Analysis \n  \nThe analysis did not result in a significant group by condition interaction. Simple effects between groups for each condition revealed a significant difference between groups only for the own > neutral face contrast. Increased activation was found in AN compared to controls in the own > neutral contrast in two clusters: one in the right inferior temporal and middle temporal gyri, and one in the right lingual gyrus (  Table   ;   Figure   ). \n  \n Increased activity in the anorexia nervosa (AN) group compared to the control group in the right inferior and middle temporal gyri   (A)  , and the right lingual gyrus   (B)   for participants\u2019 own faces compared to neutral faces (contrast: AN > controls, own > neutral face; FWE-corrected for multiple comparisons at the voxel and cluster levels).   The color scale indicates the   t  -value. \n    \nSignificant activations for participants\u2019 own faces compared to neutral faces between anorexia nervosa (AN) and control participants, Own > Neutral, AN > Healthy Controls. \n    \n\n\n### Pearson\u2019s Correlations \n  \nBOLD activity in the own > neutral face contrast in either the inferior and middle temporal gyri ROI, or the lingual gyrus ROI did not significantly correlate with any eyetracking parameter, rate of emotion identification errors, or the results of the EDE-Q and TAS-20 for either group. Eyetracking and behavioral data, and scores on the EDE-Q and TAS-20 were also not found to correlate with one another. \n\n\n\n## Discussion \n  \nThe aim of this study was to investigate own face processing and the processing of emotional faces of others, in individuals with AN. Although the AN group were found to have higher alexithymia scores, they did not differ from healthy controls in emotion identification of the Ekman emotional face stimuli. Groups were found to differ in visual scanpath behavior to the stimuli in general, with AN participants demonstrating hyperscanning, evinced by increased fixations of shorter duration, relative to controls. AN participants were also found to avoid visually attending to salient features of their own face and displayed increased activity in the right lingual, and inferior and middle temporal gyri to images of their own face, compared to neutral control images, as well as to healthy individuals. \n\nIn analyses directly comparing AN to healthy individuals, AN participants showed increased activity to their own face in the right inferior and middle temporal gyri, and lingual gyrus, areas related to higher-order visual perception. Increased activity of the lingual gyrus has been reported during the processing of human faces ( ), and increased activity in the inferior and middle temporal gyri have been specifically found in response to own face stimuli ( ;  ;  ). Therefore, the findings of the current study suggest an increased processing of own face stimuli in AN in areas related to visual perception of self. \n\nDifferences in lingual gyrus and temporal gyrus activity have also been reported when individuals with AN are presented with images of their own body compared to other individuals\u2019 bodies. However, increased activity in these areas was found for controls relative to AN participants ( ;  ), rather than increased activity in AN as found in the current study. However, neither of those studies actually involved face processing; rather, the faces of the stimuli presented in those studies were masked when presented to participants. Therefore, the differences in activation between the current findings and the findings of these two studies may be related to the specific processing of self-face and self-body images which result in increased activity and decreased activity in these areas in AN respectively. \n\nOur AN group also showed more visual attention to non-salient features and avoided fixating on salient features of their own face.   reported that when AN participants were presented with whole body images of themselves, less visual attention was allocated to their faces compared to controls, though the level of visual attention to different body areas did not differ. Furthermore,   reported a similar pattern of visual attention in a study which presented AN and control participants with food and non-food images simultaneously: although healthy individuals showed more visual attention to food stimuli, the AN group displayed roughly equal attention to food and non-food stimuli. These results may be related to the finding that decreased visual attention is associated with the presentation of anxiety-inducing and phobic stimuli. Similarly to the current findings,   reported that individuals with social anxiety disorder made fixations of short duration to salient features of emotional faces stimuli.  , reported that individuals with spider phobia displayed fewer fixations to spider images and instead diverted their attention to neutral areas of the stimulus. The authors described this behavior as a strategy to cope with threatening and confrontational stimuli, to consequently reduce anxiety. Individuals with AN may utilize similar strategies when viewing their own face as they may find these stimuli anxiety-provoking. An avoidance of salient features was not, however, found to the Ekman face stimuli, suggesting that this behavior is specific to one\u2019s own face. \n\nHyperscanning of face stimuli (increased fixations of shorter duration) was also found in our AN group, though this was not specific to self-face images. Hyperscanning behaviors are associated with increased anxiety, as has been reported in social anxiety disorder to face stimuli of different emotions ( ).   suggested the hyperscanning behavior in social anxiety disorder may reflect a fear of social evaluation. This explanation may also be relevant to the current findings: due to a preoccupation with physical appearance, individuals with AN are particularly sensitive to social evaluations made by others ( ) and may show increased scanning behaviors to related stimuli. \n\nDespite previous reports of emotion identification deficits in individuals with AN, our findings do not concur. Although higher levels of alexithymia were found in the AN group, the ability to identify emotions from a standard set of face stimuli did not differ from healthy individuals. This finding is consistent with the literature which suggests that alexithymia levels are not related to the ability to perceive emotion from faces (see   for a review). However, whether high alexithymia impairs the ability to perceive emotion in an image of one\u2019s own face has not been investigated, nor was it specifically investigated in this study, as we only presented participants with own-face images showing a neutral expression. Although the majority of AN participants reported their own face as portraying a neutral expression, they were more likely than controls to report their own face as depicting a sad expression. However, rather than indicating an emotion identification deficit specific to themselves, this result is more likely to reflect a biased self-perception in how individuals with AN   feel   they look. Thus, when identification errors occurred as they viewed their own faces, they were specific to seeing themselves as sad and not any other emotions. Furthermore, when participants were questioned following the task about their overall experience, the controls often reported that they found it \u2018funny\u2019 looking at their own face, whereas AN patients tended to feel disgusted. \n\nThough AN and control participants differed in the processing of their own face, we found no differences in BOLD activity in response to different emotions, in either group. As strict thresholding was utilized in an attempt to correct for multiple comparisons, the task may not have had sufficient power to document significant differences in activation of each emotion relative to the neutral face condition. However, as the contrasts between participants\u2019 own face and the neutral face condition survived this threshold, the results show that BOLD activity elicited in response to the different emotions was not as strong. Since we were interested in assessing the visual scanpaths of AN patients to face stimuli, the extended presentation time was required for this purpose and the number of trials was therefore limited to remain within a reasonable duration for an MRI task. In future research, where visual scanpaths are not of interest, an increased number of trials should be utilized. It would also be of interest to investigate differences in visual scanpaths and neural activity to both own-face and own-body stimuli in AN, and an evaluation of how these images make AN patients feel. \n\nThe findings of this study have important potential clinical implications. Participants with AN did not differ from healthy control participants in the areas of attentional focus when viewing the Ekman face stimuli, nor did they differ in emotion identification of these stimuli. AN participants did, however, display an avoidance of salient features of images of their own faces. This may have consequently led to the mislabelling of their own neutral expression as sad. In other words, the perception of one\u2019s own face with a neutral expression as appearing sad in AN may be related to patients not looking at the correct areas of their own face when making an affect judgment. This may be as a result of emotional disturbances and alexithymia in AN, or this alternatively may lead to these disturbances. Therefore, remediation techniques which train participants to focus on the correct areas of the face may be beneficial in AN. These techniques have proven useful in other psychiatric conditions such as schizophrenia, with trained individuals demonstrating an improvement in attention to salient facial features and emotion recognition ( ;  ). Furthermore, the majority of deficits reported in the current study were specific to the processing of self-images in AN. This emphasizes the importance of therapies such as cognitive behavioral therapy to address distorted perceptions of oneself in AN and correctly analyzing events and the patient\u2019s own internal dialog. It is also possible that the sense of disgust evoked by viewing their own face also reduced the AN group\u2019s fixations on the salient regions of images of their own faces, in the way that one might avoid eye contact with a repellent individual. \n\nIn summary, this study suggests intact emotion identification of facial affect stimuli and distinct hyperscanning behaviors when viewing faces in AN. Significant differences were also found in the processing of one\u2019s own face in AN, with AN participants showing a greater level of visual attention to non-salient features and increased activity in inferior and middle temporal, and lingual gyri, relative to healthy individuals. These findings suggest overlap with anxiety disorders, as evinced by the hyperscanning behaviors displayed, and increased anxiety, particularly to own face images evinced by an apparent avoidance of salient features. Together with the fMRI finding, the study suggests that the processing of self-face images is different in AN, and may contribute to the distorted perception of oneself experienced by individuals with this illness. \n\n\n## Author Contributions \n  \nAP, LA, SR, CG, and DC designed the research; AP completed data collection; all authors were involved in data analysis and manuscript preparation. \n\n\n## Conflict of Interest Statement \n  \nProf. David J. Castle reports grants and personal fees from Eli Lilly, grants and personal fees from Janssen-Cilag, grants and personal fees from Roche, grants and personal fees from Allergen, grants and personal fees from Bristol-Myer Squibb, grants and personal fees from Pfizer, grants and personal fees from Lundbeck, grants and personal fees from AstraZeneca, grants and personal fees from Hospira, during the conduct of the study; personal fees from Eli Lilly, personal fees from Bristol-Myer Squibb, personal fees from Lundbeck, personal fees from Janssen-Cilag, personal fees from Pfizer, personal fees from Organon, personal fees from Sanofi-Aventis, personal fees from Wyeth, personal fees from Hospira, personal fees from Servier, outside the submitted work. Prof. Larry A. Abel reports personal fees from Actelion Pharmaceuticals, Switzerland, outside the submitted work. Dr. Andrea Phillipou, Prof. Susan L. Rossell, Dr. Caroline Gurvich, Dr. Matthew E. Hughes, and Mr. Richard G. Nibbs report no conflicts of interest. \n\n \n", "metadata": {"pmcid": 4530666, "text_md5": "06b2dd9c2a95c909c31658d4d35e9d6c", "field_positions": {"authors": [0, 146], "journal": [147, 160], "publication_year": [162, 166], "title": [177, 252], "keywords": [266, 321], "abstract": [334, 1708], "body": [1717, 38219]}, "batch": 1, "pmid": 26321993, "doi": "10.3389/fpsyg.2015.01181", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4530666", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4530666"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4530666\">4530666</a>", "list_title": "PMC4530666  Self perception and facial emotion perception of others in anorexia nervosa"}
{"text": "Phillipou, Andrea and Abel, Larry A. and Castle, David J. and Hughes, Matthew E. and Gurvich, Caroline and Nibbs, Richard G. and Rossell, Susan L.\nFront Psychol, 2015\n\n# Title\n\nSelf perception and facial emotion perception of others in anorexia nervosa\n\n# Keywords\n\neating disorders\nfMRI\neyetracking\neye movements\naffect\n\n\n# Abstract\n \n Background:   Whether individuals with anorexia nervosa (AN) are able to accurately perceive emotions from faces of others is unclear. Furthermore, whether individuals with AN process images of their own face differently to healthy individuals has thus far not been investigated. Therefore, the aim of this study was to investigate facial affect processing and the processing of one\u2019s own face through measures of emotion identification, functional magnetic resonance imaging (fMRI) and eyetracking. \n\n Methods:   Twenty-four females with AN and 25 matched healthy control participants were presented with an implicit emotion processing task during fMRI and eyetracking, followed by an explicit emotion identification task. \n\n Results:   The AN group were found to \u2018hyperscan\u2019 stimuli and avoided visually attending to salient features of their own face images. Results of the fMRI revealed increased activity to own face stimuli in AN in the right inferior and middle temporal gyri, and right lingual gyrus. AN participants were not found to display emotion identification deficits to the standard emotional face stimuli. \n\n Discussion:   The findings are discussed in terms of increased anxiety to disorder-relevant stimuli in AN. Potential clinical implications are discussed in relation to the use of eyetracking techniques to improve the perception of self in AN. \n \n\n# Body\n \n## Introduction \n  \nAnorexia nervosa (AN) is a psychiatric condition characterized by a significantly low body weight, a fear of weight gain and a disturbance in the experience of one\u2019s own body weight or shape ( ). A common pathognomonic psychological factor in AN is the disturbance of body image ( ). AN also significantly overlaps with anxiety disorders in terms of symptoms and phenomenology, particularly obsessive compulsive disorder ( ;  ). Additionally, AN has long been associated with deficits in the perception of emotion; in her pioneering work,   observed a marked deficiency in the description of feelings and emotional responses in patients with AN. Later research has linked these deficiencies to the construct of alexithymia, which is defined as a difficulty in identifying and describing subjective feelings, a difficulty in distinguishing between feelings and the bodily sensations of emotional arousal, an externally oriented cognitive style, and a lack of imaginal capacity and fantasy ( ). Increased rates of alexithymia have been consistently reported in AN ( ;  ;  ), though whether individuals with AN have difficulty in the processing of other people\u2019s emotions has not been significantly elucidated. \n\nA number of studies have reported that AN patients perform poorly on tasks probing human face emotion identification (also referred to as \u2018facial affect processing\u2019). Some authors have reported that this deficit was not specific to any particular emotion ( ), while others have found deficits that are specific to the identification of surprise ( ;  ), sadness ( ;  ), disgust ( ), and fear ( ). In only one study of which we are aware, the authors did not find any emotion identification deficit in AN ( ). The consistency of these findings has prompted researchers to explore the neurobiological basis of this impairment in AN using neuroimaging techniques. \n\nTo date, only two published functional magnetic resonance imaging (fMRI) studies have investigated facial affect processing in AN.   found that AN patients exhibited increased blood-oxygen level dependent (BOLD) activation in the right fusiform gyrus in response to mildly happy, prototypically happy, and neutral human face expressions compared to controls. In contrast,   did not find any group differences with their paradigm that used fearful and happy human face expressions. However, the findings of these studies have limited utility as neither contrasted the affect images with neutral affect images to control for differential between group face processing   per se  ;   contrasted the emotions to a baseline (fixation cross) and   contrasted fearful and happy conditions with one another. \n\nAnother related set of studies have analyzed visual scanpaths and have revealed critical information for understanding face emotion processing in a range of psychiatric conditions. During face stimulus processing, healthy individuals focus on salient features such as the eyes, nose, and mouth ( ), whereas patients with psychiatric conditions such as schizophrenia and autism spectrum disorder, show reduced visual attention to these features ( ;  ). Furthermore, although poorer attention to salient features has been found in psychiatric populations such as schizophrenia during implicit emotion processing tasks, typical attention to salient features has been found when explicit task instructions are given ( ). Individuals with psychiatric conditions also show different scanpath strategies: those with anxiety disorders such as social anxiety disorder \u2018hyperscan\u2019 (increased scanpath lengths with fixations of shorter duration) face stimuli ( ), whereas those with schizophrenia show a restricted scanpath of fewer fixations of longer duration, and reduced scanpath lengths ( ). Whether individuals with AN also exhibit discrepant visual scanpath behavior when viewing face stimuli relative to healthy individuals, has thus far not been thoroughly investigated. During a free-viewing task where participants were presented with whole body stimuli and face stimuli,   reported reduced attention to the eye region of face stimuli, and less time visually attending face regions when whole bodies were presented in AN. However, the findings of that study are limited as no attempt was made to standardize the stimuli acquired from a dating website.  , on the other hand, presented participants images of their own bodies photographed in a black leotard and reported that while healthy controls spent relatively the same amount of time focusing on four interest areas (face, chest, abdomen, and legs), individuals with AN spent more time looking at the their legs and abdomen and less time looking at the face, suggesting an avoidance of fixating one\u2019s own face in AN. \n\nThe aim of the current study was to investigate the processing of emotional faces of others, and faces of self, in AN. Participants performed an implicit emotion processing task that involved gender identification of stimuli while undergoing fMRI and eyetracking, and an explicit emotion identification task outside the scanner during eyetracking. Functional imaging studies of emotional face perception are typically performed as implicit tasks, such as gender decision tasks, rather than explicit tasks as they are less cognitively demanding and do not interfere with emotional processing. As explicit tasks have a higher cognitive demand, more frontal areas are involved and it is more difficult to observe the areas involved in emotion processing ( ;  ). Performing the task both implicitly and explicitly also allowed for the investigation of emotion perception and scanpaths under different conditions. As scanning behaviors have not previously been specifically investigated in AN, individuals with AN were expected to exhibit similar scanning behaviors to related conditions such as anxiety disorders, namely, hyperscanning of face stimuli during both tasks (i.e., increased fixations of shorter duration). Similarly to other psychiatric conditions, we further hypothesized that AN participants would show an avoidance of salient features of the emotional face stimuli during the implicit task. As typical attention to salient features has been found when explicit task instructions are given in populations that demonstrate poor attention to salient features during implicit tasks ( ), groups were not expected to differ in areas of attentional focus during the explicit task. In relation to participants\u2019 own face stimuli and given the avoidance of looking at one\u2019s own face as reported by  , the AN group were hypothesized to show less attention to salient features during both tasks. Individuals with AN were also expected to show emotion identification deficits to face stimuli displaying negative emotion, but were not expected to show emotion recognition difficulties to their own face when they were asked to display a neutral expression. Related to this hypothesis, the AN group were expected to manifest reduced activity in limbic areas of the brain to negative affect stimuli, relative to neutral control faces; and to show reduced activity to stimuli of their own face in frontoparietal brain areas which are involved in the processing of ones\u2019 own face ( ). \n\n\n## Materials and Methods \n  \nThis study was approved by the human research ethics departments at The University of Melbourne, Swinburne University of Technology, The Melbourne Clinic, The Austin Hospital, and St Vincent\u2019s Hospital; all in Melbourne, VIC, Australia. Informed written consent was obtained from all participants. All procedures contributing to this work comply with the ethical standards of the relevant national and institutional committees on human experimentation and with the Helsinki Declaration of 1975, as revised in 2008. \n\n### Participants \n  \nTwenty-four right-handed individuals with AN and 25 healthy control (HC) individuals were recruited for the study. One participant in the AN group declined to have her own face included among the stimuli and her data were not included in the analysis. A programming error resulted in one HC participant being presented with too few own face stimuli and her data were excluded. Technical difficulties encountered with use of the eyetracking equipment in the MRI resulted in the data of three AN participants and three HC participants being excluded, allowing eyetracking analyses to be conducted on 20 AN and 21 HC participants. All 23 AN and 24 HC participants\u2019 data were included in the fMRI and emotion identification analyses. \n\nHCs were recruited through public advertisements, whereas AN participants were recruited through public advertisements, the Body Image and Eating Disorders Treatment and Recovery Service at the Austin and St Vincent\u2019s Hospitals, and The Melbourne Clinic (all located in Melbourne, VIC, Australia). All participants were English speaking, had no history of significant brain injury or neurological condition, no significant ocular pathology and normal (or corrected to normal) visual acuity. Controls were required to have no history of an eating disorder or other mental illness; they were also required to not be taking any medications apart from hormonal contraceptives (11 HC participants were taking this medication). AN participants were instructed to continue with their normal medications, which were: selective serotonin reuptake inhibitors (SSRIs) (10), atypical antipsychotics (10), benzodiazepines (5), serotonin-noradrenaline reuptake inhibitors (SNRIs) (3), hormonal contraceptives (3), melatonergic antidepressants (2), noradrenergic and specific serotonergic antidepressant (NaSSA) (1), and cyclopyrrolones (1). Medications that patients were taking, such as benzodiazepines and atypical antipsychotics have been found to moderately reduce saccadic peak velocity ( ), but do not affect scanpaths to the best of our knowledge. \n\nThe Mini International Neuropsychiatric Interview, 5.0.0 (MINI;  ) was used to screen participants for major Axis I psychiatric disorders according to the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV). It was also used to confirm diagnoses of AN, with the exception of the amenorrhea criterion which is not included in DSM-5 criteria. AN was required to be the primary diagnosis of the AN group; participants with comorbid psychiatric conditions, other than psychotic conditions, were not excluded as this would not have represented a typical AN sample. \n\nPremorbid intelligence was estimated using the Wechsler Test of Adult Reading (WTAR;  ). Eating disorder symptomatology was investigated with the Eating Disorders Examination Questionnaire (EDE-Q;  ) and alexithymia with the Toronto Alexithymia Scale (TAS-20;  ) (  Table   ). \n  \nParticipant information. \n    \n\n### Task \n  \nParticipants were presented with face stimuli from a standard set of black-and-white images, the Pictures of Facial Affect ( ). The stimuli consist of male and female images displaying the seven basic emotions: anger, disgust, fear, happiness, sadness, surprise, and neutral. The stimuli chosen were those with the highest inter-rater agreement for all seven emotions. Four male and four female models displaying each emotion were presented. Participants were first presented with an implicit task while undergoing fMRI and eyetracking, followed by the explicit task which involved emotion identification and eyetracking. An implicit task was undertaken during fMRI as explicit tasks have a higher cognitive demand and more frontal areas involvement, making it more difficult to observe activity in areas involved in emotion processing ( ;  ). In the implicit task, participants were presented with each stimulus twice in a pseudorandom fashion, resulting in a total of 16 presentations of each emotion over two runs. In the explicit task, participants were shown each stimulus once, resulting in eight presentations per emotion over one run. Due to the extended length of the task in the MRI, participants were not presented with surprised faces in the implicit task as it is the most ambiguous emotion, having neither a positive or negative emotional valence. Participants were also pseudorandomly presented with a black-and-white image of their own face with a neutral expression, with 16 presentations during the implicit task and eight during the explicit task. Photographs of participants were taken by the researcher while participants were instructed to look straight ahead with a relaxed expression, similarly to a passport photograph. Participants\u2019 own face photographs were edited to match the properties of the Ekman face stimuli in terms of size and resolution, and were made black-and-white. \n\nIn the implicit task, faces were displayed pseudorandomly across emotions for 8000 ms on a white background, followed by a black 1\u00b0 fixation cross for 3000\u20134300 ms. Each photograph was 336 \u00d7 640 pixels, equalling 17 cm \u00d7 27.5 cm or 18 \u00d7 13\u00b0 at the eye. Participants were instructed simply to look at the face while it was on screen and to make a gender response with their right hand by clicking one of two buttons only when the fixation cross appeared following the face presentation. Long periods of fixation, between 10200 and 11400 ms were presented pseudorandomly throughout the task to increase BOLD signal variance by allowing the signal to return to baseline. Each of the two runs also began with a long period of fixation for 15000 ms. \n\nIn the explicit task, faces were again displayed for 8000 ms on a white background, equalling to 8 \u00d7 13\u00b0 to the eye. Prior to the presentation of each stimulus, a 1\u00b0 fixation cross appeared in the center of the screen. Following the presentation of each face stimulus, a forced-choice screen appeared on the monitor asking participants to identify the emotion displayed in the previous face from a list containing all of the emotions. The participants were given as much time as they required to make a response. \n\n\n### Data Acquisition and Analysis \n  \n#### Eyetracking \n  \nStimuli were presented through SR Research\u2019s Experiment Builder program, and eyetracking was recorded using a remote view eyetracker, the EyeLink1000 (SR Research, Mississauga, ON, Canada), monocularly at 500 Hz. The recorded data were analyzed with SR Research\u2019s analysis program, DataViewer. Areas of interest (AOIs) were defined as the eyes, nose and mouth. To investigate the proportion of fixations and fixation durations to salient features and non-salient features, two spatial-temporal parameters were calculated: the feature fixation index (FFI) and the feature duration index (FDI;  ). The FFI is derived by dividing the number of fixations to salient features minus the number of fixations to non-salient features by the total number of fixations. The FDI is derived in the same manner. Indices range from -1 to +1, with positive values indicating more fixations or longer durations to salient features, and negative values indicating more visual attention to non-salient features. \n\n\n#### Functional Magnetic Resonance Imaging \n  \nMagnetic resonance imaging scans were undertaken with the Siemens Tim Trio 3 tesla system with a 32 channel head coil at Swinburne University of Technology (Melbourne, VIC, Australia). During each functional run of active task performance, 1080 T2 -weighted images were acquired axially parallel to the AC\u2013PC line using an interleaved multiband sequence (multiband acceleration factor = 4, bandwidth = 25988 Hz/Px, repetition time (TR) = 710 ms, echo-time (TE) = 30 ms, echo-spacing = 0.51 ms, flip-angle = 52\u00b0, field of view = 222 mm, voxel resolution = 3 mm \u00d7 3 mm \u00d7 3 mm, slice thickness = 3 mm, number of slices = 44). Multiband acquisition sequences were derived from the Human Connectome Project ( ). A T1-weighted image was acquired sagitally for anatomical reference (bandwidth = 170 Hz/Px, TR = 1900 ms, TE = 2.52 ms, echo spacing = 7.5 ms, flip angle = 9\u00b0, field-of-view = 350 mm \u00d7 263 mm \u00d7 350 mm, voxel resolution = 1 mm \u00d7 1 mm \u00d7 1 mm, slice thickness = 1 mm). \n\nMagnetic resonance imaging data pre-processing and statistical analyses were performed using SPM8, through Matlab R2014a (Mathworks, Natick, MA, USA). Image pre-processing included image realignment, then coregistration of the T1 image to a mean realigned functional image created during realignment. The co-registered T1 image was normalized to the T1 template supplied with SPM8 Montreal Neuroimaging Institute (MNI), then the parameters of this transformation were applied to realigned functional images. The normalized functional images were spatially smoothed with a Gaussian kernel of 8 mm \u00d7 8 mm \u00d7 8 mm. \n\n\n#### Statistical Analyses \n  \nFirst-level modeling was performed by fitting a convolved hemodynamic response function (HRF) and its temporal derivative separately to the onset times of angry, disgusted, fearful, happy, sad, neutral, and own faces (seven regressors plus their temporal derivative). After parameter estimation, each emotion parameter and the participants\u2019 own face parameter was contrasted with the neutral face parameter producing six contrast images (angry > neutral, sad > neutral, etc.). At the group level, these contrast images were first entered into one-way analysis of variance (ANOVA) models for AN and HC groups separately to investigate within-group effects (results are presented in Supplementary Material). Group differences were interrogated with a mixed-effects ANOVA model using the flexible factorial option in SPM8. This model included a between-subjects   group   factor (two levels: patients vs. controls), a within subjects   condition   factor (six levels: angry > neutral, sad > neutral, etc.) and a   subject  s factor (number of levels equals the number of participants) that controlled for within-subject variability ( ). \n\n T  -statistic images were corrected for multiple comparisons using the random-field theory approach at the voxel and cluster levels (  p   < 0.05, FWE-corrected). The mixed-design analysis involved the investigation of a group by condition interaction, followed by simple effects comparing each condition between groups. \n\nFollowing the mixed-design analysis, clusters which resulted in significant differences between groups were defined as different regions of interests (ROIs) with the MarsBar toolbox ( ) run under Matlab R2014a. Contrast estimates for each ROI were correlated with eyetracking and behavioral data, and scores on the EDE-Q and TAS-20. \n\nPerformance on eyetracking components and emotion identification (rate of emotion identification errors) were compared with mixed design ANOVAs, following normality checking and the removal of outliers. Percentage data underwent an arcsine transformation prior to inclusion in ANOVAs. Violations of sphericity were corrected with a Greenhouse\u2013Geiser correction. For conditions in which groups significantly differed in emotion identification error rate, Mann\u2013Whitney   U   tests were carried out to identify which emotions were incorrectly reported as the data were not normally distributed. Pearson\u2019s correlation analyses were also performed between eyetracking data and behavioral data, and scores on the EDE-Q and TAS-20. For brevity, only significant interactions with group will be commented on in detail. Detailed results of eyetracking and fMRI analyses unrelated to group interactions are available as Supplementary Material. \n\n\n\n\n## Results \n  \n### Behavioral \n  \nFor rate of emotion identification errors, a 2 (group) \u00d7 8 (condition) mixed design ANOVA was undertaken (see Supplementary Table  ). The analysis revealed a significant main effect of condition [  F  (2.5,113.5) = 7.4,   p   < 0.001]. A significant main effect of group [  F  (1,46) = 5.2,   p   \u2264 0.05] and a significant interaction between group and condition were also found [  F  (2.5,113.5) = 4.6,   p   \u2264 0.01]. Within subjects contrasts revealed that groups did not significantly differ in emotion identification errors to any individual emotion, but AN participants made significantly more emotion identification errors to their own face [  F  (1,46) = 5.1,   p   \u2264 0.05]. When errors were made to own face emotion, analyses revealed that AN participants were more likely than controls to report their own face as sad [  U  (46) = 200.0, Z = -2.9,   p   \u2264 0.01], whereas control participants were more likely to correctly report their own neutral face as portraying a neutral expression [  U  (46) = 209.0,   Z   = -2.3,   p   \u2264 0.05] (see Supplementary Table  ). \n\n\n### Eyetracking \n  \nAverage fixation count, fixation duration, and saccade amplitude were analyzed in separate 2 (group) \u00d7 7 (condition) \u00d7 2 (task) mixed design ANOVAs. As the first run of the fMRI task consisted of the same number of trials as the behavioral task, these two tasks were included in the analysis. Furthermore, as the behavioral task also consisted of surprised faces which were not included in the fMRI task, these trials were excluded from the analysis. Means and SD for fixation count, fixation duration and saccade amplitude are presented in see Supplementary Table  . \n\nFor fixation count, a significant main effect of condition [  F  (4.1,159.2) = 3.0,   p   \u2264 0.05] was found with a greater number of fixations made to participants\u2019 own faces and faces depicting anger and fear. A significant main effect was also found for group [  F  (1,39) = 5.3,   p   \u2264 0.05], with AN participants making more fixations than controls. A significant interaction between condition and task was also found [  F  (3.4,131.9) = 7.61,   p   < 0.001] with a decreased number of fixations between implicit and explicit tasks for participants\u2019 own faces and faces depicting fear. There was, however, no significant main effect of task, and no significant interaction between condition and group, or task and group. There was also no interaction between condition, group and task. Analyses conducted on fixation duration revealed a significant main effect of group [  F  (1,38) = 8.5,   p   \u2264 0.01] with AN participants making fixations of shorter duration than heathy individuals. No other significant main effects or interactions were found. Analyses conducted on saccade amplitude resulted in no significant main effects or interactions. \n\nA 2 (group) \u00d7 7 (condition) \u00d7 2 (task) mixed design ANOVA conducted on the FDI revealed a significant main effect of condition [  F  (2.3,82.0) = 16.4,   p   < 0.001], and a significant interaction between condition and task [  F  (4.0,144.9) = 3.0,   p   \u2264 0.05] with greater attention to salient features of one\u2019s own face during the implicit compared to explicit task. Analyses conducted on the FFI revealed significant main effects of condition [  F  (2.2,72.5) = 19.7,   p   < 0.001] and task [  F  (1,33) = 10.1,   p   \u2264 0.01] with greater attention to salient facial features during the explicit task compared to the implicit task. Significant interactions were also found between condition and task [  F  (3.8,125.9) = 3.1,   p   \u2264 0.05] and condition and group [  F  (2.2,72.5) = 3.2,   p   \u2264 0.05]. Within-subjects contrasts did not reveal any significant differences between groups for any emotion, but a significant difference for own faces between AN and control participants [  F  (1,33) = 5.9,   p   \u2264 0.05]. Further 2 (group) \u00d7 2 (task) mixed design ANOVAs were also conducted on the FFI and FDI to participants\u2019 own face. A significant main effect of task, and an interaction between task and group were not found for either the FFI or FDI. A significant main effect of group was, however, found for both the FFI [  F  (1,36) = 7.6,   p   > 0.01] and FDI [  F  (1,36) = 6.8,   p   \u2264 0.05] (see Supplementary Table  ). Control participants showed more visual attention to salient features of their own face, whereas the attention shown to salient and non-salient features of their own face in AN was roughly equal. \n\n\n### Functional Magnetic Resonance Imaging \n  \n#### Mixed Design Analysis \n  \nThe analysis did not result in a significant group by condition interaction. Simple effects between groups for each condition revealed a significant difference between groups only for the own > neutral face contrast. Increased activation was found in AN compared to controls in the own > neutral contrast in two clusters: one in the right inferior temporal and middle temporal gyri, and one in the right lingual gyrus (  Table   ;   Figure   ). \n  \n Increased activity in the anorexia nervosa (AN) group compared to the control group in the right inferior and middle temporal gyri   (A)  , and the right lingual gyrus   (B)   for participants\u2019 own faces compared to neutral faces (contrast: AN > controls, own > neutral face; FWE-corrected for multiple comparisons at the voxel and cluster levels).   The color scale indicates the   t  -value. \n    \nSignificant activations for participants\u2019 own faces compared to neutral faces between anorexia nervosa (AN) and control participants, Own > Neutral, AN > Healthy Controls. \n    \n\n\n### Pearson\u2019s Correlations \n  \nBOLD activity in the own > neutral face contrast in either the inferior and middle temporal gyri ROI, or the lingual gyrus ROI did not significantly correlate with any eyetracking parameter, rate of emotion identification errors, or the results of the EDE-Q and TAS-20 for either group. Eyetracking and behavioral data, and scores on the EDE-Q and TAS-20 were also not found to correlate with one another. \n\n\n\n## Discussion \n  \nThe aim of this study was to investigate own face processing and the processing of emotional faces of others, in individuals with AN. Although the AN group were found to have higher alexithymia scores, they did not differ from healthy controls in emotion identification of the Ekman emotional face stimuli. Groups were found to differ in visual scanpath behavior to the stimuli in general, with AN participants demonstrating hyperscanning, evinced by increased fixations of shorter duration, relative to controls. AN participants were also found to avoid visually attending to salient features of their own face and displayed increased activity in the right lingual, and inferior and middle temporal gyri to images of their own face, compared to neutral control images, as well as to healthy individuals. \n\nIn analyses directly comparing AN to healthy individuals, AN participants showed increased activity to their own face in the right inferior and middle temporal gyri, and lingual gyrus, areas related to higher-order visual perception. Increased activity of the lingual gyrus has been reported during the processing of human faces ( ), and increased activity in the inferior and middle temporal gyri have been specifically found in response to own face stimuli ( ;  ;  ). Therefore, the findings of the current study suggest an increased processing of own face stimuli in AN in areas related to visual perception of self. \n\nDifferences in lingual gyrus and temporal gyrus activity have also been reported when individuals with AN are presented with images of their own body compared to other individuals\u2019 bodies. However, increased activity in these areas was found for controls relative to AN participants ( ;  ), rather than increased activity in AN as found in the current study. However, neither of those studies actually involved face processing; rather, the faces of the stimuli presented in those studies were masked when presented to participants. Therefore, the differences in activation between the current findings and the findings of these two studies may be related to the specific processing of self-face and self-body images which result in increased activity and decreased activity in these areas in AN respectively. \n\nOur AN group also showed more visual attention to non-salient features and avoided fixating on salient features of their own face.   reported that when AN participants were presented with whole body images of themselves, less visual attention was allocated to their faces compared to controls, though the level of visual attention to different body areas did not differ. Furthermore,   reported a similar pattern of visual attention in a study which presented AN and control participants with food and non-food images simultaneously: although healthy individuals showed more visual attention to food stimuli, the AN group displayed roughly equal attention to food and non-food stimuli. These results may be related to the finding that decreased visual attention is associated with the presentation of anxiety-inducing and phobic stimuli. Similarly to the current findings,   reported that individuals with social anxiety disorder made fixations of short duration to salient features of emotional faces stimuli.  , reported that individuals with spider phobia displayed fewer fixations to spider images and instead diverted their attention to neutral areas of the stimulus. The authors described this behavior as a strategy to cope with threatening and confrontational stimuli, to consequently reduce anxiety. Individuals with AN may utilize similar strategies when viewing their own face as they may find these stimuli anxiety-provoking. An avoidance of salient features was not, however, found to the Ekman face stimuli, suggesting that this behavior is specific to one\u2019s own face. \n\nHyperscanning of face stimuli (increased fixations of shorter duration) was also found in our AN group, though this was not specific to self-face images. Hyperscanning behaviors are associated with increased anxiety, as has been reported in social anxiety disorder to face stimuli of different emotions ( ).   suggested the hyperscanning behavior in social anxiety disorder may reflect a fear of social evaluation. This explanation may also be relevant to the current findings: due to a preoccupation with physical appearance, individuals with AN are particularly sensitive to social evaluations made by others ( ) and may show increased scanning behaviors to related stimuli. \n\nDespite previous reports of emotion identification deficits in individuals with AN, our findings do not concur. Although higher levels of alexithymia were found in the AN group, the ability to identify emotions from a standard set of face stimuli did not differ from healthy individuals. This finding is consistent with the literature which suggests that alexithymia levels are not related to the ability to perceive emotion from faces (see   for a review). However, whether high alexithymia impairs the ability to perceive emotion in an image of one\u2019s own face has not been investigated, nor was it specifically investigated in this study, as we only presented participants with own-face images showing a neutral expression. Although the majority of AN participants reported their own face as portraying a neutral expression, they were more likely than controls to report their own face as depicting a sad expression. However, rather than indicating an emotion identification deficit specific to themselves, this result is more likely to reflect a biased self-perception in how individuals with AN   feel   they look. Thus, when identification errors occurred as they viewed their own faces, they were specific to seeing themselves as sad and not any other emotions. Furthermore, when participants were questioned following the task about their overall experience, the controls often reported that they found it \u2018funny\u2019 looking at their own face, whereas AN patients tended to feel disgusted. \n\nThough AN and control participants differed in the processing of their own face, we found no differences in BOLD activity in response to different emotions, in either group. As strict thresholding was utilized in an attempt to correct for multiple comparisons, the task may not have had sufficient power to document significant differences in activation of each emotion relative to the neutral face condition. However, as the contrasts between participants\u2019 own face and the neutral face condition survived this threshold, the results show that BOLD activity elicited in response to the different emotions was not as strong. Since we were interested in assessing the visual scanpaths of AN patients to face stimuli, the extended presentation time was required for this purpose and the number of trials was therefore limited to remain within a reasonable duration for an MRI task. In future research, where visual scanpaths are not of interest, an increased number of trials should be utilized. It would also be of interest to investigate differences in visual scanpaths and neural activity to both own-face and own-body stimuli in AN, and an evaluation of how these images make AN patients feel. \n\nThe findings of this study have important potential clinical implications. Participants with AN did not differ from healthy control participants in the areas of attentional focus when viewing the Ekman face stimuli, nor did they differ in emotion identification of these stimuli. AN participants did, however, display an avoidance of salient features of images of their own faces. This may have consequently led to the mislabelling of their own neutral expression as sad. In other words, the perception of one\u2019s own face with a neutral expression as appearing sad in AN may be related to patients not looking at the correct areas of their own face when making an affect judgment. This may be as a result of emotional disturbances and alexithymia in AN, or this alternatively may lead to these disturbances. Therefore, remediation techniques which train participants to focus on the correct areas of the face may be beneficial in AN. These techniques have proven useful in other psychiatric conditions such as schizophrenia, with trained individuals demonstrating an improvement in attention to salient facial features and emotion recognition ( ;  ). Furthermore, the majority of deficits reported in the current study were specific to the processing of self-images in AN. This emphasizes the importance of therapies such as cognitive behavioral therapy to address distorted perceptions of oneself in AN and correctly analyzing events and the patient\u2019s own internal dialog. It is also possible that the sense of disgust evoked by viewing their own face also reduced the AN group\u2019s fixations on the salient regions of images of their own faces, in the way that one might avoid eye contact with a repellent individual. \n\nIn summary, this study suggests intact emotion identification of facial affect stimuli and distinct hyperscanning behaviors when viewing faces in AN. Significant differences were also found in the processing of one\u2019s own face in AN, with AN participants showing a greater level of visual attention to non-salient features and increased activity in inferior and middle temporal, and lingual gyri, relative to healthy individuals. These findings suggest overlap with anxiety disorders, as evinced by the hyperscanning behaviors displayed, and increased anxiety, particularly to own face images evinced by an apparent avoidance of salient features. Together with the fMRI finding, the study suggests that the processing of self-face images is different in AN, and may contribute to the distorted perception of oneself experienced by individuals with this illness. \n\n\n## Author Contributions \n  \nAP, LA, SR, CG, and DC designed the research; AP completed data collection; all authors were involved in data analysis and manuscript preparation. \n\n\n## Conflict of Interest Statement \n  \nProf. David J. Castle reports grants and personal fees from Eli Lilly, grants and personal fees from Janssen-Cilag, grants and personal fees from Roche, grants and personal fees from Allergen, grants and personal fees from Bristol-Myer Squibb, grants and personal fees from Pfizer, grants and personal fees from Lundbeck, grants and personal fees from AstraZeneca, grants and personal fees from Hospira, during the conduct of the study; personal fees from Eli Lilly, personal fees from Bristol-Myer Squibb, personal fees from Lundbeck, personal fees from Janssen-Cilag, personal fees from Pfizer, personal fees from Organon, personal fees from Sanofi-Aventis, personal fees from Wyeth, personal fees from Hospira, personal fees from Servier, outside the submitted work. Prof. Larry A. Abel reports personal fees from Actelion Pharmaceuticals, Switzerland, outside the submitted work. Dr. Andrea Phillipou, Prof. Susan L. Rossell, Dr. Caroline Gurvich, Dr. Matthew E. Hughes, and Mr. Richard G. Nibbs report no conflicts of interest. \n\n \n\n# Table(s)\n## ID: T1\n### Label: Table 1\nUnnamed: 0_level_0\tAN\tAN\tHC\tHC\tHC\nUnnamed: 0_level_1\tM\tSD\tM\tSD\tp\nAge\t22.18\t5.45\t22.64\t3.25\t0.725\nPremorbid IQ\t104.22\t8.07\t105.71\t7.13\t0.505\nBMI\t16.47\t1.13\t22.36\t3.66\t0.001\nIllness duration\t6.89\t7.28\t\u2013\t\u2013\t\u2013\nAge of illness onset\t15.74\t3.24\t\u2013\t\u2013\t\u2013\nEDE-Q restraint\t3.84\t1.38\t0.58\t0.64\t0.001\nEDE-Q eating concern\t3.79\t1.27\t0.25\t0.32\t0.001\nEDE-Q shape concern\t5.02\t0.92\t1.15\t0.86\t0.001\nEDE-Q weight concern\t4.5\t1.45\t0.60\t0.77\t0.001\nEDE-Q global score\t4.29\t1.15\t0.65\t0.54\t0.001\nTAS-20 difficulty\t22.78\t5.88\t11.46\t4.74\t0.001\nidentifying feelings\t\t\t\t\t\nTAS-20 difficulty\t17.96\t3.36\t10.50\t4.08\t0.001\ndescribing feelings\t\t\t\t\t\nTAS-20 externally\t20.39\t3.39\t17.33\t5.29\t0.023\noriented thinking\t\t\t\t\t\nTAS-20 score\t61.13\t9.06\t38.13\t11.41\t0.001\n### Caption\nParticipant information.\n### Footer\nAN, anorexia nervosa; HC, healthy control; Premorbid IQ, Standardized Wechsler Test of Adult Reading Score; BMI, body mass index; EDE-Q, Eating Disorders Examination Questionnaire; TAS-20, Toronto Alexithymia Scale; Age, age of illness onset and duration illness are reported in years.\n\n\n## ID: T2\n### Label: Table 2\nPeak regions\tNo. of voxels\tPeak t\tPeak MNI coordinates\tPeak MNI coordinates\tPeak MNI coordinates\nUnnamed: 0_level_1\tUnnamed: 1_level_1\tUnnamed: 2_level_1\tx\ty\tz\nInferior temporal gyrus\t104.0\t6.45\t50.0\t-54.0\t-4.0\nmiddle temporal gyrus\t\t\t\t\t\nLingual gyrus\t118.0\t5.37\t28.0\t-88.0\t-8.0\n### Caption\nSignificant activations for participants\u2019 own faces compared to neutral faces between anorexia nervosa (AN) and control participants, Own > Neutral, AN > Healthy Controls.\n### Footer\nMNI, Montreal Neuroimaging Institute.\n", "metadata": {"pmcid": 4530666, "text_md5": "70d4dfe20b183af605bb88eea986d6c8", "field_positions": {"authors": [0, 146], "journal": [147, 160], "publication_year": [162, 166], "title": [177, 252], "keywords": [266, 321], "abstract": [334, 1708], "body": [1717, 38219], "tables": [38232, 39860]}, "batch": 1, "pmid": 26321993, "doi": "10.3389/fpsyg.2015.01181", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4530666", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4530666"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4530666\">4530666</a>", "list_title": "PMC4530666  Self perception and facial emotion perception of others in anorexia nervosa"}
