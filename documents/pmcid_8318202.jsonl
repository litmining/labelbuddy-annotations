{"text": "Rogers, Chad S. and Jones, Michael S. and McConkey, Sarah and Spehar, Brent and Van Engen, Kristin J. and Sommers, Mitchell S. and Peelle, Jonathan E.\nNeurobiol Lang (Camb), 2020\n\n# Title\n\nAge-Related Differences in Auditory Cortex Activity During Spoken Word Recognition\n\n# Keywords\n\nspeech perception\ncognitive aging\nspeech production\n\n\n# Abstract\n \nUnderstanding spoken words requires the rapid matching of a complex acoustic stimulus with stored lexical representations. The degree to which brain networks supporting spoken word recognition are affected by adult aging remains poorly understood. In the current study we used fMRI to measure the brain responses to spoken words in two conditions: an attentive listening condition, in which no response was required, and a repetition task. Listeners were 29 young adults (aged 19\u201330 years) and 32 older adults (aged 65\u201381 years) without self-reported hearing difficulty. We found largely similar patterns of activity during word perception for both young and older adults, centered on the bilateral superior temporal gyrus. As expected, the repetition condition resulted in significantly more activity in areas related to motor planning and execution (including the premotor cortex and supplemental motor area) compared to the attentive listening condition. Importantly, however, older adults showed significantly less activity in probabilistically defined auditory cortex than young adults when listening to individual words in both the attentive listening and repetition tasks. Age differences in auditory cortex activity were seen selectively for words (no age differences were present for 1-channel vocoded speech, used as a control condition), and could not be easily explained by accuracy on the task, movement in the scanner, or hearing sensitivity (available on a subset of participants). These findings indicate largely similar patterns of brain activity for young and older adults when listening to words in quiet, but suggest less recruitment of auditory cortex by the older adults. \n \n\n# Body\n \n## INTRODUCTION \n  \nUnderstanding spoken words requires mapping complex acoustic signals to a listener\u2019s stored lexical representations. Evidence from neuropsychology and cognitive neuroscience provides increasingly converging evidence about the roles of the bilateral temporal cortex (particularly the superior temporal gyrus and the middle temporal gyrus) in processing speech acoustics and recognizing single words ( ;  ;  ). However, the degree to which the networks supporting spoken word recognition change over our lifetime remains unclear. The goals of the current study were to test whether young and older adults relied on different brain networks during successful spoken word recognition, and whether any age differences were related to the specific task. \n\nImportant themes when considering older adults\u2019 language processing include the degree to which linguistic processing is preserved, and whether older adults may adopt different strategies when understanding language compared to young adults ( ;  ). Particularly important for spoken word recognition is that adult aging frequently brings changes to both hearing sensitivity ( ) and cognitive ability ( ). Thus, it is not surprising that older adults\u2019 spoken word perception differs from that of young adults, particularly in the presence of background noise ( ). Older adults tend to take longer to recognize words ( ;  ), make more recognition errors than young adults, and show increased sensitivity to factors such as the number of phonological neighbors (competitors) associated with a given target word ( ). An open question centers on the brain networks on which older adults rely during spoken word recognition. Of particular interest is whether additional regions may be recruited to support successful recognition, compared to those engaged by young adults. \n\nA number of studies have investigated neural activity during older adults\u2019 speech processing in noise or other acoustic degradation, using an assortment of tasks and testing participants with different levels of hearing ( ;  ;  ;  ;  ).  , for example, examined spoken word recognition in young and older adults. They varied the intelligibility of the target items using low-pass filtering of the acoustic signal. During scanning, participants were asked to repeat back the word they heard. The authors found increased activity in regions associated with word processing, including the auditory cortex and the premotor cortex, when words were more intelligible; these intelligibility-related changes did not statistically differ between young and older adults. Older adults did show more activation in the anterior cingulate cortex and the supplemental motor area than the young adults did, suggesting a possible increase in top-down executive control. \n\nAge differences in speech understanding have also been studied in the context of sentence comprehension. One common finding is that during successful sentence processing, older adults show additional activity compared to young adults (e.g., in contralateral homologs to regions seen in young adults, or in regions beyond the network activated by young adults;  ;  ). These findings have been interpreted in a compensation framework in which older adults are less efficient using a core speech network and need to recruit additional regions to support successful comprehension ( ). However, at least some of this additional activity has been shown to be related to the tasks performed by participants in the scanner, which frequently contain metalinguistic decisions not required during everyday conversation ( ). Thus, it may be that core language computations are well-preserved in aging ( ;  ). \n\nThe role of executive attention in older adults\u2019 spoken word recognition has also been of significant interest. Listening to speech that is acoustically degraded can result in perception errors, after which listeners must re-engage attention systems to support successful listening. The cingulo-opercular network, an executive attention network ( ;  ), shows increased activity following perception errors (similar to error-related activity in other domains). Crucially, when listening to spoken words in background noise, increased cingulo-opercular activity following one trial is associated with recognition success on the following trial ( ;  ), consistent with a role in maintaining task-related attention ( ). \n\nAn important challenge when considering the performance of listeners with hearing loss is that words may not be equally intelligible to all listeners. A common measure of accuracy in spoken word recognition is to ask listeners to repeat each word after hearing it; however, this type of task requires motor responses, which may obscure activations related to speech perception and increase participant motion in the scanner ( ). In addition, differences in the brain regions coordinating speech   production   in older adults ( ;  ) may interfere with clear measurements of activity during perception and recognition. The degree to which motor effects resulting from word repetition may obscure activity related to speech perception is unclear. In sentence processing tasks, task effects can be significant ( ), and if not accounted for may obscure what are actually consistent patterns of language-related activity across the lifespan ( ). \n\nIn the current study we investigated spoken word processing in young and older adult listeners in the absence of background noise. We compared paradigms requiring words to be repeated with \u201cattentive listening\u201d (no motor response required). Our interest is, first, whether age differences exist in the brain networks supporting spoken word recognition, and second, whether these differences are affected by the choice of task. Thus, our primary analyses will focus on activity seen for words (greater than noise) in the experimental conditions. \n\nThe influence of psycholinguistic factors on spoken word recognition has long been appreciated. In a secondary set of analyses, we will investigate whether word frequency or phonological neighborhood density modulate activity during spoken word recognition. Although behavioral and electrophysiological studies suggest that high frequency words are processed more quickly than low frequency words, the degree to which this might be captured in fMRI is unclear. Similarly, although neighborhood density effects are widely reported in behavioral studies (with words from dense neighborhoods typically being more difficult to process), the degree to which lexical competition effects may differ with age is unclear. \n\n\n## MATERIALS AND METHODS \n  \nStimuli, data, and analysis scripts are available from  . \n\n### Participants \n  \nWe recruited two groups of participants (young and older adults) for this study. The young adults were 29 self-reported healthy, right-handed adults, aged 19\u201330 years (  M   = 23.8,   SD   = 2.9, 19 female), and were recruited via the Washington University in St. Louis Department of Psychological and Brain Sciences Subject Pool. Older adult participants were 32 self-reported healthy, right-handed adults, aged 65\u201381 years (  M   = 71.0,   SD   = 5.0, 17 female). All participants self-reported themselves to be native speakers of American English with no history of neurological difficulty, and with normal hearing (and no history of a diagnosed hearing problem). Participants were compensated for their participation, and all provided informed consent commensurate with practices approved by the Washington University in St. Louis Institutional Review Board. \n\nAudiograms were collected on a subset of eight young and nine older participants using pure-tone audiometry ( ). We summarized hearing ability using a better-ear pure tone average (PTA) at 1, 2, and 4 kHz. PTAs in participants\u2019 better hearing ears ranged from \u22123.33 to 8.33 dB HL in young adults (  M   = 2.92,   SD   = 4.15), and 8.33 to 23.3 dB HL in older adults (  M   = 23.3,   SD   = 9.17). \n\n\n### Materials \n  \nStimuli for this study were 375 monosyllabic consonant-vowel-consonant words. The auditory stimuli were recorded at 48 kHz using a 16-bit digital-to-analog converter with an Audio Technica 2035 microphone in a quiet room. Words were spoken by a female speaker with a standard American dialect. Root-mean-square amplitude of the stimuli was equated. \n\nOut of the full set of words, 75 words were vocoded using a single channel with white noise as a carrier signal ( ) using   jp_vocode.m   from  . These stimuli were used for an unintelligible baseline \u201cnoise\u201d condition. The remaining 300 words were divided into five lists of 60 words, using MATCH software ( ), and were balanced for word frequency (as measured by the log of the Hyperspace Analogue to Language dataset), orthographic length, concreteness ( ), and familiarity ( ). The distribution of word frequency and phonological neighborhood density are shown in  . \n\nOne of these lists was combined with 15 of the noise vocoded words and used for word repetition task practice outside of the scanner. The remaining four lists of 60 words served as the critical items inside the scanner, with half of the lists used for attentive listening (120 total words) and the other half for word repetition (120 total words). Word lists were counterbalanced such that each word was presented in both \u201clisten\u201d and \u201crepeat\u201d conditions across participants. \n\n\n### Procedure \n  \nPrior to scanning, participants were taken to a quiet room. (The room was not sound isolated and low frequency noise from the building heating, ventilation, and air conditioning system was typically present.) During that time participants provided informed consent, completed demographic questionnaires, and a subset had their hearing tested using a calibrated Maico MA40 portable audiometer (Maico Diagnostics, Inc., Eden Prairie MN) by an audiologist-trained researcher. \n\nParticipants were then instructed for the two tasks they would perform in the scanner: attentive listening and word repetition. During attentive listening, participants were asked to stay alert, still, and keep their eyes focused on a fixation cross while listening to a sequence of auditory sounds, including words, silence, and noise (single-channel noise vocoded words). During word repetition, participants were asked to do the same as in attentive listening, with the addition of repeating the word they just heard aloud. Participants were instructed to repeat the words following the volume acquisition after each word ( ). Participants were told to give their best guess if they could not understand a word. Participants practiced a simulation of the word repetition task until the experimenter was confident that the participant understood the pacing and the nature of the task. Sound levels were adjusted to achieve audible presentations at the beginning of the study and thereafter not adjusted. \n\nFunctional MRI scanning took place over the course of four scanning blocks, where participants alternated between blocks of attentive listening and word repetition ( ). The order of blocks was counterbalanced such that participants were equally likely to begin with a word repetition or an attentive listening block. During word repetition, participants\u2019 spoken responses were recorded using an in-bore Fibersound optical microphone. These responses were scored for accuracy offline by a research assistant ( ). \n\n\n### MRI Data Acquisition and Processing \n  \nThe MRI data collected in this study are available from   ( ). MRI data were acquired using a Siemens Prisma scanner (Siemens Medical Systems) at 3 T equipped with a 32-channel head coil. Scan sequences began with a T1-weighted structural volume using an MPRAGE sequence (repetition time [TR] = 2.4 s, echo time [TE] = 2.2 ms, flip angle = 8\u00b0, 300 \u00d7 320 matrix, voxel size = 0.8 mm isotropic). Blood oxygenation level-dependent fMRI images were acquired using a multiband echo planar imaging sequence ( ; TR = 3.07 s, TA = 0.770 s, TE = 37 ms, flip angle = 37\u00b0, voxel size = 2 mm isotropic, multiband factor = 8). (The flip angle was suboptimal due to an error setting up the sequences; although discovered partway through the study, we left it unchanged to maintain consistent data quality. With a TR of ~3 s we would expect a better signal-to-noise ratio with a flip angle of 90\u00b0.) We used a sparse imaging design in which there was a 2.3 s delay between scanning acquisitions and the TR was longer than the acquisition time to allow for minimal scanning noise during stimulus presentation and audio recording of participant responses ( ;  ). \n\nAnalysis of the MRI data was performed using Automatic Analysis version 5.4.0 ( ; RRID:SCR_003560), which scripted a combination of SPM12 (Wellcome Trust Centre for Neuroimaging) version 7487 (RRID:SCR_007037) and FMRIB Software Library (FSL; FMRIB Analysis Group;  ) version 6.0.1 (RRID:SCR_002823). \n\nData were realigned using rigid-body image registration, and functional data were coregistered with the bias-corrected T1-weighted structural image. Spatial and functional images were normalized to MNI space using a unified segmentation approach ( ), and resampled to 2 mm. Finally, the functional data were smoothed using an 8 mm full width at half maximum Gaussian kernel. \n\nFor the attentive listening condition, we did not have measures of accuracy, so we analyzed all trials. For the repetition condition, we analyzed only trials associated with correct responses. For both tasks, we modeled the noise condition in addition to words. Finally, we included three parametric modulators for word events: word frequency, phonological neighborhood density, and their interaction. To avoid order effects ( ), these were not orthogonalized. \n\nMotion effects were of particular importance given that participants were speaking during the repetition condition. To mitigate the effects of motion, we used a thresholding approach in which high motion frames were individually modeled for each subject using a delta function in the general linear model (see, e.g.,  ). Motion was quantified using framewise displacement (FD), calculated from the six motion parameters estimated during realignment, assuming the head is a sphere having a radius of 50 mm ( ). We then chose an FD threshold (0.561) that we used for all participants. Our rationale was that some participants move more, and thus produce worse data; we therefore wanted to use a single threshold for all participants, resulting in more data exclusion from high-motion participants. This threshold resulted in 2.2\u201319.4% (  M   = 6.21,   SD   = 4.45) data exclusion for the young adults and 2.8\u201358.4% (  M   = 22.6,   SD   = 15.3) data exclusion for the older adults. For each frame exceeding this threshold, we added a column to that participant\u2019s design matrix consisting of a delta function at the time point in question, which effectively excludes the variance of that frame from the model. \n\nContrast images from single subject analyses were analyzed at the second level using permutation testing (FSL   randomise  ; 5,000 permutations;  ), with a cluster-forming threshold of   p   < 0.001 (uncorrected) and results corrected for multiple comparisons based on cluster extent (  p   < 0.05). Images (contrast images and unthresholded   t   maps) are available from   ( ). Anatomical localization was performed using converging evidence from author experience ( ) viewing statistical maps overlaid in MRIcroGL ( ), supplemented by atlas labels ( ). \n\nFor region of interest (ROI) analysis of primary auditory cortex, we used probabilistic maps based on postmortem human histological staining ( ), available in the SPM Anatomy toolbox ( ; RRID:SCR_013273). We created a binary mask for regions Te1.0 and Te1.1 and then extracted parameter estimates for noise and word contrasts for the attentive listening and repetition conditions from each participant\u2019s first-level analyses by averaging over all voxels in each ROI (left auditory, right auditory). \n\nOutputs from analysis stages used for quality control are available from   in the aa_report folder. \n\n\n\n## RESULTS \n  \n### Behavioral Data \n  \nWe analyzed the accuracy data using a linear mixed effects analysis, implemented using the   lme4   and   lmerTest   packages in R version 3.6.2 ( ;  ; RRID:SCR_001905). Because trial-level accuracy data was binary, we used logistic regression. We first tested for age differences using a model that included age group as a fixed factor and subject as a random factor:\n \n\nThe estimate of age_group was \u22121.4929 (  SE   = 0.1902),   p   = 4.24e\u201315, consistent with a main effect of age (older adults performing more poorly). Because of the ceiling effects and the lack of variability in the young adult data, we ran an additional analysis only in the older adults, using a model that included neighborhood density and word frequency as fixed factors, and item and subject as random factors:\n \n\nThe model failed to converge when including a more complex random effect structure. The results of this model are shown in  . There were no significant effects of neighborhood density or word frequency in the accuracy data. \n\nThere are many reasons a participant might make an incorrect response, and our primary interest is in the processes supporting successful comprehension. Thus, for the fMRI analyses, we restricted our analyses to correct trials only. \n\n\n### Imaging Data \n  \nWe began by looking at activity in the auditory cortex, followed by whole-brain analyses. Activity in left and right auditory cortex for noise and word conditions for young and older adults is shown in  . We analyzed these data using a linear mixed effects analysis, implemented using the   lme4   and   lmerTest   packages in R version 3.6.2 (RRID:SCR_001905). The full model included task (listen, repeat), stimulus (word, noise), hemisphere (left, right), age group (young, older), and accuracy on the repetition task as fixed factors, with subject, stimulus type, and task as random factors:\n \nWhen including hemisphere as an additional random factor, the model failed to converge, and as our main interests lay elsewhere we settled on the above model. \n\nFull model results are shown in  . The   p   values were obtained from the   lmerTest   package using the Satterthwaite method for degrees of freedom and   t   statistics. We found significant interactions between task and stimulus, consistent with a greater degree of activation for words relative to noise in the repetition task compared to the attentive listening task. Importantly, there was a significant interaction between stimulus and age group, consistent with greater age differences for words relative to noise. We verified this with follow-up   t   tests, collapsing over hemisphere, which showed a significant difference in activity between young and older adults for words, attentive listening:   t  (57.973) = 3.1428,   p   = 0.002636; repetition:   t  (56.619) = 2.5583,   p   = 0.01322, but not for noise, attentive listening:   t  (58.559) = 0.66361,   p   = 0.5095; repetition:   t  (57.241) = 0.19028,   p   = 0.8498. None of the other main effects or interactions were significant. \n\nTo explore the possible contribution of other factors to older adults\u2019 reduced activity in the auditory cortex, we conducted a series of exploratory correlation analyses with accuracy, hearing, and movement parameters from fMRI (median FD). None of these analyses showed significant correlations with auditory cortex activity. Correlations for accuracy and hearing in the word repetition task are shown in  . Overall, we interpret these results as being consistent with less auditory activity for the older adults during spoken word perception (but not during our nonspeech control condition). \n\nTo complement the ROI analyses, we next performed whole-brain analyses for all conditions of interest. Activity for word perception in the attentive listening condition (greater than the noise baseline) is shown in   (with maxima listed in  \u2013 ). As expected, both young and older adults showed significant activity in the bilateral superior temporal cortex. Young adults showed significantly stronger activity in the superior temporal cortex near the auditory cortex. There were no regions in which older adults showed greater activity than young adults. \n\nIn addition to the overall pattern associated with word perception, we examined psycholinguistic effects of word frequency and phonological neighborhood density using a parametric modulation analysis. There were no significant effects of either word frequency or neighborhood density in the attentive listening condition. \n\nActivity for word perception in the repetition condition (relative to a noise baseline) is shown in   (with maxima in  \u2013 ). Again, both young and older adults showed significant activity in the bilateral temporal cortex, as well as frontal regions related to articulatory planning, including the premotor cortex and the supplemental motor area. As with the attentive listening condition, young adults showed significantly more activity in superior temporal regions near the auditory cortex. There were no regions where older adults showed more activity than the young adults. \n\nIn addition to the overall pattern associated with word perception, we examined psycholinguistic effects of word frequency and phonological neighborhood density using a parametric modulation analysis. There were no significant effects of either word frequency or neighborhood density in the repetition condition. \n\nFinally, we directly compared the attentive listening and repetition conditions, shown in   (with maxima in   and  ). Compared to the attentive listening condition, during the repetition condition both young and older listeners showed increased activity in motor and premotor cortex. There were no significant differences between young and older adults. \n\n\n\n## DISCUSSION \n  \nWe used fMRI to examine neural activity during spoken word recognition in quiet for young and older adult listeners. In both ROI and whole-brain analyses, we found converging evidence for reduced activity in the auditory cortex for the older adults. The age differences in auditory cortex activation were present in both the attentive listening task and the word repetition task: Although the repetition task resulted in more widespread activation overall, patterns of age-related differences in the auditory cortex were comparable. \n\nThere are a number of possible explanations for older adults\u2019 reduced activity during spoken word recognition. One possibility is that age differences in intelligibility might play a role. Intelligible speech is associated with increased activity in a broad network of frontal and temporal regions ( ;  ), and in prior studies of older adults, intelligibility has correlated with auditory cortex activity ( ). We restricted our analyses to correct responses in the repetition condition, and found no statistical support for a relationship between intelligibility and auditory cortex activation (although numerically, participants with better accuracy showed more activity than participants with worse accuracy). \n\nThe fact that young and older adults showed comparable activity in the auditory cortex during noise trials, with age differences emerging for word recognition trials, is significant. Group differences in activation could be driven not only by neural processing, but also by such factors as neurovascular coupling, goodness-of-fit of a canonical hemodynamic response, or movement within the scanner\u2014in other words, artifacts that might differentially impact model parameter estimates in young and older adults but are not of theoretical interest in this context. Although impossible to completely rule out, the selective age differences for speech (but not noise) are consistent with a condition-specific\u2014and thus we argue, neural\u2014interpretation. \n\nRecent evidence suggests age-related changes in temporal sensitivity in auditory regions can be detected with fMRI ( ). Although our current stimuli do not allow us to explore specific acoustic features, one possibility is that the age-related differences in auditory activity we observed reflect well-known changes in auditory cortical processing that occur in normal aging ( ). Given the increased acoustic complexity of the words relative to noise, acoustic processing differences might drive overall response differences. Such changes may also reflect decreased stimulation as a result of hearing loss; we had insufficient data to rule out this possibility. It is important to note that we cannot completely rule out audibility effects. Even though we limited our responses to correct identification trials, specific acoustic features may still have been less audible for the older adults. It remains an open question whether varying the presentation level of the stimuli would change the age effects we observed. \n\nAge differences in auditory processing are not the only explanation for our results. The auditory cortex is positioned in a hierarchy of speech processing regions that include both ascending and descending projections ( ;  ). The auditory cortex not only is sensitive to changes in acoustic information, but also reflects top-down effects of expectation and prediction ( ;  ;  ). Thus, the observed age differences in the auditory cortex may reflect differential top-down modulation of auditory activity in young and older adult listeners. \n\nIndeed, prior to conducting this study, we expected to observe increased activity (e.g., in the prefrontal cortex) for older adults relative to young adults, reflecting top-down compensation for reduced auditory sensitivity. Such activity would be consistent with increased cognitive demand during speech perception in listeners with hearing loss or other acoustic challenges ( ;  ). Although we were somewhat surprised not to see this, in retrospect, perhaps it would be expected. The stimuli in the current study were presented in quiet, and thus may not have challenged perception sufficiently to robustly engage frontal brain networks. We conclude that during perception of acoustically clear words, older adults do not seem to require additional resources from the frontal cortex; whether this changes with increasing speech demands (either acoustic or linguistic) remains an open question. \n\nWe did not observe significant effects of either word frequency or phonological neighborhood density on activity during spoken word recognition. These results stand in contrast to prior studies showing frequency effects in visual word perception in fMRI ( ;  ), and word frequency effects in electrophysiological responses ( ). Prior fMRI studies of lexical competition (including phonological neighborhood density) have been mixed, with some studies finding effects ( ) and others not ( ). It could be that a wider range of frequency or density or a greater number of stimuli would be needed to identify such effects. \n\nFinally, we found largely comparable age differences in the attentive listening and repetition conditions in the auditory cortex. The similarity of the results suggests that using a repetition task may be a reasonable choice in studies of spoken word recognition: Although repetition tasks necessarily engage regions related to articulation and hearing one\u2019s own voice, in our data these were not differentially affected by age. An advantage of using a repetition task, of course, is that trial-by-trial accuracy measures can be obtained, which are frequently useful. It is worth noting that our finding of comparable activity in young and older adults for attentive listening and repetition tasks may not generalize to other stimuli or tasks ( ;  ). \n\nA significant limitation of our current study is that we only collected hearing sensitivity data on a minority of our participants. Thus, although we saw a trend toward poorer hearing being associated with reduced auditory cortex activation, it is challenging to draw any firm conclusions regarding the relationship between hearing sensitivity and brain activity. Prior studies using sentence-level materials have found relationships between hearing sensitivity and brain activity in both young ( ) and older ( ) adults. Future investigations with a larger sample of participants with hearing data will be needed to further explore the effects of hearing in spoken word recognition. \n\nFrom a broader perspective, the link between spoken word recognition and everyday communication is not always straightforward. Much of our everyday communication occurs in the context of semantically meaningful, coherent sentences, frequently with the added availability of visual speech and gesture cues. Given potential age differences in reliance on many of these cues\u2014including older adults\u2019 seemingly greater reliance on semantic context ( ;  ;  )\u2014it seems likely that our findings using isolated spoken words cannot be extrapolated to richer naturalistic settings. \n\nIn summary, we observed largely overlapping brain regions supporting spoken word recognition in young and older adults in the absence of background noise. Older adults showed less activity than young adults in the auditory cortex when listening to words, but not noise. These patterns of age difference were present regardless of the task (attentive listening vs. repetition). \n\n \n", "metadata": {"pmcid": 8318202, "text_md5": "5142590994d98427c1c4e2369e24c6c2", "field_positions": {"authors": [0, 150], "journal": [151, 172], "publication_year": [174, 178], "title": [189, 271], "keywords": [285, 337], "abstract": [350, 2048], "body": [2057, 31487]}, "part": 1, "chapter": 5, "page": 18, "pmid": 34327333, "doi": "10.1162/nol_a_00021"}, "display_title": "pmcid: <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8318202>8318202</a> \u2014 Part 1 Chapter 5 Page 18", "list_title": "1.5.18 Age-Related Differences in Auditory Cortex Activity During Spoken Word Recognition"}
{"text": "Rogers, Chad S. and Jones, Michael S. and McConkey, Sarah and Spehar, Brent and Van Engen, Kristin J. and Sommers, Mitchell S. and Peelle, Jonathan E.\nNeurobiol Lang (Camb), 2020\n\n# Title\n\nAge-Related Differences in Auditory Cortex Activity During Spoken Word Recognition\n\n# Keywords\n\nspeech perception\ncognitive aging\nspeech production\n\n\n# Abstract\n \nUnderstanding spoken words requires the rapid matching of a complex acoustic stimulus with stored lexical representations. The degree to which brain networks supporting spoken word recognition are affected by adult aging remains poorly understood. In the current study we used fMRI to measure the brain responses to spoken words in two conditions: an attentive listening condition, in which no response was required, and a repetition task. Listeners were 29 young adults (aged 19\u201330 years) and 32 older adults (aged 65\u201381 years) without self-reported hearing difficulty. We found largely similar patterns of activity during word perception for both young and older adults, centered on the bilateral superior temporal gyrus. As expected, the repetition condition resulted in significantly more activity in areas related to motor planning and execution (including the premotor cortex and supplemental motor area) compared to the attentive listening condition. Importantly, however, older adults showed significantly less activity in probabilistically defined auditory cortex than young adults when listening to individual words in both the attentive listening and repetition tasks. Age differences in auditory cortex activity were seen selectively for words (no age differences were present for 1-channel vocoded speech, used as a control condition), and could not be easily explained by accuracy on the task, movement in the scanner, or hearing sensitivity (available on a subset of participants). These findings indicate largely similar patterns of brain activity for young and older adults when listening to words in quiet, but suggest less recruitment of auditory cortex by the older adults. \n \n\n# Body\n \n## INTRODUCTION \n  \nUnderstanding spoken words requires mapping complex acoustic signals to a listener\u2019s stored lexical representations. Evidence from neuropsychology and cognitive neuroscience provides increasingly converging evidence about the roles of the bilateral temporal cortex (particularly the superior temporal gyrus and the middle temporal gyrus) in processing speech acoustics and recognizing single words ( ;  ;  ). However, the degree to which the networks supporting spoken word recognition change over our lifetime remains unclear. The goals of the current study were to test whether young and older adults relied on different brain networks during successful spoken word recognition, and whether any age differences were related to the specific task. \n\nImportant themes when considering older adults\u2019 language processing include the degree to which linguistic processing is preserved, and whether older adults may adopt different strategies when understanding language compared to young adults ( ;  ). Particularly important for spoken word recognition is that adult aging frequently brings changes to both hearing sensitivity ( ) and cognitive ability ( ). Thus, it is not surprising that older adults\u2019 spoken word perception differs from that of young adults, particularly in the presence of background noise ( ). Older adults tend to take longer to recognize words ( ;  ), make more recognition errors than young adults, and show increased sensitivity to factors such as the number of phonological neighbors (competitors) associated with a given target word ( ). An open question centers on the brain networks on which older adults rely during spoken word recognition. Of particular interest is whether additional regions may be recruited to support successful recognition, compared to those engaged by young adults. \n\nA number of studies have investigated neural activity during older adults\u2019 speech processing in noise or other acoustic degradation, using an assortment of tasks and testing participants with different levels of hearing ( ;  ;  ;  ;  ).  , for example, examined spoken word recognition in young and older adults. They varied the intelligibility of the target items using low-pass filtering of the acoustic signal. During scanning, participants were asked to repeat back the word they heard. The authors found increased activity in regions associated with word processing, including the auditory cortex and the premotor cortex, when words were more intelligible; these intelligibility-related changes did not statistically differ between young and older adults. Older adults did show more activation in the anterior cingulate cortex and the supplemental motor area than the young adults did, suggesting a possible increase in top-down executive control. \n\nAge differences in speech understanding have also been studied in the context of sentence comprehension. One common finding is that during successful sentence processing, older adults show additional activity compared to young adults (e.g., in contralateral homologs to regions seen in young adults, or in regions beyond the network activated by young adults;  ;  ). These findings have been interpreted in a compensation framework in which older adults are less efficient using a core speech network and need to recruit additional regions to support successful comprehension ( ). However, at least some of this additional activity has been shown to be related to the tasks performed by participants in the scanner, which frequently contain metalinguistic decisions not required during everyday conversation ( ). Thus, it may be that core language computations are well-preserved in aging ( ;  ). \n\nThe role of executive attention in older adults\u2019 spoken word recognition has also been of significant interest. Listening to speech that is acoustically degraded can result in perception errors, after which listeners must re-engage attention systems to support successful listening. The cingulo-opercular network, an executive attention network ( ;  ), shows increased activity following perception errors (similar to error-related activity in other domains). Crucially, when listening to spoken words in background noise, increased cingulo-opercular activity following one trial is associated with recognition success on the following trial ( ;  ), consistent with a role in maintaining task-related attention ( ). \n\nAn important challenge when considering the performance of listeners with hearing loss is that words may not be equally intelligible to all listeners. A common measure of accuracy in spoken word recognition is to ask listeners to repeat each word after hearing it; however, this type of task requires motor responses, which may obscure activations related to speech perception and increase participant motion in the scanner ( ). In addition, differences in the brain regions coordinating speech   production   in older adults ( ;  ) may interfere with clear measurements of activity during perception and recognition. The degree to which motor effects resulting from word repetition may obscure activity related to speech perception is unclear. In sentence processing tasks, task effects can be significant ( ), and if not accounted for may obscure what are actually consistent patterns of language-related activity across the lifespan ( ). \n\nIn the current study we investigated spoken word processing in young and older adult listeners in the absence of background noise. We compared paradigms requiring words to be repeated with \u201cattentive listening\u201d (no motor response required). Our interest is, first, whether age differences exist in the brain networks supporting spoken word recognition, and second, whether these differences are affected by the choice of task. Thus, our primary analyses will focus on activity seen for words (greater than noise) in the experimental conditions. \n\nThe influence of psycholinguistic factors on spoken word recognition has long been appreciated. In a secondary set of analyses, we will investigate whether word frequency or phonological neighborhood density modulate activity during spoken word recognition. Although behavioral and electrophysiological studies suggest that high frequency words are processed more quickly than low frequency words, the degree to which this might be captured in fMRI is unclear. Similarly, although neighborhood density effects are widely reported in behavioral studies (with words from dense neighborhoods typically being more difficult to process), the degree to which lexical competition effects may differ with age is unclear. \n\n\n## MATERIALS AND METHODS \n  \nStimuli, data, and analysis scripts are available from  . \n\n### Participants \n  \nWe recruited two groups of participants (young and older adults) for this study. The young adults were 29 self-reported healthy, right-handed adults, aged 19\u201330 years (  M   = 23.8,   SD   = 2.9, 19 female), and were recruited via the Washington University in St. Louis Department of Psychological and Brain Sciences Subject Pool. Older adult participants were 32 self-reported healthy, right-handed adults, aged 65\u201381 years (  M   = 71.0,   SD   = 5.0, 17 female). All participants self-reported themselves to be native speakers of American English with no history of neurological difficulty, and with normal hearing (and no history of a diagnosed hearing problem). Participants were compensated for their participation, and all provided informed consent commensurate with practices approved by the Washington University in St. Louis Institutional Review Board. \n\nAudiograms were collected on a subset of eight young and nine older participants using pure-tone audiometry ( ). We summarized hearing ability using a better-ear pure tone average (PTA) at 1, 2, and 4 kHz. PTAs in participants\u2019 better hearing ears ranged from \u22123.33 to 8.33 dB HL in young adults (  M   = 2.92,   SD   = 4.15), and 8.33 to 23.3 dB HL in older adults (  M   = 23.3,   SD   = 9.17). \n  \nExperiment overview. (a) Audiograms for the subset of participants on whom hearing was available for left and right ears. Individual participants are shown in thin lines, group means in thick lines. (b) Frequency of occurrence and phonological neighborhood density for the 240 experimental items. (c) Task design for attentive listening and word repetition tasks. (d) Behavioral accuracy for the repetition condition for young and older adults. HAL = Hyperspace Analogue to Language, EPI = echo planar imaging. \n  \n\n### Materials \n  \nStimuli for this study were 375 monosyllabic consonant-vowel-consonant words. The auditory stimuli were recorded at 48 kHz using a 16-bit digital-to-analog converter with an Audio Technica 2035 microphone in a quiet room. Words were spoken by a female speaker with a standard American dialect. Root-mean-square amplitude of the stimuli was equated. \n\nOut of the full set of words, 75 words were vocoded using a single channel with white noise as a carrier signal ( ) using   jp_vocode.m   from  . These stimuli were used for an unintelligible baseline \u201cnoise\u201d condition. The remaining 300 words were divided into five lists of 60 words, using MATCH software ( ), and were balanced for word frequency (as measured by the log of the Hyperspace Analogue to Language dataset), orthographic length, concreteness ( ), and familiarity ( ). The distribution of word frequency and phonological neighborhood density are shown in  . \n\nOne of these lists was combined with 15 of the noise vocoded words and used for word repetition task practice outside of the scanner. The remaining four lists of 60 words served as the critical items inside the scanner, with half of the lists used for attentive listening (120 total words) and the other half for word repetition (120 total words). Word lists were counterbalanced such that each word was presented in both \u201clisten\u201d and \u201crepeat\u201d conditions across participants. \n\n\n### Procedure \n  \nPrior to scanning, participants were taken to a quiet room. (The room was not sound isolated and low frequency noise from the building heating, ventilation, and air conditioning system was typically present.) During that time participants provided informed consent, completed demographic questionnaires, and a subset had their hearing tested using a calibrated Maico MA40 portable audiometer (Maico Diagnostics, Inc., Eden Prairie MN) by an audiologist-trained researcher. \n\nParticipants were then instructed for the two tasks they would perform in the scanner: attentive listening and word repetition. During attentive listening, participants were asked to stay alert, still, and keep their eyes focused on a fixation cross while listening to a sequence of auditory sounds, including words, silence, and noise (single-channel noise vocoded words). During word repetition, participants were asked to do the same as in attentive listening, with the addition of repeating the word they just heard aloud. Participants were instructed to repeat the words following the volume acquisition after each word ( ). Participants were told to give their best guess if they could not understand a word. Participants practiced a simulation of the word repetition task until the experimenter was confident that the participant understood the pacing and the nature of the task. Sound levels were adjusted to achieve audible presentations at the beginning of the study and thereafter not adjusted. \n\nFunctional MRI scanning took place over the course of four scanning blocks, where participants alternated between blocks of attentive listening and word repetition ( ). The order of blocks was counterbalanced such that participants were equally likely to begin with a word repetition or an attentive listening block. During word repetition, participants\u2019 spoken responses were recorded using an in-bore Fibersound optical microphone. These responses were scored for accuracy offline by a research assistant ( ). \n\n\n### MRI Data Acquisition and Processing \n  \nThe MRI data collected in this study are available from   ( ). MRI data were acquired using a Siemens Prisma scanner (Siemens Medical Systems) at 3 T equipped with a 32-channel head coil. Scan sequences began with a T1-weighted structural volume using an MPRAGE sequence (repetition time [TR] = 2.4 s, echo time [TE] = 2.2 ms, flip angle = 8\u00b0, 300 \u00d7 320 matrix, voxel size = 0.8 mm isotropic). Blood oxygenation level-dependent fMRI images were acquired using a multiband echo planar imaging sequence ( ; TR = 3.07 s, TA = 0.770 s, TE = 37 ms, flip angle = 37\u00b0, voxel size = 2 mm isotropic, multiband factor = 8). (The flip angle was suboptimal due to an error setting up the sequences; although discovered partway through the study, we left it unchanged to maintain consistent data quality. With a TR of ~3 s we would expect a better signal-to-noise ratio with a flip angle of 90\u00b0.) We used a sparse imaging design in which there was a 2.3 s delay between scanning acquisitions and the TR was longer than the acquisition time to allow for minimal scanning noise during stimulus presentation and audio recording of participant responses ( ;  ). \n\nAnalysis of the MRI data was performed using Automatic Analysis version 5.4.0 ( ; RRID:SCR_003560), which scripted a combination of SPM12 (Wellcome Trust Centre for Neuroimaging) version 7487 (RRID:SCR_007037) and FMRIB Software Library (FSL; FMRIB Analysis Group;  ) version 6.0.1 (RRID:SCR_002823). \n\nData were realigned using rigid-body image registration, and functional data were coregistered with the bias-corrected T1-weighted structural image. Spatial and functional images were normalized to MNI space using a unified segmentation approach ( ), and resampled to 2 mm. Finally, the functional data were smoothed using an 8 mm full width at half maximum Gaussian kernel. \n\nFor the attentive listening condition, we did not have measures of accuracy, so we analyzed all trials. For the repetition condition, we analyzed only trials associated with correct responses. For both tasks, we modeled the noise condition in addition to words. Finally, we included three parametric modulators for word events: word frequency, phonological neighborhood density, and their interaction. To avoid order effects ( ), these were not orthogonalized. \n\nMotion effects were of particular importance given that participants were speaking during the repetition condition. To mitigate the effects of motion, we used a thresholding approach in which high motion frames were individually modeled for each subject using a delta function in the general linear model (see, e.g.,  ). Motion was quantified using framewise displacement (FD), calculated from the six motion parameters estimated during realignment, assuming the head is a sphere having a radius of 50 mm ( ). We then chose an FD threshold (0.561) that we used for all participants. Our rationale was that some participants move more, and thus produce worse data; we therefore wanted to use a single threshold for all participants, resulting in more data exclusion from high-motion participants. This threshold resulted in 2.2\u201319.4% (  M   = 6.21,   SD   = 4.45) data exclusion for the young adults and 2.8\u201358.4% (  M   = 22.6,   SD   = 15.3) data exclusion for the older adults. For each frame exceeding this threshold, we added a column to that participant\u2019s design matrix consisting of a delta function at the time point in question, which effectively excludes the variance of that frame from the model. \n\nContrast images from single subject analyses were analyzed at the second level using permutation testing (FSL   randomise  ; 5,000 permutations;  ), with a cluster-forming threshold of   p   < 0.001 (uncorrected) and results corrected for multiple comparisons based on cluster extent (  p   < 0.05). Images (contrast images and unthresholded   t   maps) are available from   ( ). Anatomical localization was performed using converging evidence from author experience ( ) viewing statistical maps overlaid in MRIcroGL ( ), supplemented by atlas labels ( ). \n\nFor region of interest (ROI) analysis of primary auditory cortex, we used probabilistic maps based on postmortem human histological staining ( ), available in the SPM Anatomy toolbox ( ; RRID:SCR_013273). We created a binary mask for regions Te1.0 and Te1.1 and then extracted parameter estimates for noise and word contrasts for the attentive listening and repetition conditions from each participant\u2019s first-level analyses by averaging over all voxels in each ROI (left auditory, right auditory). \n\nOutputs from analysis stages used for quality control are available from   in the aa_report folder. \n\n\n\n## RESULTS \n  \n### Behavioral Data \n  \nWe analyzed the accuracy data using a linear mixed effects analysis, implemented using the   lme4   and   lmerTest   packages in R version 3.6.2 ( ;  ; RRID:SCR_001905). Because trial-level accuracy data was binary, we used logistic regression. We first tested for age differences using a model that included age group as a fixed factor and subject as a random factor:  \nm0 <- glmer(accuracy ~ age_group + (1 | subject), \n\n\u2003\u2003\u2003data = df, family = \"binomial\") \n \n\nThe estimate of age_group was \u22121.4929 (  SE   = 0.1902),   p   = 4.24e\u221215, consistent with a main effect of age (older adults performing more poorly). Because of the ceiling effects and the lack of variability in the young adult data, we ran an additional analysis only in the older adults, using a model that included neighborhood density and word frequency as fixed factors, and item and subject as random factors:  \nm1 <- glmer(accuracy ~ neighborhood_density * log_freq + \n\n\u2003\u2003\u2003(1 | word) + (1 | subject), \n\n\u2003\u2003\u2003data = df, family = \"binomial\") \n \n\nThe model failed to converge when including a more complex random effect structure. The results of this model are shown in  . There were no significant effects of neighborhood density or word frequency in the accuracy data. \n  \nFixed effects results for accuracy data model \n    \nThere are many reasons a participant might make an incorrect response, and our primary interest is in the processes supporting successful comprehension. Thus, for the fMRI analyses, we restricted our analyses to correct trials only. \n\n\n### Imaging Data \n  \nWe began by looking at activity in the auditory cortex, followed by whole-brain analyses. Activity in left and right auditory cortex for noise and word conditions for young and older adults is shown in  . We analyzed these data using a linear mixed effects analysis, implemented using the   lme4   and   lmerTest   packages in R version 3.6.2 (RRID:SCR_001905). The full model included task (listen, repeat), stimulus (word, noise), hemisphere (left, right), age group (young, older), and accuracy on the repetition task as fixed factors, with subject, stimulus type, and task as random factors:  \nm1 <- lmer(activity ~ task * stimulus * hemisphere * agegroup + accuracy + \n\n\u2003\u2003\u2003(1 + stimulus * task | subject), \n\n\u2003\u2003\u2003data = dfleftright) \n When including hemisphere as an additional random factor, the model failed to converge, and as our main interests lay elsewhere we settled on the above model. \n  \nActivity in auditory cortex regions of interest. (a) Activity (parameter estimates, arbitrary units) for left and right auditory cortex as a function of age group and task. Participants are indicated by individual dots; mean \u00b1 standard error indicated by error bars. (b) Activity for left and right auditory cortex during the word repetition task in older adults as a function of accuracy and hearing (hearing only available in a subset of participants). \n  \nFull model results are shown in  . The   p   values were obtained from the   lmerTest   package using the Satterthwaite method for degrees of freedom and   t   statistics. We found significant interactions between task and stimulus, consistent with a greater degree of activation for words relative to noise in the repetition task compared to the attentive listening task. Importantly, there was a significant interaction between stimulus and age group, consistent with greater age differences for words relative to noise. We verified this with follow-up   t   tests, collapsing over hemisphere, which showed a significant difference in activity between young and older adults for words, attentive listening:   t  (57.973) = 3.1428,   p   = 0.002636; repetition:   t  (56.619) = 2.5583,   p   = 0.01322, but not for noise, attentive listening:   t  (58.559) = 0.66361,   p   = 0.5095; repetition:   t  (57.241) = 0.19028,   p   = 0.8498. None of the other main effects or interactions were significant. \n  \nFixed effects results for auditory cortex model \n    \nTo explore the possible contribution of other factors to older adults\u2019 reduced activity in the auditory cortex, we conducted a series of exploratory correlation analyses with accuracy, hearing, and movement parameters from fMRI (median FD). None of these analyses showed significant correlations with auditory cortex activity. Correlations for accuracy and hearing in the word repetition task are shown in  . Overall, we interpret these results as being consistent with less auditory activity for the older adults during spoken word perception (but not during our nonspeech control condition). \n\nTo complement the ROI analyses, we next performed whole-brain analyses for all conditions of interest. Activity for word perception in the attentive listening condition (greater than the noise baseline) is shown in   (with maxima listed in  \u2013 ). As expected, both young and older adults showed significant activity in the bilateral superior temporal cortex. Young adults showed significantly stronger activity in the superior temporal cortex near the auditory cortex. There were no regions in which older adults showed greater activity than young adults. \n  \nWhole-brain activity for the attentive listening condition. Top: Unthresholded parameter estimates. Middle: Unthresholded   t   maps. Bottom: Thresholded   t   maps (  p   < 0.05, cluster corrected). White ovals highlight left and right auditory cortex. \n    \nPeak activations for attentive listening condition greater than noise, young adults \n    \nPeak activations for attentive listening condition greater than noise, older adults \n    \nPeak activations for attentive listening condition greater than noise, young > older adults \n  \nIn addition to the overall pattern associated with word perception, we examined psycholinguistic effects of word frequency and phonological neighborhood density using a parametric modulation analysis. There were no significant effects of either word frequency or neighborhood density in the attentive listening condition. \n\nActivity for word perception in the repetition condition (relative to a noise baseline) is shown in   (with maxima in  \u2013 ). Again, both young and older adults showed significant activity in the bilateral temporal cortex, as well as frontal regions related to articulatory planning, including the premotor cortex and the supplemental motor area. As with the attentive listening condition, young adults showed significantly more activity in superior temporal regions near the auditory cortex. There were no regions where older adults showed more activity than the young adults. \n  \nWhole-brain activity for the repetition condition (correct responses only). Top: Unthresholded parameter estimates. Middle: Unthresholded   t   maps. Bottom: Thresholded   t   maps (  p   < 0.05, cluster corrected). White ovals highlight left and right auditory cortex. \n    \nPeak activations for repetition condition greater than noise, young adults \n    \nPeak activations for repetition condition greater than noise, older adults \n    \nPeak activations for repetition condition greater than noise, young > older adults \n  \nIn addition to the overall pattern associated with word perception, we examined psycholinguistic effects of word frequency and phonological neighborhood density using a parametric modulation analysis. There were no significant effects of either word frequency or neighborhood density in the repetition condition. \n\nFinally, we directly compared the attentive listening and repetition conditions, shown in   (with maxima in   and  ). Compared to the attentive listening condition, during the repetition condition both young and older listeners showed increased activity in motor and premotor cortex. There were no significant differences between young and older adults. \n  \nWhole-brain activity for the repetition condition > attentive listening. Top: Unthresholded parameter estimates. Middle: Unthresholded   t   maps. Bottom: Thresholded   t   maps (  p   < 0.05, cluster corrected). White ovals highlight left and right auditory cortex. There were no significant differences between young and older adults in the repetition > listening contrast. \n    \nPeak activations for word recognition in the repetition condition greater than listening condition, young adults \n    \nPeak activations for word recognition in the repetition condition greater than listening condition, older adults \n  \n\n\n## DISCUSSION \n  \nWe used fMRI to examine neural activity during spoken word recognition in quiet for young and older adult listeners. In both ROI and whole-brain analyses, we found converging evidence for reduced activity in the auditory cortex for the older adults. The age differences in auditory cortex activation were present in both the attentive listening task and the word repetition task: Although the repetition task resulted in more widespread activation overall, patterns of age-related differences in the auditory cortex were comparable. \n\nThere are a number of possible explanations for older adults\u2019 reduced activity during spoken word recognition. One possibility is that age differences in intelligibility might play a role. Intelligible speech is associated with increased activity in a broad network of frontal and temporal regions ( ;  ), and in prior studies of older adults, intelligibility has correlated with auditory cortex activity ( ). We restricted our analyses to correct responses in the repetition condition, and found no statistical support for a relationship between intelligibility and auditory cortex activation (although numerically, participants with better accuracy showed more activity than participants with worse accuracy). \n\nThe fact that young and older adults showed comparable activity in the auditory cortex during noise trials, with age differences emerging for word recognition trials, is significant. Group differences in activation could be driven not only by neural processing, but also by such factors as neurovascular coupling, goodness-of-fit of a canonical hemodynamic response, or movement within the scanner\u2014in other words, artifacts that might differentially impact model parameter estimates in young and older adults but are not of theoretical interest in this context. Although impossible to completely rule out, the selective age differences for speech (but not noise) are consistent with a condition-specific\u2014and thus we argue, neural\u2014interpretation. \n\nRecent evidence suggests age-related changes in temporal sensitivity in auditory regions can be detected with fMRI ( ). Although our current stimuli do not allow us to explore specific acoustic features, one possibility is that the age-related differences in auditory activity we observed reflect well-known changes in auditory cortical processing that occur in normal aging ( ). Given the increased acoustic complexity of the words relative to noise, acoustic processing differences might drive overall response differences. Such changes may also reflect decreased stimulation as a result of hearing loss; we had insufficient data to rule out this possibility. It is important to note that we cannot completely rule out audibility effects. Even though we limited our responses to correct identification trials, specific acoustic features may still have been less audible for the older adults. It remains an open question whether varying the presentation level of the stimuli would change the age effects we observed. \n\nAge differences in auditory processing are not the only explanation for our results. The auditory cortex is positioned in a hierarchy of speech processing regions that include both ascending and descending projections ( ;  ). The auditory cortex not only is sensitive to changes in acoustic information, but also reflects top-down effects of expectation and prediction ( ;  ;  ). Thus, the observed age differences in the auditory cortex may reflect differential top-down modulation of auditory activity in young and older adult listeners. \n\nIndeed, prior to conducting this study, we expected to observe increased activity (e.g., in the prefrontal cortex) for older adults relative to young adults, reflecting top-down compensation for reduced auditory sensitivity. Such activity would be consistent with increased cognitive demand during speech perception in listeners with hearing loss or other acoustic challenges ( ;  ). Although we were somewhat surprised not to see this, in retrospect, perhaps it would be expected. The stimuli in the current study were presented in quiet, and thus may not have challenged perception sufficiently to robustly engage frontal brain networks. We conclude that during perception of acoustically clear words, older adults do not seem to require additional resources from the frontal cortex; whether this changes with increasing speech demands (either acoustic or linguistic) remains an open question. \n\nWe did not observe significant effects of either word frequency or phonological neighborhood density on activity during spoken word recognition. These results stand in contrast to prior studies showing frequency effects in visual word perception in fMRI ( ;  ), and word frequency effects in electrophysiological responses ( ). Prior fMRI studies of lexical competition (including phonological neighborhood density) have been mixed, with some studies finding effects ( ) and others not ( ). It could be that a wider range of frequency or density or a greater number of stimuli would be needed to identify such effects. \n\nFinally, we found largely comparable age differences in the attentive listening and repetition conditions in the auditory cortex. The similarity of the results suggests that using a repetition task may be a reasonable choice in studies of spoken word recognition: Although repetition tasks necessarily engage regions related to articulation and hearing one\u2019s own voice, in our data these were not differentially affected by age. An advantage of using a repetition task, of course, is that trial-by-trial accuracy measures can be obtained, which are frequently useful. It is worth noting that our finding of comparable activity in young and older adults for attentive listening and repetition tasks may not generalize to other stimuli or tasks ( ;  ). \n\nA significant limitation of our current study is that we only collected hearing sensitivity data on a minority of our participants. Thus, although we saw a trend toward poorer hearing being associated with reduced auditory cortex activation, it is challenging to draw any firm conclusions regarding the relationship between hearing sensitivity and brain activity. Prior studies using sentence-level materials have found relationships between hearing sensitivity and brain activity in both young ( ) and older ( ) adults. Future investigations with a larger sample of participants with hearing data will be needed to further explore the effects of hearing in spoken word recognition. \n\nFrom a broader perspective, the link between spoken word recognition and everyday communication is not always straightforward. Much of our everyday communication occurs in the context of semantically meaningful, coherent sentences, frequently with the added availability of visual speech and gesture cues. Given potential age differences in reliance on many of these cues\u2014including older adults\u2019 seemingly greater reliance on semantic context ( ;  ;  )\u2014it seems likely that our findings using isolated spoken words cannot be extrapolated to richer naturalistic settings. \n\nIn summary, we observed largely overlapping brain regions supporting spoken word recognition in young and older adults in the absence of background noise. Older adults showed less activity than young adults in the auditory cortex when listening to words, but not noise. These patterns of age difference were present regardless of the task (attentive listening vs. repetition). \n\n\n## ACKNOWLEDGMENTS \n  \nResearch reported here was funded by grant R01 DC014281 from the US National Institutes of Health. The multiband echo planar imaging sequence was provided by the University of Minnesota Center for Magnetic Resonance Research. We are grateful to Linda Hood for assistance with data collection, and to Henry Greenstein, Ben Muller, Olivia Murray, Connor Perkins, and Tracy Zhang for help with data scoring. \n\n\n## FUNDING INFORMATION \n  \nJonathan E. Peelle, National Institute on Deafness and Other Communication Disorders ( ), Award ID: R01 DC014281. \n\n\n## AUTHOR CONTRIBUTIONS \n  \nChad S. Rogers: Conceptualization: Equal; Data curation: Equal; Investigation: Equal; Project administration: Equal; Supervision: Supporting; Validation: Equal; Writing\u2013Review & Editing: Equal. Michael S. Jones: Formal analysis: Lead; Methodology: Equal; Software: Lead; Validation: Lead; Writing\u2013Review & Editing: Equal. Sarah McConkey: Investigation: Equal; Project administration: Equal; Writing\u2013Review & Editing: Equal. Brent Spehar: Conceptualization: Equal; Investigation: Supporting; Resources: Supporting; Writing\u2013Review & Editing: Equal. Kristin J. Van Engen: Conceptualization: Equal; Funding acquisition: Supporting; Project administration: Equal; Writing\u2013Review & Editing: Equal. Mitchell S. Sommers: Conceptualization: Equal; Funding acquisition: Supporting; Project administration: Supporting; Writing\u2013Review & Editing: Equal. Jonathan E. Peelle: Conceptualization: Equal; Data curation: Equal; Formal analysis: Equal; Funding acquisition: Lead; Project administration: Equal; Supervision: Lead; Visualization: Lead; Writing\u2013Original Draft: Lead; Writing\u2013Review & Editing: Equal. \n\n \n", "metadata": {"pmcid": 8318202, "text_md5": "8143bd723654bdf1385cf807c530bc44", "field_positions": {"authors": [0, 150], "journal": [151, 172], "publication_year": [174, 178], "title": [189, 271], "keywords": [285, 337], "abstract": [350, 2048], "body": [2057, 36320]}, "batch": 1, "pmid": 34327333, "doi": "10.1162/nol_a_00021", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8318202", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8318202"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8318202\">8318202</a>", "list_title": "PMC8318202  Age-Related Differences in Auditory Cortex Activity During Spoken Word Recognition"}
