{"text": "Limanowski, Jakub and Blankenburg, Felix\nFront Hum Neurosci, 2018\n\n# Title\n\nFronto-Parietal Brain Responses to Visuotactile Congruence in an Anatomical Reference Frame\n\n# Keywords\n\nbody representation\nmultisensory integration\nperipersonal space\nrubber hand illusion\ntouch\n\n\n# Abstract\n \nSpatially and temporally congruent visuotactile stimulation of a fake hand together with one\u2019s real hand may result in an illusory self-attribution of the fake hand. Although this illusion relies on a representation of the two touched body parts in external space, there is tentative evidence that, for the illusion to occur, the seen and felt touches also need to be congruent in an anatomical reference frame. We used functional magnetic resonance imaging and a somatotopical, virtual reality-based setup to isolate the neuronal basis of such a comparison. Participants\u2019 index or little finger was synchronously touched with the index or little finger of a virtual hand, under congruent or incongruent orientations of the real and virtual hands. The left ventral premotor cortex responded significantly more strongly to visuotactile co-stimulation of the same versus different fingers of the virtual and real hand. Conversely, the left anterior intraparietal sulcus responded significantly more strongly to co-stimulation of different versus same fingers. Both responses were independent of hand orientation congruence and of spatial congruence of the visuotactile stimuli. Our results suggest that fronto-parietal areas previously associated with multisensory processing within peripersonal space and with tactile remapping evaluate the congruence of visuotactile stimulation on the body according to an anatomical reference frame. \n \n\n# Body\n \n## Introduction \n  \nWhen I see a body being touched and at the same time feel touch on the corresponding part of my skin, I know immediately that it is   my   body that I see being touched. If I felt the touch after observing it, or if I felt it on a different body part, I would very likely conclude that the body I see being touched is not mine. The brain constantly has to solve such causal inference problems when deciding whether two stimuli, registered by separate modalities, should be attributed to the same cause (i.e., \u201cintegrated\u201d into one multisensory percept) or to different causes ( ;  ;  ;  ;  ;  ). By nature, multisensory integration and the underlying causal inference are probabilistic, and inherently flexible. This is also the case for seen and felt touch, as demonstrated by the rubber hand illusion (RHI): when participants see a fake hand being touched and simultaneously feel a touch on the corresponding location of their real hand, they often report \u201cfeeling\u201d the touch on the fake hand ( ). Crucially, the RHI only occurs if prior constraints of a pre-existing body model (an anatomically plausible shape and position of the fake hand) and basic rules of multisensory integration are satisfied\u2014i.e., if the seen and felt touch occurs at the same time and at a corresponding location ( ;  ;  ;  ;  ). \n\nBrain imaging has established a link of the RHI to activity of the ventral premotor cortex (PMv) and the intraparietal sulcus (IPS,  ;  ;  ;  ;  ). Research on non-human primates and, more recently, on humans has shown that the PMv and IPS contain neurons with visual and tactile receptive fields ( ;  ;  ;  ). The receptive fields of such visuotactile neurons are relatively large and often anchored to specific body parts, i.e., they respond to touch on that body part and to visual stimuli entering the space immediately surrounding it\u2014the \u201cperipersonal space\u201d (PPS;  ;  ;  ;  ). A multisensory representation of the upper limbs and the PPS around them is thought to ultimately serve for action control, particularly for defending the body ( ). The illusory change of the multisensory body representation induced by the RHI seems to be accompanied by a corresponding recalibration of PPS onto the rubber hand ( ). It has hence been proposed that during the induction of the RHI, the PMv and IPS work together to integrate spatially and temporally congruent visual and tactile stimuli within the approximate limits of PPS anchored to the real hand, which after induction leads to a remapping of PPS and may even lead to an illusory self-attribution of the fake hand ( ;  ;  ). In sum, a successful induction of the RHI mandates that the seen and felt touches occur on the same location of two (fake and real) correspondingly oriented body parts, within approximate limits defined by the PPS of the real hand prior to induction of the illusion. \n\nHowever, when the body is touched, the stimulus is initially processed in a somatotopical, skin-based reference frame \u2013 implying the need for a \u201cremapping\u201d into external coordinate frames for comparison with information from other modalities such as with vision during the RHI ( ;  ;  ). Although the comparison processes underlying the RHI rely on a multisensory representation of the body (and the touch on it) in external space, there is also evidence for the involvement of a multisensory representation of the body\u2019s structure that operates in an anatomical or homuncular reference frame ( ;  ;  ). Interestingly, in a behavioral study,   have shown that the RHI remained despite mismatches in the fake and real hands\u2019 position, as long as stimulation was congruent in a \u201chand-centered spatial reference frame.\u201d This indeed suggests that anatomically based comparisons may also determine the visuotactile integration process and the resulting self-attribution of the fake hand, and thus speaks to proposals of a distinct multisensory body representation according to an anatomical or homuncular reference frame. In functional imaging research on the RHI, however, the processing of visuotactile congruence in an anatomical reference frame has so far received little attention. \n\nTherefore, we investigated human brain activity during visuotactile co-stimulation of a virtual hand and the real unseen hand at varying anatomical locations independently of the congruence of the hands\u2019 orientations. We used a virtual reality-based experimental setup with stimulation locations on the index and little finger of the right hand, repeatedly varying both the virtual and real hands\u2019 orientation, and controlling for spatial attention with a catch-trial task. This setup allowed us to isolate the effects of visuotactile congruence in an anatomical reference frame independent of visuoproprioceptive congruence and of external spatial visuotactile congruence. We stimulated the hands only briefly, because arguably the most interesting observations about the brain activity underlying the RHI can be made during the (usually several seconds long) \u201cpre-illusion\u201d period leading up to the illusion, during which the brain\u2019s multisensory comparison and integration processes are most strongly engaged ( ;  ,  ). We hypothesized that although tactile remapping itself is a fast process ( ;  ), the decision of whether multisensory input during an attempted RHI induction should be integrated or not may as noted also rely on anatomical body representations ( ;  ). We speculated that these visuotactile comparison and evaluation processes would engage the fronto-parietal brain areas previously implied in a multisensory representation of the PPS, i.e., the PMv and IPS. \n\n\n## Materials and Methods \n  \n### Participants \n  \nTwenty healthy, right-handed volunteers (7 male, mean age = 28 years, range = 21\u201340, normal or corrected-to-normal vision) participated in the experiment, which was approved by the ethics committee of the Freie Universit\u00e4t Berlin and conducted in accordance with the approval. \n\n\n### Experimental Design and Procedure \n  \nDuring the experiment, participants lay inside the fMRI scanner with their right hand placed above their chest at a natural angle of about 45\u00b0 and comfortably fixed in a custom foam-padded apparatus (  Figure   , modified from  ). The apparatus holding the hand could be rotated back-and-forth by the experimenter pulling on two nylon strings from outside of the scanner bore, so that either the palm or the back of the hand was facing the participant. We used stereoscopic goggles (VisuaSTIM, 800 \u00d7 600 pixels, 30\u00b0 eye field) and the Blender graphics software package  to present participants a photorealistic virtual hand in 3D in a similar, anatomically plausible position and location in space with respect to their real arm, with either the palm or the back of the virtual hand facing the participant. The real hand was always hidden from view by the goggles. The participant\u2019s head was slightly tilted to align the perceived real and virtual hand locations as much as possible within the spatial constraints of the head coil, and participants were asked if the respective palm or back facing orientations of the virtual hand seemed to them plausibly aligned to their real unseen hand. \n  \nExperimental design.   (A)   Participants saw a photorealistic right virtual hand in either a palm or back facing orientation (presented in 3D via stereoscopic goggles), with their real unseen right hand placed above their chest, likewise palm or back facing. The orientation of the real unseen hand was changed by the experimenter in between conditions.   (B)   The virtual hand could be touched at the index or little finger by moving rods (both shown here for illustrative purpose). The participant simultaneously received electrotactile impulses at a corresponding location of the real index or little finger (both locations schematically indicated).   (C)   Schematic partial stimulus sequence. Between stimulation blocks, the orientation of the participant\u2019s hand was passively switched (1 s rotation by the experimenter) between palm and back facing. During stimulation, the virtual hand was randomly displayed in a palm or back facing orientation (both orientations were anatomically plausible); visual and tactile stimulation locations were also random. Thus, each block (6 s, separated by an 11\u201317 s fixation-only baseline) was an unpredictable combination of touch applied to the same (VT+) or a different finger (VT\u2013) of the real and virtual hand, under congruent (VP+) or incongruent (VP\u2013) hand orientations.   (D)   Each of the 4 conditions consisted of 4 different combinations of touch locations and arm orientations. \n  \nParticipants fixated a white dot in the middle of the visual display throughout the entire experiment. During each 6 s long stimulation block, the virtual hand was presented in one of two orientations (palm or back facing the participant) and was touched by virtual rods at either the index or the little finger (  Figure   ). Simultaneously, the index or little finger of the participant\u2019s real hand was stimulated by electrotactile impulses (200 \u03bcs long monophasic square wave pulses generated by a bipolar constant current stimulator, Digitimer DS7), delivered via MR-compatible adhesive electrodes attached to the respective finger\u2019s first phalanx \u2013 the stimulation locations were constant throughout each stimulation block. The intensity, location, and timing of the electrotactile impulse was carefully adjusted for each participant before the scanning session, so that it matched the anatomical location where the touch was seen on the virtual hand, and felt synchronous with it; this was adjusted between the individual runs if necessary. Six such simultaneously seen and felt touches were delivered in each stimulation block (i.e., each 6 s long presentation of a particular combination), separated by a random inter-stimulus interval (0, 167, or 500 ms) to render the stimulation sequence unpredictable (  Figure   ). The stimulation blocks were separated by a randomly jittered 11\u201317 s fixation-only inter-block interval (IBI). During each IBI, 3\u20136 s (randomly jittered) after IBI onset, the experimenter swiftly rotated the participant\u2019s hand from palm to back facing or vice versa while participants only saw the fixation dot; the rotation took 1 s and was announced and timed by auditory cues to the experimenter. To avoid surprise, the rotation was also announced to the participant by a brief disappearance of the fixation dot for 0.5 s prior to rotation onset. \n\nThe combination of touch locations and arm orientations resulted in 16 stimulation combinations, which were assigned to 4 conditions (\u00e0 4 stimulation combinations,   Figure   ): same finger touched, congruent hand orientations (VT+VP+); same finger touched, incongruent hand orientations (VT+VP-); different fingers touched, congruent hand orientations (VT-VP+); different fingers touched, incongruent hand orientations (VT-VP-). Each combination was presented twice per run in randomized order, resulting in 32 stimulation blocks and \u223c11 min run length. Each participant completed 6 runs, with initial hand orientation counterbalanced across runs, and a practice session before scanning. \n\nTo control for attentional effects and to ensure constant fixation, we included a catch trial detection task. Throughout each run, the fixation dot unpredictably pulsated briefly (25% increase in size for 300 ms) 5\u20139 times. Participants had to report the number of pulsations verbally to the experimenter after each run. \n\nAfter the scanning session, participants completed a brief questionnaire comprising the following two statements, rated on a 7-point scale from -3 (\u201cdo not agree at all\u201d) to 3 (\u201cfully agree\u201d): \u201cI was able to distinguish whether the orientation of the virtual hand and my real hand was congruent (corresponding) or incongruent (not corresponding)\u201d and \u201cIt felt as if the virtual hand was my own hand\u201d; the latter statement was rated separately for each of the four stimulation conditions. The (ordinally scaled) questionnaire data were evaluated using non-parametric Friedman and Wilcoxon tests. For comparisons between condition scores, we further report the common language (CL) effect size as the proportion of matched pairs for which the score in one condition is higher than the score in the other condition ( ). \n\n\n### fMRI Data Acquisition, Preprocessing, and Analysis \n  \nThe fMRI data were recorded using a 3 T scanner (Tim Trio, Siemens, Germany), equipped with a 12-channel head coil. T2 -weighted images were acquired using a gradient echo-planar imaging sequence (3 mm \u00d7 3 mm \u00d7 3 mm voxels, 20% gap, matrix size = 64 \u00d7 64, TR = 2000 ms, TE = 30 ms, flip angle = 70\u00b0). For each participant, we recorded 6 runs \u00e0 329 functional image volumes, a GRE field map (TE1 = 10.00 ms, TE2 = 12.46 ms), and a T1-weighted structural image (3D MPRAGE, voxel size = 1 mm \u00d7 1 mm \u00d7 1 mm, FOV = 256 mm \u00d7 256 mm, 176 slices, TR = 1900 ms, TE = 2.52 ms, flip angle = 9\u00b0). fMRI data were preprocessed and analyzed using SPM12.  Artifacts at the slice-level were corrected using the ArtRepair toolbox ( ). Images were corrected for slice acquisition time differences, realigned and unwarped using the acquired field maps, normalized to MNI space and resliced to 2 mm voxel size using DARTEL, spatially smoothed with a 5 mm full width at half maximum Gaussian kernel, detrended ( ), and images featuring excessive movement were interpolated (ArtRepair). We fitted a general linear model (GLM, 300 s high-pass filter) to each participant with regressors modeling the stimulations, arm rotations, and catch trials. The first five principal components accounting for the most variance in the cerebrospinal fluid or white matter signal time course each ( ) were added alongside the realignment parameters as regressors of no interest. \n\nFirst-level contrast images were entered into a group-level flexible factorial design, with the factors condition type (see above), stimulation location (location of the seen touch: top-right or bottom-left), and real arm orientation. Note that the factor stimulation location could also be determined by the location of the felt touch (real index or little finger), which yielded virtually identical results. Activations in the whole brain were assessed for statistical significance applying a voxel-level threshold of   p   < 0.05, family-wise error (FWE) corrected for multiple comparisons. Based on strong prior hypotheses about the well-documented involvement of the left PMv in multisensory comparison and integration, we applied peak FWE-correction within a 10 mm radius spherical region of interest (ROI) centered on coordinates from our previous related study ( ;   x   = -38,   y   = 10,   z   = 28, obtained from contrasting synchronous versus asynchronous co-stimulation of a fake and a real arm). For completeness, we report in table format all activations greater than 5 voxels that survived an uncorrected threshold of   p   < 0.001, and we indicate the method used to correct for multiple comparisons. The resulting statistical parametric maps (SPMs) are projected onto the mean normalized structural image. Reported coordinates are in MNI space; the SPM Anatomy toolbox ( ) was used for anatomical reference. The unthresholded SPMs related to the figures presented here are available online at  . \n\n\n### Behavioral Control Experiment \n  \nWe conducted an additional behavioral control experiment to verify that the perceptual effects of our stimulation was analogous to typical RHI experiments as published previously. In the fMRI experiment, we were investigating visuotactile integration and comparison processes depending on anatomical congruence of seen and felt touches. This was a novel comparison, and thus differed from the typical control condition used as a contrast to the rubber hand illusion, namely, asynchronous stimulation of the fake and real hand. Although we applied stimulation only for a brief period of time (thus targeting the \u201cpre-illusion\u201d phase associated with the evaluation of multisensory inputs), one could wonder whether the stimulation we used would also reliably induce perceptual differences depending on visuotactile synchrony, and thus be comparable to published studies on the RHI. Therefore, we conducted an additional behavioral control experiment. Eight healthy participants (5 females, mean age = 30.5 years) completed a 4 runs, in between which the real hand\u2019s position was changed between palm and back facing. The 16 stimulation conditions were presented either with synchronous seen and felt touches (as in the fMRI experiment), or with asynchronous seen and felt touches (i.e., with an added delay of 300 ms between the seen and felt touch). We adjusted the stimulation for each participant and verified that the synchrony and asynchrony of stimulations were clearly perceived. As in the fMRI experiment, stimulation blocks were of 6 s duration, with 6 touches presented in a random rhythm. \n\nFurther, for any of the fully congruent and synchronous stimulations (i.e., the classical RHI condition) that was rated with at least 2, we included an additional presentation with longer stimulation duration (18 s), and asked the participant to indicate with a verbal response the exact moment at which she would experience the ownership illusion during the stimulation (if she experienced it at all). This verbal report, measured by the experimenter with a stopwatch, was taken as a marker of the illusion onset. \n\n\n\n## Results \n  \n### Behavioral Results \n  \nAll participants were able to distinguish whether the real and virtual hands were in congruent or incongruent orientations (mean affirmation rating = 2.75, standard deviation = 0.55; Wilcoxon signed-rank test,   z   = 4.18,   p   < 0.001). Participants also correctly detected 83.75% of the presented catch trials (  SD   = 9.87 %; false alarm rate = 1.41%,   SD   = 2.71%), which was well above chance level (two-tailed   t  -test,   p   < 0.001). \n\nA non-parametric Friedman test revealed significant ownership rating differences between conditions (  \u03c7   = 41.91,   p   < 0.001). A post-hoc analysis with Wilcoxon signed-rank tests and Bonferroni-corrected alpha levels further showed that ownership ratings were only significantly positive for the fully congruent condition (VT+VP+ vs. zero,   z   = 3.09,   p   < 0.01), which likewise was rated more positively than any other condition (all   z  s > 3.4, all   p  s < 0.001, all CL effect sizes > 0.80). Further, stimulation of the same vs. different fingers was rated significantly more positively even under visuoproprioceptive incongruence (VT+VP- vs. VT-VP-,   z   = 3.55,   p   < 0.001, CL effect size = 0.80). \n\nIn a separate behavioral experiment, another group of participants rated the intensity and reported the onset of illusory ownership during the same stimulations, with seen and felt touches delivered either synchronously (as in the main experiment) or asynchronously (with an added delay of 300 ms). The ownership ratings replicated the pattern observed in the ratings of the main experiment: a non-parametric Friedman test revealed significant ownership rating differences between conditions (  \u03c7   = 40.34,   p   < 0.001). A post-hoc analysis with Wilcoxon signed-rank tests showed that ownership ratings of the fully congruent and synchronous condition (VT+VP+ , i.e., the classical RHI condition) were significantly higher than any other condition (all   p  s < 0.01, CL effect size = 1). Further, for each of the four conditions, ownership ratings were significantly higher following synchronous versus asynchronous stimulation (all   p  s < 0.05, all CL effect sizes > 0.75). See   Table    for details. The mean reported illusion onset in the fully congruent, synchronous condition (the only condition in which the RHI was affirmed by participants) was at 5.44 s (  SD   = 1.46 s) after onset of stimulation. \n  \nAverage ownership ratings (with standard deviations in brackets) per condition for the fMRI experiment and the behavioral experiment, with added manipulation of synchrony vs. asynchrony of touches per condition. \n  \n\n### fMRI Results \n  \nAs expected, we observed a clear somatotopical representation of touch locations on the real fingers in the contralateral primary somatosensory cortex (S1, see   Figure   ). Contrasting all stimulations on the real index versus little finger revealed an activation cluster at a more lateral inferior location (  x   = -54,   y   = -20,   z   = 56,   T   = 7.47,   p   < 0.05, FWE-corrected); little versus index finger stimulation conversely more dorsally and medially (  x   = -44,   y   = -22,   z   = 68,   T   = 4.54,   p   < 0.001, uncorrected). Visual stimulation in the left versus right visual hemifield was correspondingly retinotopically reflected by significant (  p   < 0.05, corrected) activity increases in the primary and associative visual cortices contralateral to stimulation (  Figure   ). Catch trials produced significant activity increases in the bilateral anterior insulae, as well as in fronto-parietal and occipital areas; passive hand rotation produced significant activity increases in a somatomotor network, including the left S1, bilateral secondary somatosensory cortex, and bilateral LOTC. \n  \n (A)   Somatotopical organization of responses in the left S1 to tactile stimulation of the right (real) index finger (I) versus little finger (L) and vice versa, across all conditions. The corresponding unthresholded SPM is available at  .   (B)   Retinotopical organization of responses in the primary visual cortex and motion-sensitive lateral occipitotemporal cortex to visual stimulation in the right (R) versus left (L) hemifield, and vice versa, across all conditions. The corresponding unthresholded SPM is available at  . \n  \nIn our main analysis, we sought for effects of visuotactile congruence in an anatomical reference frame, i.e., for brain areas that would show response differences to stimulations of the same or different fingers of the real and the virtual hand (  Figure    and   Table   ). Stimulation of the same versus different fingers, i.e., the contrast   (VT+VP+ + VT+VP-) > (VT-VP+ + VT-VP-)  , produced significantly stronger responses in the left PMv (  p   < 0.05, FWE-corrected within an   a priori   defined ROI). The reverse comparison, i.e.,   (VT-VP+ + VT-VP-) > (VT+VP+ + VT+VP-)  , revealed significantly stronger responses to stimulation of different versus same fingers at the fundus of the left anterior IPS (aIPS,   p   < 0.05, FWE-corrected), bordering BA 2 of S1. This area of IPS was significantly activated by touch at both somatotopic locations (null conjunction of real index and little finger,   p   < 0.05, FWE-corrected). All of the reported activity differences were consistent over retinotopic and somatotopic stimulation locations, real and virtual hand orientations, and visuoproprioceptive congruence (i.e., we found no significant interaction effects, even when lowering the statistical threshold to   p   < 0.005, uncorrected). \n  \nSignificant activation differences related to the anatomical congruence of visuotactile stimulation. A region in the left PMv showed significantly stronger responses to touches applied to the same finger of the real and virtual hand (  p   < 0.05, FWE-corrected). Conversely, a region in the anterior part of the left aIPS showed significantly stronger responses to touches applied to different fingers of the two hands (  p   < 0.05, FWE-corrected). These response differences were independent of visuoproprioceptive congruence and spatial (external) visuotactile congruence. The bar plots show the contrast estimates and associated standard errors for each condition at the respective peak voxels. The SPMs are thresholded at   p   < 0.001, uncorrected, for display purposes. See   Table    for details. The corresponding unthresholded SPM is available at  . \n    \nActivations obtained from the contrasts same vs. different finger stimulation, and vice versa (  p   < 0.001, uncorrected; activations that survived FWE-correction for multiple comparisons are marked in bold font). \n    \nWe did not find any significant activity differences related to visuoproprioceptive congruence, i.e., neither from the main effect nor its interaction with visuotactile congruence. \n\n\n\n## Discussion \n  \nIn this study, we used a somatotopical, virtual reality-based setup to investigate the effects of brief periods of synchronous visuotactile stimulation at anatomically congruent versus incongruent locations (i.e., index or little finger) of a virtual and the real right hand on human brain activity. In contrast to previous work, we aimed to identify BOLD signal correlates of early multisensory comparison processes during the stimulation phase prior to a full-blown RHI experience, and to examine if these comparisons were potentially made with reference to an anatomical (homuncular) reference frame. \n\nfMRI revealed that the left PMv responded significantly more strongly when the touch was seen on the same finger of the virtual hand as it was felt on the real hand. Conversely, the left aIPS responded more strongly when the touch was seen and felt on different fingers. These results suggest fronto-parietal brain areas previously linked to visuotactile integration in PPS ( ;  ;  ;  ) in evaluating visuotactile congruence according to an anatomical reference frame. Interestingly, even following the relatively brief stimulation period, participants reported illusory virtual hand ownership under fully congruent stimulation; these results were replicated in an additional behavioral experiment, which revealed that experienced ownership was significantly higher for fully congruent synchronous versus asynchronous stimulation. Moreover, participants reported higher ownership for anatomically congruent versus incongruent stimulations also under mismatching hand orientations (i.e., visuoproprioceptive incongruence), which demonstrates a perceptual difference depending primarily on visuotactile congruence in an anatomical reference frame. However, these ratings were acquired post scanning and should therefore be interpreted with some caution. \n\nHuman brain imaging experiments have demonstrated that, during the RHI, the PMv and IPS integrate spatiotemporally congruent visual and tactile stimuli, within the approximate limits of PPS anchored to the real hand ( ;  ;  ,  ;  ;  ,  ). Behavioral studies had shown that the RHI also requires visual and tactile stimuli to be congruent in an anatomical reference frame ( ), but the neuronal correlates of this comparison were unknown. Our results provide evidence that the evaluation and potential integration of seen and felt touches based on an anatomical reference frame is implemented in the contralateral PMv and aIPS; they moreover suggest that such an evaluation of visuotactile input occurs very early \u2013 potentially before a complete RHI may be induced. This is tentatively supported by the fact that in the behavioral experiment, participants on average reported experiencing the RHI after 5.44 s, which almost covers the entire 6 s stimulation period in the fMRI experiment. \n\nThe PMv showed a preference for conditions in which the same finger of the real and virtual hand was stimulated. Our design allowed us to verify that this response pattern was independent of the orientation of the virtual and real hands, and thus independent of the spatial (external) location congruence of the visuotactile stimuli. The PMv is often considered to be the hierarchically highest level of the multisensory body representation targeted by the RHI, among other things based on the fact that activity in this area is often directly related to subjectively perceived body ownership ( ;  ;  ;  ). Our results support the PMv\u2019s assumed role in assigning body ownership based on a multisensory body and PPS representation, and add novel evidence that the PMv also takes into account the anatomical congruence of visuotactile stimuli when attributing them to one\u2019s body. \n\nConversely, the left aIPS responded more strongly when touch was seen and felt on different fingers of the real and virtual hand. Again, this response was independent of the hands\u2019 orientation, and of external spatial stimulus location and congruence. The IPS is a multisensory area that compares and aligns visual (external) and tactile (anatomical) reference frames ( ;  ;  ; cf.  ), and is a key area involved in various stages of multisensory integration, including evaluation of uncertainty of the individual sensory estimates ( ). Although tactile remapping itself is a fast, sub-second process ( ;  ), it is conceivable that brain areas involved in tactile remapping \u2013 and hence having a fundamental role in the processing of touch for multisensory comparisons \u2013 may also be involved in determining whether or not some synchronous visuo-tactile input (as during our stimulation) should be integrated. That such computational decisions may also be made within an anatomical reference frame has been suggested before ( ;  ). Our results suggest that the aIPS of the PPC may be essential to them. \n\nThis result may seem surprising, as previous studies have reported increased IPS activation during the RHI (e.g.,  ;  ;  ). However, we believe our findings may be reconciled with this work. During the RHI, the IPS is most likely involved in integrating the initially mismatching multisensory information and remapping of the corresponding reference frames ( ;  ), and therefore is specifically engaged during the early stimulation period before illusion onset ( ;  ). The previously observed increased connectivity between the IPS and PMv during the early phase of the RHI ( ;  ,  ) suggests that the IPS communicates with the PMv to enable body ownership, in line with neurocognitive models of the RHI ( ;  ). It may be noteworthy that the IPS activation observed here was located more anteriorly (peak y coordinate = -36, bordering S1) than activations we observed in the IPS during synchronous vs. asynchronous touch ( :   y   = -50; cf.  :   y   = -51;  :   y   = -44) or purely visuo-proprioceptive comparisons ( :   y   = -56). There are proposals of functional \u201csomatosensory-to-visual\u201d gradients in the IPS (e.g.,  ), and an exciting question for future research is whether during the RHI and similar manipulations, multiple comparison processes are in play along such gradients. \n\nIn a recent electroencephalography study on the RHI,   also found increased responses at very similar coordinates to ours (at the junction of IPS and S1) during co-stimulation of a fake hand in an incongruent versus congruent position. In their setup, the incongruent fake hand position implied anatomically incongruent stimulation locations on the real and fake hand\u2014therefore, tentatively, their finding aligns with ours (while we were, moreover, able to isolate the effect of anatomical visuotactile congruence from visuoproprioceptive congruence).   propose that during congruent touch (the RHI), somatosensory responses are attenuated in S1 and IPS to enable a resolution of multisensory conflict (cf.  ). Thus our results could also be interpreted as a relative attenuation of aIPS activity when congruent fingers were seen and felt touched\u2014and conversely, as a prediction error signal when different fingers were seen and felt touched. \n\nIn another line of research, the left IPS has also been implied in representing body   structure   ( ; cf.  ;  ). For example, the left IPS was found activated when participants pointed to different own body parts (versus different spatial locations,  ), or when they evaluated the distance between body parts ( ,  ). When two fingers of the left and right hand were touched synchronously, the aIPS response increased with structural (i.e., anatomical) distance between the fingers ( ,  ). In our case, aIPS activity likewise increased with increasing anatomical distance between stimulated fingers\u2014crucially, our visuotactile virtual reality setup allowed us to manipulate the congruence of touches on the fingers of the same hand (rather than touching the invisible left and right hands, cf.  ). \n\nIn sum, and in the light of these previous findings, a compelling interpretation of the aIPS activation differences observed in our study is that they reflect an early activation of an anatomical body representation, and perhaps an initial registration of the anatomical mismatch of seen and felt touches in the aIPS, as part of several comparison process of received multisensory information in the IPS. \n\nAn alternative interpretation of the observed aIPS responses could be that they reflected vicarious responses to observed touch. Recent research suggests that touches observed on other bodies may be represented on the anatomical map of one\u2019s own body ( ), a process that could involve the left aIPS ( ,  ;  ). Note that this interpretation of our findings would also imply a body representation in the aIPS that differentiates between seen and felt touches based on anatomical distance of the touched body parts. Future research will have to test these alternative but potentially complementary explanations against each other, and specifically clarify the role of the IPS in multisensory processes pre- and post-illusory ownership. \n\nFinally, it should be noted that the absence of any significant effects of visuoproprioceptive congruence (i.e., alignment of the hand positions) on brain activity was somewhat unexpected. While it may indeed suggest a general, anatomically based evaluation mechanism, this result contrasts with previous reports and should hence be evaluated by future work\u2014one possibility would be to compare periods of visuoproprioceptive (in)congruence with versus without visuotactile stimulation. \n\n\n## Conclusion \n  \nWe found that early anatomical comparisons during visuotactile co-stimulation of a fake and the real hand significantly modulated activity in the PMv and aIPS, brain areas previously linked to visuotactile integration in the PPS and to tactile remapping. These activation differences could not be explained by spatial congruence of the stimuli or by visuoproprioceptive congruence effects, but by visuotactile congruence in an anatomical reference frame. Thus, our results highlight the importance of anatomical congruence for attributing touch and even body parts to oneself, and support the proposal of body representation in multiple reference frames. \n\n\n## Author Contributions \n  \nJL and FB designed the research and wrote the manuscript. JL performed the research and analyzed the data. \n\n\n## Conflict of Interest Statement \n  \nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. \n\n \n", "metadata": {"pmcid": 5845128, "text_md5": "9ed118210e0a5d10a7106d59954fb89a", "field_positions": {"authors": [0, 40], "journal": [41, 59], "publication_year": [61, 65], "title": [76, 167], "keywords": [181, 272], "abstract": [285, 1724], "body": [1733, 36682]}, "batch": 1, "pmid": 29556183, "doi": "10.3389/fnhum.2018.00084", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5845128", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=5845128"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5845128\">5845128</a>", "list_title": "PMC5845128  Fronto-Parietal Brain Responses to Visuotactile Congruence in an Anatomical Reference Frame"}
