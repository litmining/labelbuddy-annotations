{"text": "Lau, Tatiana and Gershman, Samuel J and Cikara, Mina\neLife, 2020\n\n# Title\n\nSocial structure learning in human anterior insula\n\n# Keywords\n\nsocial groups\nstructure learning\nBayesian inference\nanterior insula\nmedial prefrontal cortex\nfMRI\nHuman\n\n\n# Abstract\n \nHumans form social coalitions in every society, yet we know little about how we learn and represent social group boundaries. Here we derive predictions from a computational model of latent structure learning to move beyond explicit category labels and interpersonal, or   dyadic  , similarity as the sole inputs to social group representations. Using a model-based analysis of functional neuroimaging data, we find that separate areas correlate with dyadic similarity and latent structure learning. Trial-by-trial estimates of \u2018allyship\u2019 based on dyadic similarity between participants and each agent recruited medial prefrontal cortex/pregenual anterior cingulate (pgACC). Latent social group structure-based allyship estimates, in contrast, recruited right anterior insula (rAI). Variability in the brain signal from rAI improved prediction of variability in ally-choice behavior, whereas variability from the pgACC did not. These results provide novel insights into the psychological and neural mechanisms by which people learn to distinguish \u2018us\u2019 from \u2018them.\u2019 \n   eLife digest  \nIn every society, people form social coalitions \u2014 we draw boundaries between 'us' and 'them'. But how do we decide who is one of 'us' and who is one of 'them'? One way is to use arbitrary categories. For example, we say that those living 49 degrees north of the Earth\u2019s equator are Canadian, whereas those living south of it are American. Another possibility is to use physical characteristics. But what about when neither of these options are available? \n\nBy monitoring brain activity in healthy volunteers learning about other people\u2019s political values, Lau et al. obtained insights into how people make these decisions. Participants lying in a brain scanner were asked to report their position on a political issue. They then learned the positions of three other hypothetical participants \u2013 A, B and C \u2013 on the same issue. After repeating this procedure for eight different issues, the volunteers had to decide whether they would align with A or with B on a 'mystery' political issue. \n\nSo how do participants choose between A and B? One possibility is that they simply choose whichever one has views most similar to their own. If this is the case, the views of hypothetical person C should not affect their decision. But in practice, C's views \u2013 specifically how much they resemble the volunteer's own \u2013 do influence whether the volunteer chooses A or B. This suggests that we choose our allies based on more than just their similarity to ourselves. \n\nUsing a mathematical model, Lau et al. show that volunteers also take into account how similar the views of the other \u2018participants\u2019 are to each other. In other words, they consider the structure of the social group as a whole. Moreover, the results from brain imaging show that different regions of the brain are active when volunteers track the structure of the entire group, as opposed to their own similarity with each individual. \n\nNotably though, the activity of the group-tracking region explains people's alignment choices better than the activity of the similarity-tracking region. This suggests that we base our judgments of 'us' versus 'them' more on the structure of the group as a whole than on our own similarity with individual group members. Understanding how we determine whether others are on the same \u2018team\u2019 as ourselves could ultimately help us find ways to reduce bias and discrimination between groups. \n \n\n# Body\n \n## Introduction \n  \nBeing able to distinguish \u2018us\u2019 from \u2018them\u2019 is a core social capacity ( ). In an increasingly interconnected world where people have multiple intersecting identities that guide their thoughts, feelings, and behaviors, being able to differentiate friend from foe is of paramount importance. Yet we know surprisingly little about how social group boundaries are learned and represented in the brain\u2014particularly in the absence of overt cues to individuals\u2019 group membership. \n\nOne dominant account is that people use judgments of similarity to one\u2019s self on some contextually relevant feature (e.g., skin tone). Accordingly, neuroimaging studies have attempted to identify an overlap between brain regions associated with self-referential processes and categorization of others as in-group members ( ;  ) A ventral region of medial prefrontal cortex (vmPFC), including pregenual anterior cingulate cortex (pgACC), is reliably associated with thinking about one\u2019s own and similar others\u2019 traits, mental states, and characteristics ( ;  ;  ). But are similarity-based estimates sufficient for categorizing others as in-group versus out-group members and informing subsequent behavior? \n\nClassic social psychological theories of intergroup relations indicate that there are other dimensions by which groups are defined ( ). Rather than prioritizing similarity to oneself, people may rely on functional relations between one\u2019s self and a target (e.g., 'Are you with me or against me?';  ). \n\nGiven that social categorization is such a flexible, dynamic process, how do people accumulate group structure information (especially in the absence of overt cues to group membership)? On one hand, they might try to characterize their ties with each individual (e.g., how well do I get along with Sue, with Dan, etc.). However, social group dynamics may be better captured by a model that integrates information about how agents relate to one another in addition to oneself (e.g., how do Sue and Dan get along with each other, and how do I get along with either of them?), which would allow perceivers to infer social\u00a0latent group structure. \n\nIf people represent social\u00a0latent group structure ( ) in addition to dyadic similarities, then even when two agents\u2019 choices (Agents A's\u00a0and B's)\u00a0are equally similar to their own, the presence of a third agent (Agent C) altering the group structure should influence their decisions ( ). Importantly, dyadic similarity accounts would not predict differential ally-choice behavior in these cases (because similarity is equated for the two agents in question). In other words, the key difference between the two models is whether or not the presence of the third agent can affect how the first two agents are perceived. \n   A formal account of social latent\u00a0structure learning.  \n(  A  ) Model schematic illustrating how choice patterns are transformed using Bayes\u2019 rule to create a posterior over different possible latent groupings of agents. (  B  ) Agents are represented as letters in an abstract space (P is the participant), where the distance between letters indicates the degree to which agents agree in their choices (i.e., choice overlap). Red ovals indicate the latent structures that have high posterior probability. Left: The placement of Agent C creates a cluster that includes both the participant and Agent B, which should increase estimates of Agent B as an ally. Right: The\u00a0placement of Agent\u00a0C excludes the\u00a0participant from the\u00a0cluster with Agents B and C, which should decrease estimates of Agent B as an ally. \n  \nTo determine whether people possess different neural mechanisms for dyadic similarity and latent group structure learning, we created a structure-learning task in which participants reported their own position on a political issue and then guessed and learned via feedback the positions of three other agents, Agents A, B, and C, on the same issue ( ). After repeating this for eight political issues, participants were shown pictures of two of the three agents and asked to indicate with which of the\u00a0two agents, Agent A or Agent B, they would align on an unknown political issue ( ;  ). We focused on political issues because recent evidence suggests that implicit bias and behavioral discrimination along political party lines is now as potent as bias against racial out-groups ( ;  ). \n   Order of events in task.  \n(  A  ) Learning Trials: Participants began every trial by seeing a political issue and reporting their personal stance on it. After receiving confirmation of their choice, they then guessed and received feedback on how the first agent responded to the same political issue and repeated this for the other two agents before moving onto a new political issue. (  B  ) Ally-choice Trial: After eight learning trials, participants saw photos of Agents\u00a0A and B sequentially in a random order and chose to align with\u00a0either Agent A or B on a \u2018mystery\u2019 (i.e., unknown) political issue. \n  \nThis design allowed us to i) investigate participants\u2019 trial-by-trial alignment signals based on dyadic similarity (with each respective agent), feature\u00a0similarity-over-agents, and social\u00a0latent structures, and ii) identify which brain regions tracked each of these representations. We then tested whether variability in the neural signal associated with these representations improved prediction of variability in participants\u2019 ally-choice behavior. \n\n\n## Results \n  \nWe scanned 42 participants using functional magnetic resonance imaging (fMRI) as they completed our structure-learning task. Each participant completed six runs; each run comprised learning about three novel agents across eight political issues and then choosing to ally with one of two agents on an unknown, \u2018mystery\u2019 issue. Each participant saw 48 political issues and learned about 18 novel agents in total. \n\n### Learning about other agents\u2019 policy preferences and ally-choice trials \n  \nTo model the probability of choosing Agent B\u2019s choice in the ally-choice trial as a function of social\u00a0latent structure, we used a logistic regression predicting whether our participants in the scanner (N\u00a0=\u00a042) chose Agent B\u2019s choice during the ally-choice trial as a function of Agent B\u2019s agreement and Agent C\u2019s agreement with the participant. (See Materials and methods for analysis of\u00a0the full N\u00a0=\u00a0333 sample.) Because Agent A\u2019s preferences were always the inverse of Agent B\u2019s, including Agent A\u2019s agreement would have created a multicollinearity problem (recall also that participants could only choose either Agent B or Agent A). Including random slopes to account for subject-level effects resulted in a singular fit of the model (i.e., overfitting), so we removed them. We compared the full model including both main effects and the interaction with simpler models (including only main effects or including only Agent B\u2019s agreement with the participant). Likelihood-ratio tests indicated that the fully saturated model with both main effects and the interaction term fit the data better than without the interaction term (\u03c7 (1)\u00a0=\u00a07.246, p=0.007). \n\nReplicating previous behavioral results ( ), we found that increasing Agent C\u2019s alignment with the participant made respondents more likely to choose Agent B on the ally-choice trial, above and beyond the participant\u2019s similarity with Agents A and B ( ). As a simple dyadic similarity account would predict, the model indicated a significant positive effect of Agent B\u2019s agreement in predicting the likelihood of choosing Agent B in the ally-choice trial,   b  \u00a0=\u00a02.325, Wald\u2019s   z  \u00a0=\u00a04.099, 95% CI [1.284, 3.519], p<0.001. However, as predicted by the latent structure learning account, the model also indicated a significant positive effect of Agent C\u2019s agreement in predicting the likelihood of choosing Agent B in the ally-choice trial,   b  \u00a0=\u00a01.322, Wald\u2019s   z  \u00a0=\u00a02.633, 95% CI [0.371, 2.349], p=0.008 ( ). This was qualified by a significant negative interaction between the agreements of Agent B and Agent C,   b  \u00a0=\u00a0\u22120.307, Wald\u2019s   z  \u00a0=\u00a0\u22122.588, 95% CI [\u22120.550,\u20130.082], p=0.010. \n   Percentage choosing Agent B as a function of agreement with Agents B and C.  \nA smoothed level plot illustrates that as agreement with both Agents B and C increases (towards the top-right corner), so does the probability of choosing Agent B on the ally-choice trial. If Agent B had been the only influence on whether or not participants chose Agent B on the ally-choice trial, then the transition from yellow to red should only occur in the horizontal direction (as agreement with\u00a0Agent B increases). Instead, there is a radial transition from the bottom-left corner. \n  \nIn other words, even when adjusting for Agent B\u2019s agreement with the participant, increasing Agent C\u2019s alignment with the participant made respondents more likely to choose Agent B on the ally-choice trial. However, this result was expectably qualified by a weak interaction: when Agent B agreed with the participant a majority of the time, the additional variance explained by Agent C's agreement decreased. While a dyadic similarity model would not predict that the level of agreement with Agent C should matter for choosing on the ally-choice trial, the latent structure learning\u00a0model does predict that Agent C\u2019s level of agreement with the participant should matter in whether or not participants choose Agent B on the ally-choice trial. Indeed, any difference in choice behavior as a result of Agent C\u2019s level of agreement is already inconsistent with the dyadic similarity account. Disambiguation between these two accounts of ally-choice have been demonstrated previously with model simulations and behavioral studies ( ;  ). \n\n\n### Computational models and neuroimaging data \n  \nWe developed three models to capture participants\u2019 trial-by-trial estimates of i) dyadic similarity with each agent, ii) similarity-over-agents, and iii)\u00a0social latent structure. \n\nWe calculated dyadic similarity   S   as a function of the number of previous agreement instances between the agent under consideration and the participant divided by the number of trials elapsed, initialized at 0.50 for each agent (see Materials\u00a0and\u00a0methods for model details). For example, if an agent agreed with the participant on the first political issue,   S   would be calculated as 0.66 for the second issue; if the agent did not agree with the participant on the first political issue,   S   would be calculated as 0.33 for the second issue. \n\nUnlike the latent structure learning model described below, the dyadic similarity model did not account for the feedback of other agents when calculating   S  . In other words, for the third trial, the dyadic similarity model would calculate a similarity for an agent who had agreed twice with the participant on the previous two trials as 0.75, regardless of how the other two agents had responded. On each new run,   S  \u00a0for\u00a0each agent was set to 0.50 given that participants had no information about those three new agents.   As such, this value reflected how likely each agent was to agree with the participant on a new issue given that agent\u2019s agreement with the participant on previous issues.  \n\nThe person-as-feature similarity model calculated   S   as the correlation between rows of a similarity matrix constructed from all possible dyadic similarities (i.e.,   S   between the participant and each agent as well as   S   between each pair of agents). In other words,   S   could be considered a second-order dyadic similarity in that it captures similarity over people rather than over choices.   As such, this value reflected how likely each agent was to agree with the participant given that the agent's and the participant\u2019s political preferences similarly resembled those of other agents.  \n\nIn contrast, the latent structure learning model ( ) assumes that participants infer latent group assignments (a partition of agents into groups) on the basis of the agents\u2019 choice data (see Materials\u00a0and\u00a0methods for model details). This model uses the Chinese restaurant process ( ) as a prior over group assignments, which effectively \u2018infers\u2019 the most probable number of clusters in the environment given the existing data, therefore bypassing the need to   a priori   set an expected number of clusters (e.g., one rarely walks into a room expecting there to be   n   number of groups). Through the observations of agents\u2019 choices, the posterior is inferred using Bayes\u2019 rule, and the likelihood, derived from analytically marginalizing the latent parameters under a Dirichlet-Multinomial model, will favor groupings where individuals in the same group exhibit similar choice patterns. Parametric modulator values for the latent structure learning model were calculated as the marginal posterior probabilities of relevant partitions (i.e., partitions wherein the participant and the respective agent were grouped together). To generate values, we did not fit any free parameters to participant behavior. Unlike our other two models, information about all three agents contributed on each trial to the prior for guessing about each particular agent. In other words, the prior for the second agent during the third trial took into account the feedback for the first agent in the third trial as well as the feedback for all three agents during the first and second trials.   As such, this value reflected how likely each agent was to belong to the same social group as the participant based on how all of\u00a0the agents and the participant related to one another on previous issues.  \n\nWe examined which voxels\u2019 signal correlated with the dyadic similarity, feature\u00a0similarity-over-agents, and latent structure learning\u00a0parametric modulator contrasts ( ; no other regions other than the ones reported here exceeded our corrected threshold.). As predicted, trial-by-trial dyadic similarity correlated with activity in the ventral medial prefrontal cortex/pregenual anterior cingulate (pgACC;  , green). The similarity-over-agents modulator identified clusters in the pgACC, bilateral temporoparietal junction, right superior temporal sulcus, and left supplementary motor area ( , yellow). Perhaps unsurprisingly, the pgACC cluster from this parametric modulator encompassed the pgACC cluster found by the dyadic similarity modulator. \n   Results from whole-brain contrast (FWE-corrected p<0.05) of parametric modulators.  \nDyadic similarity model (green), feature similarity model (yellow), and latent structure model (red). Note the overlap between the dyadic similarity and feature similarity models in the pgACC (e.g., at   x  \u00a0=\u00a010). \n \n   Overlap between latent structure model parametric modulator and a separately derived ROI.  \nOverlap (yellow) between our latent structure model result (red) and a separately derived ROI of cluster structure updating (blue;  ). \n  \n    Results from parametric modulator contrasts.      \nIn contrast, the latent structure\u00a0learning model parametric modulator identified only a cluster in right anterior insula (rAI) that extended into the inferior frontal gyrus (IFG pars orbitalis;  , red). This rAI cluster did not overlap with any of the clusters identified by the previous two models. Of note, this rAI cluster overlapped with a separately identified rAI cluster associated with non-social cluster assignment updating ( ). To provide a description of the similarity: our rAI overlapped with 44.7% of that rAI result (i.e., number of common voxels across both ROIs divided by total number of rAI voxels in  ;  ). Activity in this independently defined ROI correlated significantly with our latent group model, cluster-level FDR-corrected   q  \u00a0=\u00a00.014. \n\n\n### Testing the specificity of the rAI result \n  \nWe conducted a k-fold cross validation (leave one out procedure) and tested whether the latent structure learning\u00a0model (compared to the other models) explained more variance in the rAI. To do this, we iteratively generated rAI and pgACC clusters by conducting second-level analyses using all participants but one and tested the fit of the models in each cluster on the left-out participant. For each iteration, we computed a Bayesian information criterion for each model as that particular model score and multiplied it by \u22120.5 to convert it into log model evidence. We used this calculated log model evidence (one for each model for each fold) for Bayesian model selection and calculated protected exceedance probabilities (PXP) and Bayesian omnibus risk (BOR;  ). A PXP reflects the probability that a\u00a0particular model is more frequent in the population compared to\u00a0the other models considered (beyond what would be expected by chance), while BORs reflect the probability that all model frequencies are equal to one another. To put these results into context, a previous ROC analysis found the disambiguation threshold, or the point at which we can best discriminate between H  (that both models are equally represented) and H  (that one model is represented more\u00a0so than another), to exist somewhere around 50% for PXPs and around 0.25 for BORs ( ). The PXP in the rAI was 82.34% for the latent structure model, but only 6.44% for the dyadic similarity model and 11.23% for the similarity-over-agents model. The BOR was 0.190. In sum, the latent structure model explained significant variance in the rAI. \n\nOn the other hand, when\u00a0using the same method to\u00a0test the specificity of the dyadic similarity model in the pgACC, we found that the PXPs\u00a0were 51.58% for the latent structure model, 23.31% for the dyadic similarity model, and 25.11% for the similarity-over-agents model. The BOR was 0.675. In other words, for the pgACC, no single model was especially frequent over the other two. \n\n\n### Predicting ally-choice behavior from neural activity \n  \nWe also tested whether variability in the brain signal from our resulting ROIs would help predict variability in participants\u2019 behavior during the ally-choice trial. In other words, we asked whether the neural \u2018noise\u2019 from our clusters improved prediction of participant choice above and beyond mere model predictions. \n\nWe first decoded the neural signal in each ROI\u2014pgACC and rAI\u2014corresponding to the parametric modulator by removing the variance corresponding to other regressors in our model (see Materials\u00a0and\u00a0methods). We then isolated the signal corresponding to the temporal onsets of the photos of Agents A and B, which appeared right before each ally-choice trial. For each agent, we averaged the signal of interest across voxels within each ROI, respectively (i.e., we\u00a0calculated the signals corresponding to Agent A and Agent B for the pgACC and the signals corresponding to Agent A and Agent B for the rAI). For each ROI, we then calculated the log difference between the signals corresponding to each agent. \n\nWe tested whether this signal would improve the fit of a logistic regression that modeled ally-choice behavior against model predictions from each of our models. Model predictions were calculated as the log difference between either the similarity with A and the similarity with B (e.g.,\u00a0in the case of\u00a0the dyadic similarity model) at the end of the eight learning trials or the probability of the participant being grouped with A and the probability of the participant being grouped with B (in the case of the latent structure model) at the end of the eight learning trials. Log difference of the signals was orthogonalized with respect to the log difference of corresponding model predictions. Note that our parametric modulator ROIs were identified by fitting the signal during the learning phase of each run, whereas in the behavior prediction analyses here, we used signal from the ally-choice phase. As such, there is no circularity in this analysis. \n\nWhile a likelihood ratio test showed that adding the signal from the rAI cluster helped to better predict variability in choice behavior for the latent structure model (\u03c7 (1)\u00a0=\u00a05.312, p=0.021), neither the addition of the signal from the pgACC to the dyadic similarity model (\u03c7 (1)\u00a0=\u00a01.526, p=0.217), nor the addition of signal from any of the clusters associated with the similarity-over-agents model helped improve predictions of choice variability (\u03c7 (1)s\u00a0<\u00a02.112,   p  s\u00a0>0.250). \n\n\n\n## Discussion \n  \nHere, we used a model-based analysis to compare different accounts by which people may differentiate \u2018us\u2019 from \u2018them\u2019 and found evidence for separable neural areas tracking each concurrently. While social alignment estimates based on dyadic similarity and feature\u00a0similarity-over-agents recruited the pgACC, allyship estimates via latent structure learning recruited the rAI. Additionally, signal variability in the rAI cluster, but not any other cluster identified by the two other models, during the ally-choice trials helped predict ally-choice above and beyond model predictions. Furthermore, a cross-validation demonstrated that the variability explained in the rAI by our latent structure learning model was much higher than the competing models. \n\nSeveral aspects of these results merit further discussion. First, this is the only evidence of which we are aware that pgACC supports incremental revisions of representations of similarity between oneself and others, both directly and across third agents. In contrast to research that relies on preexisting knowledge about specific groups or individuals, using novel agents allowed us to examine how participants\u2019 degree of alignment changed as they learned agents\u2019 preferences over time. \n\nSecond, our rAI result is consistent with previous work on updating of non-social latent structure ( ). Note, though, that social categorization is distinct from other forms of categorization because it requires participants to categorize themselves ( ). Thus, our results demonstrate that rAI is capable of learning egocentrically defined latent structures. \n\nAnterior insula is topographically well-situated to relay social information related to coalition structure. Connectivity of the anterior insula with the anterior cingulate cortex, amygdala, and ventral tegmental area (a \u2018salience detection\u2019 network;  ) and the dorsolateral prefrontal cortex (associated with cognitive control;  ) likely affords the flexibility required to represent context-specific coalition members\u2014someone who may be a coalition member at a debate may not be a fellow coalition member at a sports event. AI is also heavily involved in socially-relevant computations, including but not limited to self-awareness tasks such as awareness of emotions and subjective pain, and exhibits\u00a0hypoactivity in persons with autism spectrum disorder on tasks such as face processing and theory of mind ( ). \n\nThe rAI region we identified included a part of the IFG (specifically, pars orbitalis). In studies of hierarchical processing in music and language, this area has been associated with sentence comprehension ( ) and found to exhibit sensitivity to violations of hierarchical regularities ( ). This area is also involved in building up sentence structure and meaning as semantic information is processed ( ). Additionally, the IFG has been hypothesized to represent individual episodes for later recombination ( ). Just as this area may be recruited to build up the structures of sentences and tonal patterns, it may also be building up inferences\u00a0of\u00a0social latent structures as participants learn more about other agents\u2019 preferences. \n\nFinally, it is worth noting that while the brain tracks both of these alignment signals, variability in the signal from only one of the regions, rAI, helped improve model predictions of behavioral choices. It is possible that had participants been asked to make a different choice (e.g., identify which agent better represented a particular trait), the pgACC signal may have been more relevant. Nonetheless, this result underscores the need to further understand how social latent structures and coalitions feature in shaping people\u2019s social choices. \n\nAccurately distinguishing \u2018us\u2019 from \u2018them\u2019 is crucial to navigating our social lives. To do so, we could rely on computing dyadic similarity with each individual agent or across agents; however, a more sophisticated approach would be to incorporate information about how agents relate to one another in order to infer latent groups in the environment through Bayesian inference. Our approach moves beyond explicit category labels and mere similarity as the sole inputs to social group representations, appealing instead to a domain-general latent\u00a0structure learning mechanism, which we demonstrate predicts ally-choice. Furthermore, we provide evidence for separable neural areas tracking each of these processes; not only do we demonstrate that the rAI tracks estimates of any agent being a fellow coalition member, but we also show that the pgACC can track the fluctuations in similarity between oneself and agents (both with individual agents and over agents) in the environment. These findings advance our understanding of the complex processes underlying social group inference and ally selection in humans and potentially other species. \n\n\n## Materials and methods \n  \n### Participant selection \n  \nWe first recruited participants (N\u00a0=\u00a0333) under the pretense of playing a game in lab in which they would tell us about their political issue preferences and learn about others\u2019 preferences. All participants first completed this behavioral version of the task to familiarize themselves with the task and the political\u00a0issues prior to scanning. Participants completed all six runs of the task as described in the scanner procedure in the following section. The main differences between the behavioral version and the scanner version were that (i) the behavioral version allowed participants to spend as much time as needed to read the prompt before proceeding, while the scanner version limited the reading time to 6 s, (ii) the behavioral version did not allow for participants to acquaint themselves with the political\u00a0issues before beginning the main task, and (iii) the response buttons differed (i.e., the behavioral version involved pushing \u2018E\u2019 and \u2018I\u2019 on a keyboard rather than \u20181\u2019 and \u20182\u2019 on a button box). We could then ensure that we were only recruiting participants who could successfully make responses within the time limit. Analysis of behavioral data from this phase can be found below. \n\nUpon completion of the behavioral task, participants were asked if they were interested in completing a similar fMRI study for pay and asked to confirm or disconfirm a series of statements relating\u00a0to qualifications for participating in an fMRI study (e.g., whether they had metal in their body, being able to lie still for over an hour, etc.). Interested participants who reported no contra-indicators were invited to participate in the scanner task. \n\n\n### Behavioral data analyses for sample N\u00a0=\u00a0333 \n  \nIn the task for the participant selection phase, we fixed Agent A\u2019s and Agent B\u2019s agreement with the participant such that each agent agreed with the participant on only four of the eight trials. Additionally, Agent C always agreed with Agent B and Agent A on five trials and three trials, respectively. To create our conditions, we varied Agent C\u2019s agreement with the participant such that Agent C agreed with the participant on either seven trials (high-C) or only one trial (low-C). Given that participants had to respond within 6 s, some participants missed ally-choice trials, and only trials where data was recorded were analyzed (high-C: 906 trials, low-C: 918 trials). We used a logistic regression to model the probability of choosing Agent B\u2019s choice in the ally-choice trial as a function of condition (high-C or low-C). Including random slopes to account for subject-level effects resulted in a singular fit of the model (i.e., overfitting), so we removed them. We did not find a significant difference between our two conditions in predicting\u00a0the probability for choosing Agent B,   b  \u00a0=\u00a00.023, Wald\u2019s   z  \u00a0=\u00a00.246, 95% CI = [\u22120.161, 0.207], p>0.250. \n\nNonetheless, we have stated before this is a small behavioral effect when previously demonstrating it across another set of experiments ( ). Our sample size here may have been too small to detect a difference. Additionally, because participants were inexperienced and had only 6 s to respond (whereas our previous participants had an unlimited time to respond), they may have responded differently. When we limit the analysis to only the final, sixth ally-choice\u00a0trial, we see a small effect of condition on the probability of choosing Agent B,   b  \u00a0=\u00a00.454, Wald\u2019s   z  \u00a0=\u00a02.00, 95% CI = [0.010, 0.901], p=0.046. \n\nFinally, our two conditions, which drastically varied Agent C\u2019s level of agreement (i.e., Agent C either mostly agreed or mostly disagreed with the participant), may have been too obvious for a version of the task that was not self-paced. One participant reported as much in the debriefing. In the actual scanner version of the study, we varied the degree to which Agents A, B, and C agreed with the participant while maintaining the same agreement relationships between the agents as found in the behavioral portion to avoid this limitation. \n\n\n### fMRI participants and exclusions \n  \nFrom our 333 participants, we recruited 61 right-handed participants (48 female,   M  \u00a0=\u00a021.74 years,   SD  \u00a0=\u00a04.17) in order to achieve a sample size of at least 40 participants after exclusions. This sample size was determined based on sample sizes used in other fMRI experiments (e.g.,  ;  ). Two participants requested to be removed from the scanner prior to the end of the study, and two participants were excluded due to a computer crash, resulting in unrecorded responses. Prior to any data analysis, we excluded five participants who fell asleep in the scanner, eight participants for excessive head movements of 4 mm or more, one participant who correctly deduced the hypothesis of the study, and one participant for missing at least one self-response per run. This left us with a sample size of 42 participants (32 female,   M  \u00a0=\u00a022.07 years,   SD  \u00a0=\u00a04.82). Participants provided informed consent to participate and consent to publish; all procedures complied with Harvard University\u2019s Committee on the Use of Human Subjects\u2019 guidelines (Protocol #IRB15-2048). \n\n\n### Materials \n  \nTo develop stimuli, we used  , a website that helps people determine the political party and/or candidate with which their positions best align based on yes and no responses to nationally relevant, political issues (e.g., 'Do you support the death penalty?'). The website also aggregates survey responses and makes this data publicly available ( ). We selected issues that had accumulated at least 500,000 votes and had the greatest agreement/disagreement discrepancies, as described in Experiment 2 in  . We included the 48 issues with the lowest yes-no differences as of May 2017 in the main task (see OSF for complete materials). \n\nOn each trial, we displayed the issue as text at the top of the screen. Underneath, we signified a \u2018yes\u2019 or \u2018no\u2019 response to the issue by superimposing a green check mark or a red \u2018X,\u2019 respectively, atop an image representing the issue. To avoid confusion, we also displayed the words, \u2018YES\u2019 and \u2018NO\u2019, underneath the corresponding images. The order of presentation of the 48 issues as well as the sides on which the agreement positions appeared on the screen were randomized for each participant. \n\n\n### Face selection \n  \nFor agent pictures, we selected a total of 36 photos from the Chicago Face Database (CFD;  ) and gender-matched agents to the participant. We extracted the pool of \u2018White\u2019 faces (based on CFD designations) and eliminated faces based on the norming data provided by the CFD until 18 female and 18 male faces were left. \n\nGiven that the pool of faces varies for male and female faces, face selection processes varied slightly. For female faces, at least half of the respondents had to rate the face as looking \u2018white\u2019, and then we eliminated any face that respondents rated as unusual compared to other white females (i.e., above the midpoint of a 7-point Likert scale ranging from 1, not at all unusual, to 7, very unusual). Using the ratings of how prototypical the faces looked compared to other white females (5-point Likert scale ranging from 1, not at all prototypical, to 5, very prototypical), we then removed the faces scoring less than one standard deviation from the average of this pool to remove non-prototypical faces. We also then removed the faces that scored one standard deviation below the mean in terms of femininity (1, not at all feminine, to 7, extremely feminine). To norm for attractiveness, we eliminated anyone who was rated as one standard deviation above and below the mean of attractiveness ratings, leaving us with 38 faces. Any face\u00a0two standard deviations above or below the mean age was then removed, leaving us with 37 faces, and the youngest face was then removed to generate a set of 36 faces with an average age of 27.01 years (  SD  \u00a0=\u00a03.81). \n\nFor male faces, we followed the same initial five steps, except we eliminated any face that scored one standard deviation above the mean in femininity ratings in order to retain the more masculine-looking faces. Of the remaining 43 faces, we then eliminated anyone who was one and a half standard deviations above or below the mean age, which left us with 38 faces. For the final step, we removed the two youngest faces in the set of faces to generate a set of 36 male faces with an average age of 28.58 years (  SD  \u00a0=\u00a05.23). \n\n\n### Pre-task instructions \n  \nAfter being consented, participants first completed a round of instructions guiding them through a trial. They expressed their own opinion on a topic (\u2018Should cartoons include plotlines involving duck-hunting?\u2019) by selecting \u2018Yes\u2019 or \u2018No\u2019 and then guessed and received feedback on the opinions of Bugs and Daffy. Participants were then\u00a0guided through an ally-choice trial. We told participants that for these trials, gray boxes with question marks on them would represent two different positions on a political issue. The only information participants had about the boxes was the choices of other agents\u2014the same ones whose preferences they had just learned. We told participants to select the box they would prefer based on the other agents\u2019 choices. Participants were then introduced to the timing of the task (see below) and completed four practice trials (\u2018Should cartoon characters conquer Mars?', \u2018Should cartoon characters be allowed to pilot planes?', \u2018Should cartoon plotlines feature day jobs?', \u2018Should cartoon characters be allowed to do yoga?') with these timings in place with Bugs and Daffy again. An ally-choice trial followed these four practice trials in which participants were asked to choose between Bugs' and Daffy\u2019s mystery positions. After this, the computer displayed how many responses they successfully made within the time limits, and participants were prompted to notify the experimenter. The experimenter checked that all participants made at least 11 of the 13 responses within the time limits and probed for any remaining questions about the task. Given that participants had limited time to think about the political\u00a0issue at hand and because their responses were important for generating\u00a0the feedback they received from the other agents, we then gave participants a list of all of the political\u00a0issues they would be seeing in the scanner prior to being scanned and asked\u00a0them to review all the issues. \n\n\n### Task \n  \nEach run consisted of two phases: (i) eight learning trials during which participants expressed their own preferences on a political issue and learned about three others\u2019 preferences; (ii) an ally-choice trial. Each learning trial began with the participant seeing a political\u00a0issue for 6 s. They then had 4.5 s to choose whether they would support that issue; a gray rectangle appeared around their selection when they had made their choice. A gray arrow pointing to their choice appeared for 3 s to confirm their choice. Each participant then learned about the preferences of other three individuals (Agents A, B, and C) via feedback. \n\nTo avoid participants weighting information from agents who looked more similar to them, we gender-matched the agents to participants\u2019 self-reported gender and only used White faces. Participants first saw a picture of one of the agents alongside his/her name and were asked to predict the preference of that agent with regards to the issue (e.g., \u2018Which do you think Annie chose?'); each participant had 4.5 s to guess, and a gray rectangle appeared around their selection after they made a choice. They then learned of the agent\u2019s choice when a gray arrow pointing to one of the preferences (\u2018Yes\u2019 or \u2018No\u2019) appeared for 3 s ( ). Participants then repeated this guess and feedback process for the other two individuals. Between each screen, a fixation cross appeared for 3 s-16\u00a0s (jittered). Jitter was generated using the optseq algorithm ( ). Once this process (political\u00a0issue prompt, self-choice, confirmation of self-choice, guess and feedback for A, guess and feedback for B, and guess and feedback for C) was completed for one issue, participants started a new regular trial by repeating the process for a new issue with the same three Agents. A table consisting of eight rows and four columns displayed on the right-hand side of the screen recorded the participant\u2019s and each Agent\u2019s responses on each trial. The order of the policy positions and agents was randomized; every participant saw each of the 48 policies and 18 agents only once during the experiment. The order of the agents was also randomized across runs. Agents were randomly assigned names from a preset list of names of five-letter length. \n\nFollowing the eight learning trials, participants saw one \u2018mystery\u2019 ally-choice trial. On these trials, participants saw pictures of Agents A and B sequentially presented\u00a0in a random order, each of which was followed by a fixation cross that appeared for 3 s-16\u00a0s (jittered), prior to reaching the decision screen. On the decision screen, participants saw two boxes with question marks representing two unknown positions on a political issue. Underneath the two boxes were photos of Agents A and B and gray arrows pointing to their respective choices ( ). We told participants that the mystery boxes contained Agent\u00a0A\u2019s and Agent\u00a0B\u2019s preferred positions. Participants had to indicate which one of the two unknown positions they would rather choose (e.g., 'Which would you choose? Remember, Annie and Betsy know what\u2019s inside the boxes.'). A gray rectangle appeared around the selection after they indicated their choice. Thus, participants had to align themselves with one of the two agents. The response table summarizing participants\u2019 and all agents\u2019 preferences during the block was visible during the ally-choice trial. After the ally-choice trial, participants started a new run with a new set of three agents and a new set of eight policy positions. \n\nParticipants completed six runs in total. For each run, Agents B and C agreed on five issues; Agents A and C agreed on three issues, and Agents A and B never agreed with each other. This agreement structure made it more likely that participants would cluster Agents B and C together and that Agent C\u2019s agreement with the participant would increase the selection of Agent B on the ally-choice trial. However, agreement counts between the participant and each agent varied across blocks and participants. All agent positions were entirely randomly assigned (within the constraints of the agreement/disagreement structure). Thus, across all runs and participants, some agents expressed a mixture of highly partisan left and right beliefs. In our previous behavioral studies ( ), Experiment 2), we found that participants still exhibited the behavior predicted by the latent structure learning model despite learning about agents with less ideologically coherent profiles (i.e., agents with preference profiles constructed from a mix of left and right partisan topics). \n\n\n### fMRI data acquisition and analyses \n  \nWe collected data using a 32-channel head coil in a 3.0-tesla Prisma MRI scanner (Siemens) located at the University. At the beginning of each scan session, we acquired a high-resolution T-1 weighted anatomical image (T1-MPRAGE, 1\u00a0\u00d7\u00a01\u00a0\u00d7\u00a01 mm, parallel to the anterior commissure-posterior commissure plane) for use in registering activity to each participant\u2019s anatomy and spatially normalizing data across participants. Functional images were then acquired through six echo-planar imaging (EPI) sessions each lasting 12 min. For whole brain coverage, we acquired 69 interleaved 2.0 mm slices (repetition time\u00a0=\u00a01.5 s; echo time\u00a0=\u00a030 ms; flip angle\u00a0=\u00a075 degrees; field of view\u00a0=\u00a0208 mm; matrix\u00a0=\u00a0104\u00a0\u00d7\u00a0104; in-plane acceleration (GRAPPA)\u00a0=\u00a02; multi-band acceleration factor\u00a0=\u00a03). The multi-band EPI sequence was provided by the University of Minnesota Center for Magnetic Resonance Research ( ;  ;  ;  ). \n\nWe conducted preprocessing and statistical analyses using SPM12 (Wellcome Trust Centre for Neuroimaging, London, UK,  ). We realigned functional images to the first volume, unwarped the functional images, segmented the structural image into its respective tissue types, and normalized the gray matter of the structural to the gray matter of a standard Montreal Neurological Institute (MNI) reference brain. The mean functional images were co-registered to the structural image, and functional images were normalized to the MNI template, resliced to 2 mm\u00a0\u00d7\u00a02 mm\u00a0\u00d7\u00a02 mm voxels, and smoothed using an 8 mm FWHM Gaussian kernel. \n\n\n### fMRI analyses \n  \nWe modeled data with an event-related design using a general linear model. For each of the six runs, one regressor modeling the onset of self-choice (eight onsets), a second regressor modeling the onset of guesses for each of the three agents across the eight issues (24 onsets) and a parametric modulator for the second regressor, were convolved with the canonical hemodynamic response function. The parametric modulator values indexed the specific model output (the dyadic similarity, feature similarity-over-agents, or the latent structure learning model; see Computational models below) of either the similarity between the participant and the target or the prior probability of the participant belonging to the same group as the target of the guess (i.e., Agents A, B, or C). For example, when the participant was making a guess for Agent\u00a0A on the seventh trial in a block, the latent structure parametric modulator took on the value of the probability that Agent A was in the same group as the participant given previous feedback. No clusters survived correction when all three models were included as parametric modulators. \n\nWe did not orthogonalize parametric modulators with respect to the second regressor (the onsets of guesses) given that we were interested in which voxels tracked our parametric modulator values rather than the mean-centered values ( ). In addition, we included six nuisance regressors containing the temporal and spatial derivatives for the main regressor and six run regressors. Alignment values from the latent structure learning model and the dyadic similarity model were modeled in separate models given that each represented a different hypothesis about processes in the brain. To determine which areas of the brain tracked each these models, we then entered the images resulting from contrasting the parametric modulator against baseline into a second-level analysis treating participants as a random effect. We used a voxelwise threshold of p<0.001 and corrected for multiple comparisons using whole-brain cluster-wise family-wise error (FWE) correction from bspmview ( ) at the \u03b1\u00a0=\u00a00.05 level. \n\n\n### Parametric modulator correlations \n  \nGiven that all three model outputs were derived from similar inputs, the correlations among the different parametric modulators were moderate to large. The average correlation between dyadic similarity and latent structure modulators was 0.8627, with values ranging from 0.5592 to 0.9704. The average correlation between the latent structure and feature\u00a0similarity-over-agents parametric modulators was 0.7889 (ranging from 0.3778 to 0.9496). Finally, the average correlation between the dyadic similarity and feature\u00a0similarity-over-agents parametric modulators was 0.9614 (ranging from 0.9023 to 0.9874). \n\nTo measure collinearity between the dyadic similarity,\u00a0feature\u00a0similarity-over-agents and latent structure modulators, we calculated the VIF (1/(1-R ), where R  is the r-squared from regressing one parametric modulator on the other). The VIF between dyadic similarity and latent structure modulators was 5.063 (generally regarded as low collinearity), while the VIF between dyadic similarity and feature similarity-over-agents parametric modulators was 13.786. \n\n\n### Computational models \n  \nDyadic similarity,   S ,   is calculated as a function of the number of previous agreement instances between the agent and the\u00a0participant divided by the number of trials elapsed, where priors for the first trial are 0.50 for each agent: \n\nFeature\u00a0similarity-over-agents,   S  , uses output from the dyadic similarity model to construct a similarity matrix whose values are   S   between the participant and each agent as well as   S   between agents. Feature similarity between a participant and a particular agent is\u00a0computed as the correlation between the row of the similarity matrix representing the dyadic similarity between the participant and each agent and the row of the similarity matrix representing the dyadic similarity between that particular agent and everyone else (i.e., the participant and the other two agents;  ). To transform these correlations to interpretable probabilities, output values were rescaled to a 0 to 1 range, and a log odds transformation (i.e., log(  S  ) \u2013 log(1-   S  )) was applied. \n\nThe latent structure learning model assumes that participants infer latent group assignments (a partition of agents) based on agents\u2019 choice data. The prior distribution over group assignments is a Chinese restaurant process ( ), where the probability of partition z = [z , \u2026, z ] given M individuals is our prior: where \u03b1\u00a0\u2265\u00a00 serves as the dispersion parameter (as \u03b1 approaches infinity, each individual is assigned to a unique group), T  is the number of individuals assigned to group   k   and \u0393(\u2219) is the gamma function. In our modeling, we used \u03b1\u00a0=\u00a02, though the results are relatively robust to variation in this parameter. An infinite number of groups can be generated, but a \u2018rich get richer\u2019 dynamic favoring more popular clusters will produce more parsimonious groupings (see  ). We can derive the posterior using Bayes\u2019 rule with observed choices   C   = [c   ,\u00a0\u2026,c   ]: \n\nThe likelihood is obtained by analytically marginalizing the latent parameters under a Dirichlet-Multinomial model: \n\nWhere \u03b8 is a set of multinomial parameters,   is the number of options on problem   n   and   is the number of individuals assigned to group   k   who choose stance   c   on issue   n  . The likelihood favors group\u00a0assignments for which choice patterns are similar between individuals assigned to the same group. \n\nParametric modulator values use the probability that the agent under consideration is in the same group as the participant and are derived as the marginal posterior probability of the relevant partitions: \n\n\n### Neural signal decoding \n  \nThe neural signal of interest can be algebraically derived from the standard GLM with L2-norm regularization: where   Y   is the overall signal from the voxel and   X   is the corresponding vector from the original design matrix. For these analyses, we set our regularization parameter, \u03bb, to a value of 1 following  . \n\n\n \n", "metadata": {"pmcid": 7136019, "text_md5": "3cd114d94ea98bdf17a670041142a0bc", "field_positions": {"authors": [0, 52], "journal": [53, 58], "publication_year": [60, 64], "title": [75, 125], "keywords": [139, 243], "abstract": [256, 3724], "body": [3733, 51746]}, "batch": 1, "pmid": 32067635, "doi": "10.7554/eLife.53162", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7136019", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=7136019"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7136019\">7136019</a>", "list_title": "PMC7136019  Social structure learning in human anterior insula"}
