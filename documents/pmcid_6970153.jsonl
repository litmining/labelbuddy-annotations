{"text": "Ogawa, Akitoshi and Kameda, Tatsuya\nSoc Cogn Affect Neurosci, 2019\n\n# Title\n\nDissociable roles of left and right temporoparietal junction in strategic competitive interaction\n\n# Keywords\n\nfMRI\ntemporoparietal junction\ncomputational model\ntheory of mind\nmachine learning\n\n\n# Abstract\n \nAlthough many studies have shown that the temporoparietal junction (TPJ) is involved in inferring others\u2019 beliefs, neural correlates of \u2018second-order\u2019 inferences (inferring another\u2019s inference about one\u2019s own belief) are still elusive. Here we report a functional magnetic resonance imaging experiment to examine the involvement of TPJ for second-order inferences. Participants played an economic game with three types of opponents: a human opponent outside the scanner, an artificial agent that followed a fixed probabilistic strategy according to a game-theoretic solution (FIX) and an artificial agent that adjusted its choices through a machine-learning algorithm (LRN). Participants\u2019 choice behaviors against the human opponent and LRN were similar but remarkably different from those against FIX. The activation of the left TPJ (LTPJ) was correlated with choice behavior against the human opponent and LRN but not against FIX. The overall activity pattern of the LTPJ for the human opponent was also similar to that for LRN but not for FIX. In contrast, the right TPJ (RTPJ) showed higher activation for the human opponent than FIX and LRN. These results suggest that, while the RTPJ is associated with the perception of human agency, the LTPJ is involved in second-order inferences in strategic decision making. \n \n\n# Body\n \n## Introduction \n  \nCompetitive situations in which one\u2019s own benefit means the opponent\u2019s loss (and vice versa) are common in many facets of our social lives, including resource allocation, rivalry for social status, games such as football or chess, public debate and so on. When such competitive situations are repeated, we must not only learn and predict the opponent\u2019s behavior but also simultaneously be aware that the opponent also learns and predicts our behavior. This bilateral (\u2018higher-order\u2019) inference about the opponent\u2019s state of mind is a major ingredient in human strategic interactions. \n\nA rational solution to such a competitive game is a mixed strategy that assigns a choice probability to each of the options according to the Nash equilibrium ( ). However, it is known that humans often deviate from the equilibrium ( ; see also  ). Rather than adopting the theoretically derived, fixed Nash strategy, people often seem motivated to explore the choice probabilities of the opponent for possible exploitation ( ;  ;  ;  ). However, such attempts at exploitation must take into account how our own choices may influence the opponent\u2019s strategy ( ;  ) and also reason about how the opponent may infer our current strategies ( ). In contrast, such bilateral, higher-order processes may not be invoked when people play against a computer opponent operating according to some fixed strategy. In other words, human choice behavior in a competitive situation can be expected to depend heavily on the nature of the opponent and how the opponent makes decisions. \n\nMany neuroimaging studies have shown that the temporoparietal junction (TPJ) is associated with \u2018theory of mind\u2019\u2014the ability to infer the agency and mind state of another ( ;  ;  ;  ;  ;  ;  ;  ;  ). The right TPJ (RTPJ) has been suggested to be involved in inferring others\u2019 mental states in various social situations (e.g. moral judgment,  ; distribution,  ; risky decision making,  ). The left TPJ (LTPJ) is also important for representing and inferring another\u2019s beliefs ( ;  ). Human imaging studies have suggested that the LTPJ is associated with attending to the gap in perspective between self and other ( ;  ;  ). Although the functional roles of TPJ in inferring others\u2019 beliefs have been progressively revealed, details remain elusive concerning the TPJ\u2019s involvement in the higher-order recognition that others also infer one\u2019s own beliefs during strategic decision making. \n\nIn this functional magnetic resonance imaging (fMRI) study, we investigated how the bilateral TPJs associated with ToM were involved in competitive decision making, in which correct inference of another\u2019s inference about one\u2019s own belief is essential. Participants played an economic game (the \u2018asymmetric matching pennies game\u2019:  ) in the MRI scanner against three types of opponents: a human opponent who played the game from outside the scanner and two artificial agents (FIX and LRN) whose choices followed computer algorithms (which was explicitly noted in participant instructions). Thus, neither FIX nor LRN involved human agency, and their choice algorithms were quite different from each other. FIX followed a fixed probabilistic mixed strategy according to a game-theoretic Nash equilibrium, without any inference of the participant\u2019s strategy. On the other hand, LRN was programmed to predict the participant\u2019s choice strategy and adjust its own choices using a machine-learning technique. As both the human opponent and LRN were expected to be responsive to the participant\u2019s choices in a bilateral manner, we expected that the participant\u2019s choice behaviors against the human opponent and against LRN would be similar to each other, but different from those used against FIX. Thus, we investigated how the neural activities of RTPJ and LTPJ were associated with both the perception of human agency and the higher-order recognition of the other agent\u2019s inference about one\u2019s own strategy in the competitive game. \n\n\n## Methods \n  \n### Subjects \n  \nWe scanned 30 right-handed student participants (14 females and 16 males; aged 18 to 22\u00a0years, mean\u2009=\u200919.3\u00a0years) at the University of Tokyo with no history of neurological or psychiatric illness. Thirty gender-matched, right-handed students (aged 18 to 22\u00a0years, mean\u2009=\u200919.4\u00a0years) also participated in this experiment as human opponents who played the game outside the scanner. The study was approved by the ethical committee of the Department of Social Psychology in the University of Tokyo. All participants gave written informed consent prior to the experiment. \n\n\n### Task \n  \nThe participants played the \u2018asymmetric matching pennies game\u2019 in the MRI scanner with three types of opponents: the human opponent (HUM) who played the game outside the scanner and the two artificial agents (FIX and LRN). FIX\u2019s choices were determined stochastically according to the mixed Nash strategy, while LRN used a machine-learning algorithm to attempt to predict and exploit the participant\u2019s choices. In the instructions, the human opponent was described as a student of the same sex at the same university, FIX as a computer program that would always follow a fixed, economically rational strategy and LRN as a computer program that would constantly learn to predict the participant\u2019s choices through interaction. \n\nAt the beginning of each trial, a cue indicating the type of opponent (HUM, FIX or LRN) was presented for 0.5\u00a0s ( ). Then two choice options were presented on the left and right sides of the display. The participant was asked to choose either option within 2\u00a0s. Immediately after either button was pressed, the frame of the chosen option was colored red. After a jittered fixation duration (2, 4 or 6\u00a0s), the choice of the opponent was indicated by a green frame, along with the outcome amount for the participant displayed at the center. After a jittered inter-trial interval (ITI) (2, 4 or 6\u00a0s), the next trial began. \n  \nTask sequence (A) and payoff matrix (B). A. In each trial, a conditional cue was presented for 0.5\u00a0s. Two options were presented on the left and right sides of the display for 2\u00a0s. When the participant pressed a button, the frame of the chosen option was colored red. After a jittered fixation duration, the choice of the opponent was shown as a green frame along with the monetary outcome the participant obtained. ITI\u2009=\u2009inter-trial interval. B. The participant won the game when his/her choice matched with the opponent\u2019s choice. The participant received 60 JPY by winning with the left choice (\u2018star\u2019) and 20 JPY by winning with the right choice (\u2018diamond\u2019) but received nothing for losing the game. The combination of left/right, star/diamond and outcome amounts was counterbalanced across participants. \n  \nThe participant won the game and received a monetary reward when their choice matched the opponent\u2019s choice. As shown in  , the participant received 60 JPY if both participant and opponent selected the left choice (\u2018star\u2019 in this example) and 20 JPY if both selected the right choice (\u2018diamond\u2019), while the opponent received nothing in either case. In contrast, when the two selections did not match, the participant received nothing, while the opponent received 40 JPY for \u2018star\u2019 or 20 JPY for \u2018diamond\u2019. Thus, the game payoff was asymmetric between the two players. We adopted this asymmetric payoff to make the game-theoretic probabilistic mixed strategy different from a random 50\u201350 choice; if the payoffs were symmetrical, it would be impossible to distinguish the game-theoretically rational strategy from a purely random choice ( ;  ). The combination of left/right sides, star/diamond shapes and outcome amounts was counterbalanced across participants. The participant\u2019s understanding of the payoff matrix was confirmed by a series of quizzes prior to the experiment. \n\nThe choice behavior in this study is characterized by how the participants balance between choosing the option (\u2018star\u2019 in the example) that could yield the higher outcome (high choice, hereafter) and the option (\u2018diamond\u2019) with the lower outcome (low choice). As mentioned earlier, we expected that participants\u2019 high choice rates in the HUM condition would be more similar to those in the LRN condition than in the FIX condition. \n\nEach of four fMRI runs included 3 blocks, with 12 consecutive trials in each block. During each block, the participant played the game against the same opponent. The order of the opponent blocks was counterbalanced across participants. The participants performed 144 trials in total (i.e. 48 trials for each of the three opponents). The participants received the accumulated monetary outcomes in the experiment as a bonus in addition to a fixed compensation of 3000 JPY (approximately 30 USD) for a participant of fMRI or 1000 JPY (approximately 10 USD) for a participant of human opponent. \n\nStimuli were presented on an MRI-compatible 32\u00a0inch LCD display with a resolution of 1920 pixels by 1080 pixels (NordicNeuroLab, Norway) placed at back of the MR bore. An MRI-compatible response pad (Current Designs, USA) was used to record responses. Psychtoolbox ( ) running on MATLAB (Mathworks Inc.) was used to present stimuli and record the responses. \n\n\n### Details of the opponents \n  \nThe participants played the game against three types of opponents. One was a human opponent (HUM) who was also a student of the University of Tokyo, with the same gender as the participant. The human opponent played the game outside the scanner. The computer opponent FIX made choices according to the probability dictated by the mixed strategy of the Nash equilibrium. In the example payoff matrix shown in  , FIX chooses the star with the probability of one-fourth and the diamond with the probability of three-fourths. The corresponding mixed strategy for the participant against FIX would be a probabilistic choice of one-third for the star and two-thirds for the diamond. The choice probability of FIX was fixed at the equilibrium rate regardless of the participant\u2019s choices. The other computer opponent LRN learned and predicted the participant\u2019s choices using a machine-learning algorithm (specifically, a perceptron with a sigmoid activation function:  ). LRN considered the choices of the participant (high/low), its own choices (high/low) and the results (win/loss) of the most recent six trials, plus the average bias of the participant toward the high choice (which was represented as a single constant term in the learning algorithm). Thus, the perceptron received 19 inputs (=\u20093\u2009\u00d7\u20096\u2009+\u20091) in each trial (represented by    x    below; see   for details about each variable). The following equation was used to determine the probability of high choice for LRN: \n\nEach of the 19 elements,   w  , in the weight vector    w    that corresponds to each of the 19 inputs ( ) was updated every trial using the following equation: where \n\nThe learning rate,   a  , was set to 0.25 throughout the experiment, which was determined by pilot tests. The choice data in the practice session was used to determine the initial    w   .   T   was set equal to 1 if the participant\u2019s choice was low choice, otherwise 0. \n\n\n### Model-based analysis \n  \nWe compared the following three models to explain the choice behavior of participants. The first was a standard reinforcement learning (RL) model, and the second was a winning rate maximization (WRM) model, which is a variant of an RL model. The third was a quantal response equilibrium (QRE) model that can yield an equilibrium with bounded rationality different from the Nash equilibrium ( ;  ). The RL model assumes that the choice probabilities are adjusted to maximize accumulated payoff outcomes for the participant. The WRM model posits that the choice probabilities are updated to maximize the participant\u2019s winning rate. The QRE model assumes that the choice probabilities are calculated based on the expected values estimated from the opponent\u2019s choice probability and payoff. \n\nBoth RL and WRM models assume that the participant learns the subjective values of chosen options based on the prediction error, i.e. the difference between the participant\u2019s subjective or predicted value for the chosen option and the actual outcome. The subjective values of the high and low choices in the RL model,   and  , were updated in a trial-by-trial manner according to the following equation: where the parameter   r  (  t  )  indicates the reward amount that the participant received at the   t  -th trial. The parameter   \u03b1   is the learning rate, dictating how much each update influences the learned value. Probabilities of the high and low choices were calculated using the softmax function:\n \nwhere the parameter   \u03b2   indicates the sensitivity to the learned values. When   \u03b2   approaches zero, the choice becomes random. When   \u03b2   increases, the choice probability of the option with larger value approaches one. \n\nThe WRM model is a variant of the RL model. The subjective values of the high and low choices in the WRM model were updated in a trial-by-trial manner according to the following equation: where   \u03b1   is the learning rate. Probabilities of the high and low choices were calculated using the softmax function:\n \n\nIn the WRM model, regardless of the amount of the outcome,   r  (  t  )  was 1 when the participant won, otherwise   r  (  t  )  was 0. (In the analysis, we aligned   r  (  t  )  of the RL model to be 1 or one-thirds instead of 60 or 20 JPY when the participant won, and 0 when the participant lost, so that the range of   r  (  t  ) for the RL model matched with that of the WRM model.) \n\nWe also examined a QRE model to see whether participants\u2019 choice behaviors were explained by the expected values of options. The expected value could be calculated by multiplying the opponent\u2019s choice probability and the payoff. The probabilities of high and low choices were calculated across four fMRI runs, for each opponent of each participant. The participant\u2019s probabilities of the high and low choices were estimated using the following equation:\n \nwhere   EV   and   EV   indicate expected values of the high and low choices, respectively. \n\nTo individually estimate the parameters of \u03b1 and \u03b2 from the behavioral data, we used the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm included in the Optimization Toolbox of MATLAB. The models were compared using the Akaike information criterion (AIC) ( ). \n\n\n### Image acquisition \n  \nA 3T Prisma scanner (Siemens Medical Systems, Erlangen, Germany) was used to acquire functional images using a 64-ch head/neck coil. Mild cushioning minimized participant head movement. Sixty-eight slices of functional images were acquired using blood oxygenation level-dependent imaging (192\u00a0mm\u2009\u00d7\u2009192\u00a0mm\u2009\u00d7\u2009136\u00a0mm, in-plane resolution\u2009=\u200996\u2009\u00d7\u200996, voxel size\u2009=\u20092\u00a0mm\u2009\u00d7\u20092\u00a0mm\u2009\u00d7\u20092\u00a0mm, thickness\u2009=\u20092\u00a0mm, TR\u2009=\u20091.5\u00a0s, TE\u2009=\u200925\u00a0ms, FA\u2009=\u200970\u00b0) using multi-band gradient-echo echo-planar sequences ( ;  ;  ) with multi-band factor\u2009=\u20094. The slices were rotated 30 degrees from the AC-PC plane to the forehead to minimize the artifact due to the sinus. The images covered the entire cerebrum after the rotation. We acquired 320 volumes in each fMRI run of the main experiment and 196 volume scans in the functional localizer task. \n\n\n### Image pre-processing \n  \nWe used SPM12 (Wellcome Department of Cognitive Neurology, University College London) in MATLAB to process the scanned images. We performed slice-timing correction using the middle slice as a reference, scan-to-scan realignment, normalization to the EPI template of SPM12, resampling the images with the voxel size of 2\u00a0mm\u2009\u00d7\u20092\u00a0mm\u2009\u00d7\u20092\u00a0mm and spatial smoothing (full width at half maximum of isotropic Gaussian kernel\u2009=\u20098\u00a0mm). A high-pass filter of 128\u00a0s was used to remove low frequency noise in the main and localizer experiments. \n\n\n### Image processing \n  \nAs many studies have revealed that the ventral striatum (VS) and ventromedial prefrontal cortex (VMPFC) are involved in learning behaviors based on RL ( ;  ;  ; see also  ), we examined whether the VS and VMPFC were associated with the prediction error of option value \u0394  V   in the first general linear model analysis (GLM1). The conditions of opponent type (HUM/FIX/LRN) were modeled for both the choice phase and feedback phase. GLM1 included the parametric modulations of prediction error \u0394  V   in the feedback phase for the opponent types separately. \n\nWe also included the brain activation for the high choice and low choice in each of the conditions analyzed in the second general linear model analysis (GLM2). For the choice phase, we prepared six conditions by combining choice (high/low)\u2009\u00d7\u2009opponent (HUM/FIX/LRN). The duration for each trial was measured from the onset (appearance of the choice options) to the button press indicating a choice. For the feedback phase, we prepared six conditions by combining result (win/loss)\u2009\u00d7\u2009opponent (HUM/FIX/LRN). The duration of these conditions was 2\u00a0s from the onset (appearance of feedback). The condition for button presses was also included in the design matrix. \n\n\n### Region of interest analysis for RTPJ and LTPJ \n  \nTo analyze the activation in RTPJ and LTPJ, we performed an analysis of regions of interest (ROIs) ( ). The RTPJ and LTPJ were individually identified using a functional localizer task for ToM that was performed after the main task ( ). In the localizer task, participants were first presented with stories about human agents who held false beliefs (false belief condition:  , upper panel) or stories about outdated physical objects (false photo condition:  , lower panel); after each, participants were queried for their inferences about the respective situations. \n\nFor the ROI definition, the peak coordinates of RTPJ and LTPJ were first identified in the group-level analysis, using the contrast of false belief condition   vs   false photo condition. Then, the individual peak within 10\u00a0mm from the group peak was identified in RTPJ and LTPJ. The ROI of RTPJ/LTPJ was individually defined as an 8\u00a0mm sphere centered at the individual peak. Not all individual ROIs overlapped due to their sizes and positions. The beta estimates in RTPJ and LTPJ in GLM2 reflecting brain activities in the choice phase and the feedback phase were extracted using MarsBaR ( ) and sent to further statistical analyses. \n\nWe also performed a multivariate pattern analysis to examine the similarity of brain activation patterns during the game against the three types of opponents. Here, we focused on the brain activity in the ROI during the choice phase. Because we were concerned with the overall similarity (or distance) of the LTPJ/RTPJ activation patterns in the HUM condition with those in the FIX and LRN conditions, we used representational similarity analysis (RSA,  ), rather than pattern classification analysis (e.g. support vector machine, which evaluates the accuracy of classification of an activation pattern in an ROI by defining a hyperplane as the border between the conditions:  ). We first calculated spatial correlations (Spearman\u2019s rank-order correlation) between the beta estimates for the six conditions of choice (high/low)\u2009\u00d7\u2009opponent (HUM/FIX/LRN) and then created a representational distance matrix (RDM,  ;  ) by subtracting these correlations from one. The RDM in this study was a symmetric 6\u2009\u00d7\u20096 matrix, where the off-diagonal elements indicated the distances of pairs among the six (choice \u00d7 opponent) conditions. \n\nTo see which artificial agent (FIX or LRN) solicited participant responses that better represented their response patterns to actual human opponents (HUM) at the neural level (i.e. activities of LTPJ/RTPJ identified by the functional localizer with the ToM task), we first collapsed high and low choices and then compared the Fisher-  z  -transformed Spearman\u2019s correlation of HUM\u2013LRN with that of HUM\u2013FIX. We hypothesized that the activity pattern for HUM would be significantly more similar to that for LRN (the learning agent) than to FIX (the agent using fixed mixed strategy), because LRN (like HUM) was responsive to the participants\u2019 choices in a bilateral manner, while FIX was not. \n\n\n\n## Results \n  \n### Behavioral results \n  \nAs shown in  , the participants\u2019 high choice rates decreased over the course of the session. A two-way repeated-measures analysis of variance (ANOVA) over opponent (HUM/FIX/LRN)\u2009\u00d7\u2009fMRI runs yielded a significant main effect of fMRI run,   F  \u2009=\u200912.0,   P  \u2009<\u20090.001. A planned contrast between HUM\u2013LRN combined   vs   FIX was also significant (  F  \u2009=\u20096.08,   P  \u2009=\u20090.020), supporting our hypothesis that participants\u2019 choice behaviors would be similar between HUM and LRN but distinct from FIX. The percentage of high choices in the HUM condition was also significantly correlated with that of LRN condition (  r  \u2009=\u20090.60,   P  \u2009<\u20090.001;  ) but not with that of FIX condition (  r  \u2009=\u20090.20,   P  \u2009=\u20090.28;  ). The high choice rates in FIX and LRN conditions were also not correlated with each other (  r  \u2009=\u20090.13,   P  \u2009=\u20090.49,  ). \n  \nBehavioral results. (A) Percentage of high choices in four fMRI runs. The participants significantly reduced their high choice rates over time. A planned contrast between HUM-LRN combined and FIX was also significant. (B) Significant correlation of high choice rates between HUM and LRN conditions (  r  \u2009=\u20090.60,   P  \u2009<\u20090.001). (C) No significant correlation of High choice rates between HUM and FIX conditions (  r  \u2009=\u20090.20,   P  \u2009=\u20090.28). (D) No significant correlation of high choice rates between FIX and LRN conditions (  r  \u2009=\u20090.13,   P  \u2009=\u20090.49). (E) No significant difference in winning rates among the opponent types. (F) Monetary outcomes in each opponent condition. The outcomes in HUM and LRN conditions (not distinguishable from each other) were significantly higher than that in FIX condition. The asterisks indicate statistical significance (  P  \u2009<\u20090.001). \n  \nThe winning rates (collapsed over the high and low choices) were all approximately 0.5 ( ; one-sample   t   test: HUM,   t  \u2009=\u2009\u22121.08,   P  \u2009=\u20090.29; FIX,   t  \u2009=\u20090.15,   P  \u2009=\u20090.88; LRN,   t  \u2009=\u20090.22,   P  \u2009=\u20090.83) and not significantly different among the three opponent conditions (one-way repeated-measures ANOVA,   F  \u2009=\u20090.41,   P  \u2009=\u20090.67). The monetary outcomes that participants obtained in the HUM and LRN conditions were not distinguishable from each other ( :   t  \u2009=\u20090.56,   P  \u2009=\u20090.58), but both were significantly higher than those in the FIX condition (  vs   HUM,   t  \u2009=\u20095.56,   P  \u2009<\u20090.001;   vs   LRN,   t  \u2009=\u20095.83,   P  \u2009<\u20090.001; see also   for details). \n\nWe confirmed that the estimated learning rate of human opponent was similar to the learning rate of LRN (which had been set to 0.25), although the WRM and RL models were different from the learning model of LRN. The learning rates estimated using the WRM and RL models for the human opponent were 0.26\u2009\u00b1\u20090.06 and 0.22\u2009\u00b1\u20090.07 (mean\u2009\u00b1\u2009SEM), respectively, which were not significantly different from that of LRN (RL:   t  \u2009=\u2009\u22120.38,   P  \u2009=\u20090.71; WRM:   t  \u2009=\u20090.18,   P  \u2009=\u20090.86). Next, we examined the learning rate of the participant using the WRM model. The learning rates estimated in the HUM condition (0.34\u2009\u00b1\u20090.08) and the LRN condition (0.40\u2009\u00b1\u20090.06) were indistinguishable from each other (  t  \u2009=\u2009\u22120.80,   P  \u2009=\u20090.43) and correlated (Spearman\u2019s   r  \u2009=\u20090.51,   P  \u2009=\u20090.0044\u2009<\u20090.01). On the other hand, neither the correlation of the learning rates between the HUM and FIX conditions (Spearman\u2019s   r  \u2009=\u2009\u22120.082,   P  \u2009=\u20090.67) nor that between the LRN and FIX conditions (Spearman\u2019s   r  \u2009=\u20090.198,   P  \u2009=\u20090.29) was significant. These results indicate that learning processes were similar between the HUM condition and the LRN condition but different from the FIX condition. \n\nThe response times for choices were similar among all three opponent conditions (HUM: 511\u2009\u00b1\u200925\u00a0ms; FIX: 487\u2009\u00b1\u200922\u00a0ms; LRN: 476\u00a0ms\u2009\u00b1\u200922\u00a0ms; mean\u2009\u00b1\u2009SEM,   F  \u2009=\u20091.30,   P  \u2009=\u20090.28 by a one-way repeated-measures ANOVA). \n\n\n### Model comparison \n  \nWe compared the WRM model, RL model and QRE model in terms of goodness of fit to the participants\u2019 behavioral choices. The AIC of the WRM model was smaller than those of the RL model and the QRE model in all three conditions ( ), indicating that the WRM model achieved the best fit to the participants\u2019 choices. The posterior predictive check on the WRM model showed significantly higher match rates (relative to the chance level) in all three conditions, whereas the RL model showed a significantly higher match rate only in the FIX condition ( ). Furthermore, the WRM model also best explained the choice behaviors of the human opponents who played the game outside the scanner. However, it should be noted that even the best-fitting WRM model was able to capture only 58.5% of the participants\u2019 actual choices at most ( ) and the difference in AIC between the WRM and RL models was also very small ( ). Therefore, the following analysis of imaging data used both the WRM and the RL models. \n  \nAICs of the three models considered in this study \n  \nNote. The row for HUM  shows AICs of the three models when fitted to the choices of the human opponents who played the game from outside the scanner. \n  \n\n### Whole brain activation for \u0394V  and \u0394V \n  \nThe analysis for parametric modulation of \u0394  V   and \u0394  V   in GLM1 using the standard threshold (  P  \u2009<\u20090.001 for cluster identification without correction,   P  \u2009<\u20090.05 for cluster level significance with FWE correction) showed the activation of various and large regions ( ), so we used a stricter threshold (  P  \u2009<\u20090.05 for voxel level significance with FWE correction, cluster size threshold   k  \u2009>\u200950).   shows clear activations of VS and VMPFC in response to both \u0394  V   and \u0394  V  , as in the previous imaging studies using variants of RL models (e.g.  ). The activities shown by the WRM and RL models ( ) mostly overlapped, in accord with the results of the model comparisons ( ). \n  \nImaging results of the whole brain analysis for the parametric modulation of prediction errors, \u0394  V   and \u0394  V  . The activation for prediction errors was observed in the VS and the VMPFC. The region colored with magenta shows the sheer activation for WRM, while the region colored with blue shows the sheer activation for RL. The overlapping region is colored violet. \n    \nSummary of the results of parametric modulation analysis \n  \nL, left; R, right; PCC, posterior cingulate cortex. \n  \n\n### Activation difference of RTPJ and LTPJ \n  \nRTPJ and LTPJ were individually identified using the functional localizer in this study ( , see also   and  ). The peak coordinates in the group-level analysis with the stricter threshold (  P  \u2009<\u20090.05 for voxel level significance with FWE correction:  ) were used to identify the individual peaks in RTPJ and LTPJ and their activities while the participants played the game were extracted and compared. The LTPJ was activated in the choice phase, whereas the RTPJ was activated in the feedback phase, as shown in   and confirmed by a significant interaction in a two-way repeated measures ANOVA [phase (choice/feedback)\u2009\u00d7\u2009ROIs (LTPJ/RTPJ);   F  \u2009=\u200924.3,   P  \u2009<\u20090.001]. In comparisons, the LTPJ showed the significantly higher activation than the RTPJ in the choice phase (  t  \u2009=\u20092.79,   P  \u2009=\u20090.009), and the activation of RTPJ was significantly higher than that of LTPJ in the feedback phase (  t  \u2009=\u20095.46,   P  \u2009<\u20090.001). More specifically, the activation of RTPJ was significantly higher than zero in the feedback phase (  t  \u2009=\u20097.31,   P  \u2009<\u20090.001, Bonferroni corrected for the number of ROIs) but not in the choice phase (  t  \u2009=\u2009\u22120.95,   P  \u2009=\u20090.70, Bonferroni corrected). In contrast, the activation of LTPJ was significantly higher than zero in the choice phase (  t  \u2009=\u20092.49,   P  \u2009=\u20090.019, Bonferroni corrected) but not in the feedback phase (  t  \u2009=\u2009\u22120.68,   P  \u2009=\u20090.50, Bonferroni corrected). \n  \nActivation in the ROIs of RTPJ and LTPJ. A. The individually defined LTPJ and RTPJ, identified using the theory-of-mind functional localizer (see Supplementary  ). The color reflects the number of overlapped individual ROIs. B. Activation of LTPJ and RTPJ in the choice and feedback phases. The phase\u2013ROI interaction was significant, indicating that the LTPJ was activated in the choice phase, while the RTPJ was activated in the feedback phase. The asterisks indicate statistical significance (  P  \u2009<\u20090.05,   P  \u2009<\u20090.001). \n    \nSummary of the group-level results of the functional localizer \n  \nL\u2009=\u2009left, R\u2009=\u2009right. \n  \n\n### RTPJ activity \n  \nWe compared the brain activation of RTPJ in the choice phase for the combination of three opponents (HUM/FIX/LRN) and two choice options (high/low). Although the overall activation level was not statistically distinguishable from zero during the choice phase (  left), the main effect of opponent was significant by a two-way repeated measures ANOVA (  F  \u2009=\u20096.90,   P  \u2009=\u20090.002 after Bonferroni correction for the number of ROIs:  ). The RTPJ activation was significantly higher in HUM than in FIX and LRN conditions (post-hoc Tukey\u2013Kramer test, both   P  \u2009<\u20090.01). \n  \nImaging results for RTPJ. A. The activation of RTPJ in the choice phase. Although the overall activation was not statistically distinguishable from zero (see  ), the main effect of opponent was significant by a two-way repeated measures ANOVA over opponent (HUM/FIX/LRN)\u2009\u00d7\u2009choice (high/low): RTPJ activation was significantly higher in HUM than in FIX and LRN conditions. B. Activation of RTPJ in the feedback phase. RTPJ activation was significantly higher in the HUM condition than in the FIX and LRN conditions. The asterisks indicate statistical significance (  P  \u2009<\u20090.01,   P  \u2009<\u20090.001). \n  \nAs seen in  , this main effect was also clearly observed in the feedback phase in which the overall activation of RTPJ was significantly greater than zero (  right). A two-way repeated measures ANOVA of opponent (HUM/FIX/LRN)\u2009\u00d7\u2009result (win/loss) revealed the significant main effect of opponent type (  F  \u2009=\u200918.4,   P  \u2009<\u20090.001 after Bonferroni correction for the number of ROIs), and the post-hoc Tukey\u2013Kramer test again confirmed that the brain activation in HUM condition was significantly larger than those in FIX and LRN condition (both   P  \u2009<\u20090.001). Taken together, these patterns suggest that the activation of RTPJ reflected the perception of human agency when the participants played against the human opponent outside of the scanner. \n\nIn line with the absence of overall activation in the choice phase (  left), RTPJ activity had no significant relation with behavioral choices in any of the three conditions. As shown in  , correlations between participant\u2019s high choice rate and RTPJ beta for high choice were all non-significant (HUM:   r  \u2009=\u20090.16,   P  \u2009=\u20090.39; FIX:   r  \u2009=\u2009\u22120.09,   P  \u2009=\u20090.64; LRN:   r  \u2009=\u20090.33,   P  \u2009=\u20090.08). \n\n\n### LTPJ activity \n  \nWe examined the LTPJ activation similarly to that of the RTPJ. LTPJ activation showed no significant effect in a two-way repeated-measures ANOVA over opponent (HUM/FIX/LRN)\u2009\u00d7\u2009choice (high/low) either in the choice phase ( ) or the feedback phase ( ). \n\nIn the behavioral results ( ), we observed that the high choice rate in the HUM condition was remarkably similar to the LRN condition but clearly distinct from the FIX condition. A similar pattern can be observed for LTPJ activation in the choice phase (  left). Consistent with this conjecture, LTPJ activation for high choice was significantly correlated with the high choice rate in the HUM and LRN conditions, but this relationship was weak in the FIX condition ( ; HUM:   r  \u2009=\u20090.48,   P  \u2009=\u20090.007; FIX:   r  \u2009=\u20090.34,   P  \u2009>\u20090.10; LRN:   r  \u2009=\u20090.43,   P  \u2009=\u20090.018, Bonferroni corrected for the number of ROIs). \n  \nImaging results in the LTPJ and results of RSA in the TPJs. A. The relation between LTPJ activation for high choice and the high choice rate. Significant correlation was observed in HUM and LRN conditions but not in FIX condition. B. Schema to calculate RDM. Beta estimates were extracted from the individually defined TPJ, and then the correlation matrix of the beta estimates was calculated and converted to the RDM. C. The RDM of activation in the LTPJ. The color bar shows the distance between conditions. D. The difference of Fisher-  z  -transformed correlations of HUM-FIX and HUM-LRN. The Fisher-  z  -transformed correlation of HUM-LRN was significantly higher than that of HUM-FIX. The asterisk indicates statistical significance (  P  \u2009<\u20090.05). E. The RDM of activation in the RTPJ. The color bar shows the distance between the conditions. F. Similar activity patterns of RTPJ between HUM-FIX and HUM-LRN comparisons. The Fisher-  z  -transformed correlation was not significantly different between HUM-FIX and HUM-LRN. \n  \nThese results indicate that LTPJ activity in the HUM condition may be more similar to that in the LRN condition than to that in the FIX condition. We compared the activity patterns of the HUM, FIX and LRN conditions using a RSA. First, the beta estimates in the choice phase between the six conditions [choice (high/low)\u2009\u00d7\u2009opponent (HUM/FIX/LRN)] were extracted from the LTPJ ROI ( ). Next, the distance in each pair of the six conditions was defined as 1 minus the spatial correlation of beta estimates. The RDM of LTPJ indicated that the activity pattern was similar within each opponent type but less similar across the opponent types ( ). Most importantly, the Fisher-  z  -transformed correlations showed that the activation pattern of LTPJ in the HUM condition was more similar to that in the LRN condition than to that in the FIX condition ( :   t  \u2009=\u20092.12,   P  \u2009=\u20090.042, Bonferroni corrected for the number of ROIs). However, no such dissociation among the three conditions was observed in the activation pattern in RTPJ (  F:   t  \u2009=\u20090.38,   P  \u2009=\u20090.71, Bonferroni corrected for the number of ROIs; in contrast to the RSA approach, the classifier approach using a support vector machine did not yield significantly higher classification above the chance level for either LTPJ or RTPJ\u2014see  ). In the feedback phase, the activation patterns in LTPJ (and also RTPJ) were not dissociable among the three conditions ( ). Taken together, these results may indicate that LTPJ is involved in second-order inferences (inferring another agent\u2019s inferences about one\u2019s own belief) when making competitive strategic choices. \n\n\n\n## Discussion \n  \nThis fMRI study investigated the behavioral and neural bases for strategic decision making when the participant must consider an opponent\u2019s inferences about the participant\u2019s own beliefs in a bilateral manner. As hypothesized, the participants\u2019 choice behaviors against the human opponent (HUM) were remarkably close to those against LRN, which learned and exploited the participant\u2019s prior choices, as compared with those against FIX, which always followed a fixed probabilistic strategy. The model-based analyses showed that the participants\u2019 choice behaviors were better predicted by a learning model that maximized winning rate rather than monetary outcome. We also confirmed that winning was rewarding in itself, as shown by the activation of VS and VMPFC associated with the prediction error of the winning rate (\u0394  V  ). Furthermore, RTPJ and LTPJ, identified by the ToM localizer, showed double dissociation of activation and activity patterns between the choice and feedback phases. The activation of RTPJ in the feedback phase was significantly larger when playing against the human opponent (HUM) than the computer opponents (FIX and LRN). In contrast, the activity patterns of LTPJ in the choice phase showed a greater similarity between HUM and LRN conditions than between HUM and FIX conditions, paralleling the significant correlation between the participants\u2019 choice behaviors and the corresponding LTPJ activity in HUM and LRN (but not in FIX) conditions. \n\nAs mentioned earlier, we assumed two main features of the human opponent in this study: human agency and strategic bilateral inference about the participant\u2019s choice behaviors. Obviously, neither computer agent had human agency. But similar to HUM, LRN learned and constantly adjusted to the participant\u2019s choice behavior for possible exploitation, while FIX did not and instead followed a fixed choice probability. The double dissociation we have observed in this study seems to reflect these two features (human agency and bilateral strategic inference). In the following, we first discuss these differential activations of RTPJ and LTPJ in more detail. \n\nRTPJ activation was larger for the human opponent than for the computer opponents (FIX and LRN) in the feedback phase, while its activity was negligible in the choice phase and also uncorrelated with the participants\u2019 high choice rates. In previous research using behavioral games (e.g.  ), human face pictures (in contrast to computer pictures) have often been used to increase participants\u2019 feelings of an opponent\u2019s human agency. However, because we were concerned that such manipulation might also evoke differences in arousal or emotional states beyond the perception of human agency, we presented only a word cue to signal that the opponent in the current trial was either a student of the same university or a computer. We still observed a significant difference in RTPJ activation between the HUM condition and the two computer conditions. \n\nIn contrast, LTPJ activation in the choice phase was correlated with high choice rate in the HUM and LRN conditions, but not in the FIX condition; behaviorally, high choice rate in HUM was also correlated with that in LRN, but not with FIX. Moreover, LTPJ activity patterns, as identified by the RSA, were similar between the HUM and LRN conditions, but not between HUM and FIX. These behavioral and neuroimaging results indicate that the LTPJ is involved in strategic decision making against an opponent that can actively learn from the prior actions of the player and infer their future actions. Previous studies have suggested that the LTPJ is associated with perceiving differences between the mental states of self and other ( ;  ;  ). A recent study by  )) also reported a similar result involving the LTPJ in the context of a trust game. These researchers found a significant correlation between strength of LTPJ connectivity (e.g. with posterior superior temporal sulcus) and investment behavior when participants made trust decisions with a human counterpart, but not when they made the same investment decisions in a non-social control game. These results corroborate the argument that LTPJ activity reflects elements of social decision making. \n\nTaken together, our results showed a functional dissociation of RTPJ and LTPJ in a strategic decision making context. The RTPJ is involved in the perception of the human agency that constitutes a basis for reasoning about the opponent\u2019s mental state ( ;  ;  ;  ;  ;  ;  ;  ;  ), whereas the LTPJ may be involved in strategic planning of choices against intelligent, bilaterally-responsive agents, be they human or non-human. \n\nIn this study, we identified the ROIs of LTPJ and RTPJ for each of the participants separately, using the ToM localizer ( ). Notice that the individual identification of functional brain regions was used not only for comparing activation between conditions but also for comparing activity patterns through RSA. The former compared the averaged activities in each ROI, whereas the latter examined the voxel-scale similarities/differences of activity in the ROI. That is, these two analyses reflected different aspects of neural activity. We believe that the ToM localizer, which enables using exactly the same ROI throughout the two analyses, will thus be beneficial in future research that investigates functional dissociations between RTPJ and LTPJ in strategic decision making. \n\nIt could be argued that the asymmetric payoff matrix employed in this study may have introduced the possibility of social preferences (e.g. inequity aversion, empathy, etc.) biasing participants\u2019 computations of value when making choices. It is true that the asymmetric payoff matrix allowed participants to earn more than the human opponent if they played the mixed strategy of the Nash equilibrium, and this may have triggered reactions of empathy or advantageous inequity aversion. We thus additionally analyzed the activation of the anterior insula (AI), which has been shown to be associated with such social preferences ( ;  ;  ; but see   indicating association of AI with other functions). The result showed no significant difference in AI activation between the win and loss feedback situations where inequity aversion could be at work ( ), which could imply that its influence was less evident in the competitive context of our matching-pennies game, as compared with a distributive context ( ). However, as we did not directly assess neural correlates of inequity aversion, the absence of AI activation remains only suggestive and should be treated with caution against reverse inference ( ;  ). Future research should address the role of social preferences in a competitive context more directly, along with possible involvement of other brain regions (e.g. TPJ:  ) in inequity aversion. \n\nFinally, our model-based behavior analysis showed that the WRM model was the best fit to the participants\u2019 choices, although the difference between the WRM model and the (second best) RL model in fitness was small. Activation related to the learning process specified by the WRM model and the RL model was observed (and mostly overlapped) in the VS and the VMPFC, the reward regions identified by previous studies ( ;  ;  ;  ;  ;  ). These indicate that winning itself worked as a reward for the participants as well as the monetary payoff. It could be argued that the participants might have felt the monetary reward in this experiment to be too small and thus focused more on winning. However, the participants were clearly instructed that the accumulated payoff outcome could be substantial and would be paid as a cash bonus. Furthermore, the accumulated payoff outcomes for the high choice were significantly larger than for the low choice in HUM and LRN conditions ( ), suggesting that the outcome difference between high and low choices was meaningful for the participants. Taken together, the results indicate that winning in itself, as well as the monetary outcomes, worked as a strong reward for the participants (cf.  ;  ). \n\nThis study investigated the neural basis for inferring an opponent\u2019s inferences about one\u2019s own beliefs in a competitive decision making context. Choice behaviors against the human opponent and LRN were systematically different from those against FIX. The RTPJ showed significantly higher activation in HUM condition than in the computer conditions in the feedback phase, while the LTPJ activity pattern showed higher similarity between HUM and LRN conditions than between HUM and FIX conditions. These results suggest that the RTPJ is mainly associated with the perception of human agency, and the LTPJ is involved in second-order inferences (those about others\u2019 inferences about one\u2019s own beliefs) in competitive situations. \n\n\n## Funding \n  \nThis work was supported by Japan Society for the Promotion of Science (JSPS) KAKENHI (JP25118004 and JP16H06324 to T.K. and JP16K16076 and 19K07807 to A.O.) and Japan Science and Technology Agency CREST (JPMJCR17A4 (17941861) to T.K.). Support from CiSHub at the University of Tokyo is also appreciated. \n\n\n## Supplementary Material \n  \n \n\n# Table(s)\n\n## ID: TB1\n\n### Label: Table 1\n\n     Condition  WRM model  RL model  QRE model\n0          HUM      65.72     66.08      66.68\n1          FIX      63.58     65.60      66.86\n2          LRN      62.36     62.76      66.92\n3  HUMopponent      66.26     67.36      67.42\n\n### Caption\n\nAICs of the three models considered in this study\n\n### Footer\n\nNote. The row for HUMopponent shows AICs of the three models when fitted to the choices of the human opponents who played the game from outside the scanner.\n\n\n\n## ID: TB2\n\n### Label: Table 2\n\n    Parameter/regions MNI coordinates of the peak (mm) MNI coordinates of the peak (mm).1 MNI coordinates of the peak (mm).2  z score (peak)  P FWE (peak)  Number of voxels\n0   Parameter/regions                                x                                  y                                  z  z score (peak)  P FWE (peak)  Number of voxels\n1               \u0394VWRM                              NaN                                NaN                                NaN             NaN           NaN               NaN\n2                  VS                              \u221214                                  6                                \u221212            6.65        <0.001             482.0\n3                R VS                               12                                 12                                \u221212            6.65        <0.001             501.0\n4               VMPFC                               \u22128                                 46                                \u221210            6.26        <0.001            1379.0\n5                 PCC                               \u22122                                \u221224                                 38            5.82        <0.001             301.0\n6                \u0394VRL                              NaN                                NaN                                NaN             NaN           NaN               NaN\n7                L VS                              \u221214                                  6                                \u221212            7.06        <0.001             547.0\n8                R VS                               12                                  8                                \u221214            6.92        <0.001             567.0\n9               VMPFC                                0                                 54                                 \u22128            6.29        <0.001            1604.0\n10                PCC                                0                                \u221222                                 36            6.46        <0.001             539.0\n\n### Caption\n\nSummary of the results of parametric modulation analysis\n\n### Footer\n\nL, left; R, right; PCC, posterior cingulate cortex.\n\n\n\n## ID: TB3\n\n### Label: Table 3\n\n              Contrast/regions MNI coordinates of the peak (mm) MNI coordinates of the peak (mm).1 MNI coordinates of the peak (mm).2  z score (peak)  P FWE (peak)  Number of voxels\n0             Contrast/regions                                x                                  y                                  z  z score (peak)  P FWE (peak)  Number of voxels\n1  False belief vs false photo                              NaN                                NaN                                NaN             NaN           NaN               NaN\n2                         RTPJ                               52                                \u221252                               18.0            6.36        <0.001             878.0\n3                         LTPJ                              \u221242                                \u221254                               18.0            6.72        <0.001             718.0\n\n### Caption\n\nSummary of the group-level results of the functional localizer\n\n### Footer\n\nL\u2009=\u2009left, R\u2009=\u2009right.\n\n", "metadata": {"pmcid": 6970153, "text_md5": "d18565d787ce090a258ab0b5a3e121e9", "field_positions": {"authors": [0, 35], "journal": [36, 60], "publication_year": [62, 66], "title": [77, 174], "keywords": [188, 270], "abstract": [283, 1606], "body": [1615, 45268], "tables": [45282, 49086]}, "batch": 1, "pmid": 31680151, "doi": "10.1093/scan/nsz082", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6970153", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6970153"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6970153\">6970153</a>", "list_title": "PMC6970153  Dissociable roles of left and right temporoparietal junction in strategic competitive interaction"}
