{"text": "Mancini, Flavia and Zhang, Suyi and Seymour, Ben\nNat Commun, 2022\n\n# Title\n\nComputational and neural mechanisms of statistical pain learning\n\n# Keywords\n\nPain\nLearning algorithms\n\n\n# Abstract\n \nPain invariably changes over time. These fluctuations contain statistical regularities which, in theory, could be learned by the brain to generate expectations and control responses. We demonstrate that humans learn to extract these regularities and explicitly predict the likelihood of forthcoming pain intensities in a manner consistent with optimal Bayesian inference with dynamic update of beliefs. Healthy participants received probabilistic, volatile sequences of low and high-intensity electrical stimuli to the hand during brain fMRI. The inferred frequency of pain correlated with activity in sensorimotor cortical regions and dorsal striatum, whereas the uncertainty of these inferences was encoded in the right superior parietal cortex. Unexpected changes in stimulus frequencies drove the update of internal models by engaging premotor, prefrontal and posterior parietal regions. This study extends our understanding of sensory processing of pain to include the generation of Bayesian internal models of the temporal statistics of pain. \n  \nPain fluctuates over time in ways that are non-random. Here, the authors show that the human brain can learn to predict these changes in a manner consistent with optimal Bayesian inference by engaging sensorimotor, parietal, and premotor regions. \n \n\n# Body\n \n## Introduction \n  \nThe main function of the pain system is to minimise harm and, to achieve this goal, it needs to learn to predict forthcoming pain. To date, this has been studied using cue-based paradigms, in which a learned or given cue, such as a visual image, contains the relevant information about an upcoming pain stimulus . A much more general, although neglected, route to generate predictions relates to the background statistics of pain over time, i.e. the underlying base-rate of getting pain, and of different pain intensities, at any one moment. This is important because clinical pain typically involves long-lasting streams of noxious signals, characterised by temporal regularities that underscore the temporal evolution of pain . \n\nThe brain can use associative learning strategies to predict pain from cues, but these algorithms do not learn the structure of the environment . In principle, the pain system should be able to generate predictions by learning regularities (i.e. structures) in its temporal evolution, in absence of other information. This possibility is suggested by research in other sensory domains, showing that the temporal statistics of sequences of inputs are learned and inferred through experience - a process termed temporal statistical learning . We hypothesise that temporal statistical learning also occurs in the pain system, allowing the brain to infer the prospective likelihood of pain by keeping track of ongoing temporal statistics and patterns. \n\nHere, we tested this hypothesis by designing a temporal statistical learning paradigm involving long, probabilistic sequences of noxious stimuli of low and high intensities, whose transition probabilities could suddenly change. We tested people\u2019s ability to generate explicit predictions about the probability of forthcoming pain, defined the underlying computational principles and revealed their neural correlates. We investigated the following computational features: (1) is the stimulus sequence learnt using an optimal Bayesian inference strategy, or an heuristic (a model-free delta rule)? (2) Does the learning update take into account the volatility of the sequence? (3) Which temporal statistics is inferred, stimulus frequencies or transition probabilities? \n\nAfter identifying the computational principles of learning to predict pain sequences, we reveal the brain regions that encode these predictions, their uncertainty and update, using functional MRI. We hypothesised statistical predictions for pain would follow the fundamental rules of optimal Bayesian inference, based on previous work on other sensory modalities . We were particularly interested in understanding whether probabilistic predictions of pain might be encoded in somatosensory processing regions (primary/secondary somatosensory cortex and insula), given that statistical inferences of visual and auditory inputs can be encoded in visual and auditory regions . This would allow us to map core regions of the pain system to specific functional information processing operations, i.e. the statistical inference of pain. \n\n\n## Results \n  \nThirty-five participants (17 females; mean age 27.4 years old; age range 18\u201345 years) completed an experiment with concurrent whole-brain fMRI. They received continuous sequences of low- and high-intensity electrical stimuli, eliciting painful sensations. Participants were required to intermittently judge the likelihood that the next stimulus was of high versus low intensity, given the previous stimulus (Fig.\u00a0 a, b).    Behavioural task and model explanation.  \n a   Example trials from a representative participant, showing: the true probability of high (H) and low (L) stimuli given current stimuli, trial stimulation given, and participant rated probabilities. The arrows point to jump points of true probabilities, where a sudden change happens.   b   Rating screens. Occasionally, the sequence was paused and participants were asked to estimate the likelihood of the upcoming stimulus given the current one. For example, after a low stimulus participants would be asked to rate the probability of the upcoming stimulus being low (L\u2009\u2212\u2009>\u2009L) or high (L\u2009\u2212\u2009>\u2009H).   c   Graphical representation of the Markovian generative process of the sequence of low and high-intensity stimuli. The transition probability matrix was resampled at change points, determined by a fixed probability of a jump. \n  \n\nWe designed the task such that the statistics of the sequence could occasionally and suddenly change (i.e. they were volatile), which meant that the sequences of 1500 stimuli included sub-sequences of stimuli (mean 25\u2009\u00b1\u20094 stimuli per sub-sequence). Participants were not explicitly informed when these changes happened. Figure\u00a0 a illustrates an example of a snapshot of a typical sequence, showing a couple of \u2018jump\u2019 points where the probabilities change. The sequence statistics were Markovian and, thus, incorporated two types of information. First, they varied in terms of the relative frequency of high and low-intensity stimuli (from 15 to 85%), to test whether the frequency statistics can be learned. Second, sequences also contained an additional aspect of predictability, in which the conditional probability of a stimulus depended on the identity of the previous stimulus (i.e. the transition probability of high or low pain following a high pain stimulus, and the transition probability of high or low pain following a low pain stimulus; Fig.\u00a0 c). By having different transition probabilities between high and low stimuli within sub-sequences, it is possible to make a more accurate prediction of a forthcoming stimulus intensity over-and-above simply learning the general background statistics. Thus, we were able to test whether participants learn the frequency or the transition probabilities between different intensities, as shown previously with visual stimuli . For this reason, our design mirrored a well-studied task used to probe statistical learning with visual stimuli . From a mathematical point of view, the frequency can always be derived from the transition probabilities, but not vice versa. Therefore, participants were not asked to rate the frequency of the stimuli because it can be simply derived from their transition probability ratings; in contrast, transition probability estimates cannot be derived from frequency estimates. \n\n### Behavioural results \n  \nParticipants were able to successfully learn to predict the intensity (high versus low) of the upcoming painful stimulus within the sequence based on its frequency. We measured the linear relation between the true and rated frequency of low and high pain respectively, for each participant. As shown in Fig.\u00a0 a, 83% of participants showed a positive association, greater than 0, between true and rated frequencies (median Pearson\u2019s r\u2009=\u20090.25, SD\u2009=\u20090.19). Across subjects, the within-individual Pearson\u2019s r between true and rated frequencies was significantly above zero (t(34)\u2009=\u20096.10,   p  \u2009<\u20090.001, Cohen\u2019s d\u2009=\u20091.03;  c). This indicates that the majority of participants were able to predict the frequency of the stimuli, despite the volatility of the sequence.    Behavioural results.  \n a   Relation between generative and rated frequencies, p(H), in each participant (one regression line per participant,   n  \u2009=\u200935).   b   Relation between generative and rated transition probabilities, p(H\u2223L), in each participant (one regression line per participant,   n  \u2009=\u200935).   c   Estimation accuracy, as measured by the correlation coefficient between generative and rated probabilities: frequency p(H) and transition probability p(H\u2223L); each circle represents one participant (  n  \u2009=\u200935). The boxes show the quartile of the data and the whiskers illustrate the rest of the distribution, except for points that are classified as \"outliers'' based on the inter-quartile range. The two sets of correlations were not significantly different, based on a two-sided z-test on Fisher-transformed correlation coefficients (z\u2009=\u20090.376,   p  \u2009=\u20090.707). Source data are provided as a Source Data file. \n  \n\nNext, we checked whether participants were able to also predict higher order statistics, i.e. the transition probability between the stimuli. This seemed to be quite challenging for our participants. In 74% of participants there was a positive correlation between true and rated transition probabilities (P(H\u2223H): median Pearson\u2019s r\u2009=\u20090.16, SD\u2009=\u20090.23; P(H\u2223L): median Pearson\u2019s r\u2009=\u20090.13, SD\u2009=\u20090.22). As shown in Fig.\u00a0 b, c, 26% of participants showed negative linear relations, indicating that they could not predict transition probabilities. At group level, the relation between true and rated transition probabilities was significantly greater than 0 (p(H\u2223H): t(34)\u2009=\u20093.65,   p  \u2009<\u20090.001, Cohen\u2019s d\u2009=\u20090.616; p(H\u2223L): t(34)\u2009=\u20093.15,   p  \u2009=\u20090.0034, Cohen\u2019s d\u2009=\u20090.532; note that p(H\u2223L) and p(L\u2223L) are reciprocal, as well as p(H\u2223L) and p(L\u2223L)). \n\nFurthermore, we found no evidence for a correlation between the participant prediction accuracy (as measured by the correlation coefficient between generative and rated probabilities) and perceived pain intensity for the high pain stimulus, averaged across sessions (frequency prediction accuracy by high pain intensity: r\u2009=\u2009\u22120.175,   p  \u2009=\u20090.337; p(H\u2223H) prediction accuracy by high pain intensity: r\u2009=\u2009\u22120.178,   p  \u2009=\u20090.305; Supplementary Fig.\u00a0 ). \n\n\n### Behavioural data modelling \n  \n#### Model choice \n  \nWe adopted a normative approach to identify the mathematical principles that underlie learning to predict pain sequences, based on previous evidence in other sensory domains . We designed six computational models to address three main questions. \n\nFirstly, we investigated whether the inference follows the rules of optimal Bayesian inference, an heuristic (a simple delta rule), or it is simply random. To this purpose, we compared a family of four Bayesian inference models , a basic reinforcement learning model with a fixed learning rate  and a baseline random model that assumes constant probabilities throughout the experiment for high and low pain respectively. \n\nSecondly, we evaluated whether the Bayesian inference incorporates the possibility (prior probability) of sudden changes in stimulus probability or ignores such possibilities. Given that the volatility of the stimuli did not change over time, the Bayesian \u2019jump\u2019 models had a constant prior of the probability of a jump. The Bayesian \u2019fixed\u2019 models did not have any prior over the volatility of the stimuli, but had a leaky integration with an exponential decay to mimic forgetting. \n\nLastly, the Bayesian (jump and fixed) models differed according to the temporal statistics they inferred: the stimulus frequency or the transition probability. The Bayesian frequency models assume the sequence as generated by a Bernoulli process, where observers track how often they encountered previous stimuli. In contrast, the Bayesian transition probability models assume the sequence follows a Markov transition probability between successive stimuli, where observers estimate such transition of previous stimuli. \n\n\n#### Model fitting \n  \nThe selected models estimate the probability of a pain stimulus\u2019 identity in each trial. The values predicted by the model can be fitted to the subjects\u2019 probability ratings gathered during the experiment. A model is considered a good fit to the data if the total difference between the model-predicted values and the subjects\u2019 predictions is small. Within each model, free parameters were allowed to differ for individual subjects in order to minimise prediction differences. For Bayesian \u2018jump\u2019 models, the free parameter is the prior probability of sequence jump occurrence. For Bayesian fixed models, the free parameters are the window length for stimuli history tracking, and an exponential decay parameter that discounts increasingly distant previous stimuli. The RL model\u2019s free parameter is the initial learning rate, and the random model assumes a fixed high pain probability that varies across subjects. The model fitting procedure minimises each subject\u2019s negative log likelihood for each model, based on residuals from a linear model that predicts subject\u2019s ratings using model predictors. The smaller the sum residual, the better fit a model\u2019s predictions are to the subject\u2019s ratings. \n\n\n#### Model comparison \n  \nWe compared the different models using the likelihood calculated during fitting as model evidence. Figure\u00a0 a shows model frequency, model exceedance probability, and protected exceedance probability for each model fitted (see \u2019Model comparison\u2019). Both comparisons showed the winning model was the \u2019Bayesian jump frequency\u2019 model inferring both the frequency of pain states and their volatility, producing predictions significantly better than alternative models (Bayesian jump frequency model frequency\u2009=\u20090.563, exceedance probability\u2009=\u20090.923, protected exceedance\u2009=\u20090.924). Figure\u00a0 b reports the model evidence for each subject; it shows that, although the majority (  n  \u2009=\u200923) of participants were best fit by the model that infers the background frequency, some participants (  n  \u2009=\u200912) were better fit by the more sophisticated model that infers specific transition probabilities. In Supplementary Fig.\u00a0 , we show quality of the fit of the Bayesian jump frequency model for each participant.    Model comparison results.  \n a   Bayesian model comparison based on model fitting evidence. Subjects' predictive ratings of next trial\u2019s pain intensity were fitted with posterior means from Bayesian models, values from Rescorla\u2013Wagner (reinforcement learning) model, and random fixed probabilities. The winning model was the Bayesian jump frequency model, which assumes jumps in the sequence and infers the stimulus frequency. In our model comparison, the model frequency indicates how often a given model is used by participants; the model exceedance probability measures how likely it is that any given model is more frequent than the other models, and the protected exceedance probability is the corrected exceedance probability for observations due to chance.   b   Individual subject model evidence (each row represents a subject; colorbar indicates the model probability ranging from 0 to 1). Source data are provided as a Source Data file. \n  \n\n\n\n### Neuroimaging results \n  \nWe used the winning computational model to generate trial-by-trial regressors for the hemodynamic responses. The rationale behind this approach is that neural correlation of core computational components of a specific model provides evidence that and how the model is implemented in the brain . \n\nFirst, a simple high\u2009>\u2009low pain contrast identified BOLD responses in the right thalamus, sensorimotor, premotor, supplementary motor, insula, anterior cingulate cortices and left cerebellum (with peaks in laminae V\u2013VI), consistent with the known neuroanatomy of pain responses (Fig.\u00a0 , cluster list in Supplementary Table\u00a0 ).    Brain responses to noxious stimuli.  \nRed: high\u2009>\u2009low pain stimuli, blue: high\u2009<\u2009low pain stimuli. Two-sided statistics corrected for the false discovery rate (FDR) at level   p  \u2009<\u20090.001; colorbar shows Z scores\u2009>\u20093.3. \n  \n\nNext, we evaluated the neural correlates of the modelled posterior probability of high pain. For any stimulus, this reflects the newly calculated probability that the next stimulus will be high, i.e. the dynamic and probabilistic inference of high pain. Given that the predicted probabilities of high and low pain are reciprocal, their neural correlates can be revealed by using positive and negative contrasts. The prediction of high pain frequency was associated with BOLD responses in the bilateral primary and secondary somatosensory cortex, primary motor cortex, caudate and putamen (pink clusters in Fig.\u00a0 , Table\u00a0 ). The prediction of low pain frequency implicated the right (controlateral to stimulation) sensorimotor cortex, the supplementary motor cortex, dorsal anterior cingulate cortex, thalamus and posterior insular-opercular cortex bilaterally (green clusters in Fig.\u00a0 , Table\u00a0 ).    Brain activity associated with the temporal statistical inference of pain intensity.  \nNeural correlates of the mean posterior probability of low pain (green) and high pain (pink) in the Bayesian jump frequency model (two-sided statistics, FDR corrected   p  \u2009<\u20090.001, colorbar shows Z scores\u2009>\u20093.3). \n    \nActivation clusters associated with the mean posterior p(High pain) and p(Low pain) of the Bayesian jump frequency model \n  \n\nUncertainty signals, quantified by the variability (SD) of the posterior probability distribution of high pain, were found in a right superior parietal region, bordering with the supramarginal gyrus (Fig.\u00a0 a and Table\u00a0 ). The negative contrast of the posterior SD did not yield any significant cluster.    Uncertainty and learning signals.  \n a   Uncertainty (SD) of the posterior probability of high pain in the Bayesian jump frequency model was associated with activations in the right superior parietal cortex (FDR corrected   p  \u2009<\u20090.001, colorbar shows Z scores\u2009>\u20093.3).   b   Neural activity associated with the model update, i.e. the Kullback\u2013Leibler (KL) divergence between posteriors from successive trials (positive contrast, FDR corrected   p  \u2009<\u20090.001, colorbar shows Z scores\u2009>\u20093.3). \n    \nActivation clusters positively associated with the uncertainty (SD posterior) and update (KL divergence) of the Bayesian jump frequency model \n  \n\nA key aspect of the Bayesian model is that it provides a metric of the model update, quantified as the Kullback\u2013Leibler (KL) divergence between successive trials\u2019 posterior distribution. The KL divergence increases when the two successive posteriors are more different from each other, and decreases when the posteriors are similar. We found that the KL divergence was associated with BOLD responses in left premotor cortex, bilateral dorsolateral prefrontal cortex, superior parietal lobe, supramarginal gyrus, and left somatosensory cortex (Fig.\u00a0 b, Table\u00a0 ). For completeness, we report the negative contrast in Supplementary Fig.\u00a0  and Supplementary Table\u00a0 . Figure\u00a0  overlays the posterior probability of pain with its uncertainty and update (KL divergence). This shows that the temporal prediction of high pain and its update activate distinct, although neighbouring regions in the sensorimotor and premotor cortex, bilaterally. In contrast, the uncertainty of pain predictions activates a right superior parietal region that partially overlaps with the neural correlates of model update.    Statistical inference and model update activate adjacent sensorimotor and premotor regions.  \nOverlay of the temporal prediction (mean posterior probability) of low (green) and high pain (pink), their uncertainty (SD posterior probability, blue) and the model update (KL divergence between successive posterior distributions, red-yellow); FDR corrected   p  \u2009<\u20090.001, colorbar shows Z scores\u2009>\u20093.3. \n  \n\n\n\n## Discussion \n  \nPain is typically uncertain, and this is most often true when pain persists after an injury. When this happens, the brain needs to be able to track changes in intensity and patterns over time, in order to predict what will happen next and decide what to do about it. Here we show that the human brain can generate explicit predictions about the likelihood of forthcoming pain, in absence of external cues. Pain predictions are best described by an optimal Bayesian inference model with dynamic update of beliefs, allowing explicit prediction of the probability of forthcoming pain at any moment in time. Using neuroimaging, we found distinct neural correlates for the probabilistic, predictive inference of pain and its update. Predictions (i.e. mean posterior probability) of high pain intensity are encoded in the bilateral, primary somatosensory and motor regions, secondary somatosensory cortex, caudate and putamen, whereas predictions of low pain intensity involve the controlateral sensorimotor cortex, the supplementary motor cortex, dorsal anterior cingulate cortex, bilateral thalamus and posterior insular-opercular cortex. The signal representing the update of the probabilistic model localises in adjacent premotor and superior parietal cortex. The superior parietal cortex is also implicated in the computation of the uncertainty of the probabilistic inference of pain. Overall, the results show that cortical regions typically associated with the sensory processing of pain (primary and secondary somatosensory cortices) encode how likely different pain intensities are to occur at any moment in time, in the absence of any other cues or information; the uncertainty of this inference is encoded in the superior parietal cortex and used by a network of parietal-prefrontal regions to update the temporal statistical representation of pain intensity. \n\nThe ability of the brain to extract regularities from temporal sequences is well-documented in other sensory domains such as vision and audition . However, pain is a fundamentally different system with intrinsic motivational value and direct impact on the state of the body . Pain can constrain cognitive functions, such as working memory and attention . Furthermore, the cortical representation of pain is distributed  and a primary brain region for pain has not been found . \n\nWe show that pain predictions are best described by an optimal Bayesian inference model, tracking the frequency of pain states and their volatility based on past experience. A more complex strategy involves inferring higher level statistical patterns within these sequences, i.e. representing all the transition probabilities between different states. It has been shown that optimal inference of transition probabilities can be achieved using similar paradigms with visual and auditory stimuli . Although this model fits 1/3 of subjects best, overall it was not favoured over the simpler frequency learning model, which best describes the behaviour of ~2/3 of our sample (Fig.\u00a0 ). At this stage it is not clear whether this is because of stable inter-individual differences, or whether given more time, more participants would be able to learn specific transition probabilities. However, it is worth noting that stable, individual differences in learning strategy have been previously reported in visual statistical learning . In supplementary analyses, we show that the neural correlates of both frequency and transition probability learning were generally comparable between the subgroups of participants who favoured a frequency inference strategy and those who preferred a transition probability strategy (Supplementary Notes\u00a0 \u2013 ). \n\nAs in other domains, we focused on conscious judgement of the relative overall probability of pain, as opposed to looking at autonomic responses or other physiological measures of pain prediction; this was done to allow direct comparisons between the participant\u2019s predictions and ideal observers . We did not test whether statistical learning for pain is automatic but, based on previous work, we expect it to happen spontaneously, without requiring the need to explicitly report stimulus probabilities. Indeed, statistical learning of visual or auditory inputs has been reported in songbirds, primates and newborns . Furthermore, previous studies have shown implicit expectation effects from sequences of stimuli in humans . \n\nThe present evidence in support of Bayesian inference is broadly consistent with previous work on the learning of a cognitive model or acquisition of explicit contingency knowledge across modalities, including pain . This reflects a fundamentally different process to pain response learning\u2014either in Pavlovian conditioning where simple autonomic, physiological or motoric responses are acquired, or basic stimulus-response (instrumental / operant) avoidance or escape response learning. These behaviours are usually best captured by reinforcement learning models such as temporal difference learning , and reflect a computationally different process . Having said that, such error-driven learning models have been applied to statistical learning paradigms in other domains before , and so here we were able to directly demonstrate that they\u00a0provided a worse fit\u00a0than Bayesian inference models (Fig.\u00a0 ). In contrast to simple reinforcement learning models, Bayesian models allow to build an internal, hierarchical representation of the temporal statistics of the environment that can support a range of cognitive functions . \n\nA key benefit of the computational approach is that it allows us to accurately map underlying operations of pain information processing to their neural substrates . Our study shows that the probabilistic inference of pain frequency is encoded in somatosensory processing regions, such as the primary somatosensory cortex and posterior insula/operculum; it also involves supramodal regions, such as premotor cortex and dorsal striatum (Fig.\u00a0 ). This is broadly consistent with the view that statistical learning involves both sensory and supramodal regions . \n\nA specific facet of the Bayesian model is the representation of an uncertainty signal, i.e. the posterior SD, and a model update signal, defined as the statistical KL divergence between consecutive posterior distributions. This captures the extent to which a model is updated when an incoming pain signal deviates from that expected, taking into account the uncertainty inherent in the original prediction. In our task, the uncertainty of the prediction was encoded in a right superior parietal region, which partially overlapped with a wider parietal region associated with the encoding of the model update (Figs.\u00a0 ,  ). This emphasises the close relationship between uncertainty and learning in Bayesian inference . A previous study on statistical learning in other sensory domains reported that a more posterior, intraparietal region was associated with the precision of the temporal inference . The role of the superior parietal cortex in uncertainty representation is also evident in other memory-based decision-making tasks; e.g. the superior parietal cortex was found to be more active for low vs. high confidence judgements . In addition to the parietal cortex, the model update signal was encoded in the left premotor cortex and bilateral dorsolateral prefrontal cortex (Fig.\u00a0 ), which neighboured regions activated by pain predictions (Fig.\u00a0 ). This is particularly interesting, as the premotor cortex sits along a hierarchy of reciprocally and highly interconnected regions within the sensorimotor cortex. The premotor cortex has also been implicated in the computation of an update signal in visual and auditory statistical learning tasks . \n\nIn conclusion, our study demonstrates that the nociceptive system generates probabilistic predictions about the background temporal statistics of pain states, in absence of external cues, and this is best described by a Bayesian inference strategy. This extends both current anatomical and functional concepts of what is conventionally considered a \u2019sensory pain pathway\u2019, to include the encoding not just of stimulus intensity  and location , but also the generation of dynamic internal models of the temporal statistics of pain intensity levels. Future studies will need to determine whether temporal statistical predictions modulate pain perception, similarly to other kinds of pain expectations . More broadly, temporal statistical learning is likely to be most important after injury, when continuous streams of fluctuating signals ascend nociceptive afferents to the brain, and their underlying pattern may hold important clues as to the nature of the injury, its future evolution, and its broader semantic meaning in terms of the survival and prospects of the individual. It is therefore possible that the underlying computational process might go awry in certain instances of chronic pain, especially when instrumental actions can be performed that might influence the pattern of pain intensity . Thus, future studies could explore both how temporal statistical learning interacts with pain perception and controllability, as well as its application to clinical pain. \n\n\n## Methods \n  \n### Participants \n  \nThirty-five healthy participants (17 females; mean age 27.4 years old; age range 18\u201345 years) took part in two experimental sessions, 2\u20133 days apart: a pain-tuning and training session and an MRI session. Each participant gave informed consent according to procedures approved by University of Cambridge ethics committee (PRE.2018.046). \n\n\n### Protocol \n  \nThe electrical stimuli were generated using a DS5 isolated bipolar current stimulator (Digitimer), delivered to surface electrodes placed on the index and middle fingers of the left hand. All participants underwent a standardised intensity work-up procedure at the start of each testing day, in order to match subjective pain levels across sessions to a low-intensity level (just above pain detection threshold) and a high-intensity level that was reported to be painful but bearable (>4 out of 10 on a VAS ranging from 0 [\u2018no pain\u2019] to 10 [\u2018worst imaginable pain\u2019]). The stimulus delivery setup was identical for lab-based and MR sessions. After identifying appropriate intensity levels, we checked that discrimination accuracy was >95% in a short sequence of 20 randomised stimuli. This was done to ensure that uncertainty in the sequence task would derive from the temporal order of the stimuli rather than their current intensity level or discriminability. If needed, we tweaked the stimulus intensities to achieve our target discriminability. Next, we gave the task instructions to each participants (openly available ). \n\nAfter receiving a shock on trial   t  , subjects were asked to predict the probability of receiving a stimulus of the same or different intensity on the upcoming trial (trial   t  \u2009+\u20091). We informed participants that in the task they \"would receive two kinds of stimuli, a low-intensity shock and a high-intensity shock. The L and H stimuli would be presented in a sequence, in an order set by the computer. After each stimulus, the following stimulus intensity could be either the same or change. The computer sets the probability that after a given stimulus (for example L) there would be either L or H\" (we showed a visual representation of this example). We asked participants to \"always try to guess the probability that after each stimulus there will the same or a different one\" and we informed them that \"the computer sometimes changes its settings and sets new probabilities\", so to pay attention all the time. We also told them the sequence would be paused occasionally in order to collect probability estimates from participants using the scale depicted in Fig.\u00a0 . A white fixation cross was displayed on a dark screen throughout the trial, except when a response was requested every 12\u201318 trials. The interstimulus interval was 2.8\u20133 seconds. There were 300 stimuli in each block, lasting ~8\u2009min. Average intensity ratings for each stimulus level were collected after each block during a short break. Low-intensity stimuli were felt by participants as barely painful, rated on average 1.39 (SD 0.77) on a scale ranging from 0 (no pain) to 10 (worst pain imaginable). In contrast, high-intensity stimuli were rated as more than 4 times higher than low-intensity stimuli (mean 5.74, SD 4.85). Participants were given 4 blocks of practice, 2\u20133 days prior the imaging sessions and 5 blocks (1500 stimuli) during task fMRI. \n\nA unique sequence was generated every time the experiment was launched as in ref.  . L and H stimuli were drawn randomly from a 2\u2009\u00d7\u20092 transition probability matrix, which remained constant for a number of trials (chunks). The probability of a change was 0.014. Chunks had to be >5 and <200 trials long. In each chunk, transition probabilities were sampled independently and uniformly in the 0.15\u20130.85 range (in steps of 0.05), with the constraint that at least one of the two transition probabilities must be >/< 0.2 than in the previous chunk. Participants were not informed when the matrix was resampled, and a new chunk started. \n\nAt the end of the task we asked participants for general comments about the strategy they used. A minority of participants reported to have no clue about the stimulus sequence and were rather bored by the task. The majority of subjects thought that it was difficult to predict the sequence but they were starting to get a sense of the temporal pattern after a while. Some tried to use strategies like counting, but they tended to abandon it to favour a more spontaneous approach of feeling the \"flow\" or the \"rhythm\" of the sequence. \n\nBehavioural data analysis were conducted with Python packages pandas (pypi version 1.1.3) and scipy (pypi version 1.5.3). Effect size was calculated as Cohen\u2019s d for   t  -tests. \n\n\n### Computational modelling of temporal statistical learning \n  \n#### Learning models \n  \nThe models used in comparison are listed as followed: \n\n Random (baseline model)  \n\nProbabilities are assumed fixed and reciprocal for high and low stimuli where   p   was fitted as a free parameter. Uncertainty was assumed to be fixed. \n\n Rescorla\u2013Wagner (RW model)  \n\nRated probabilities are assumed to be state values, which were updated as where   R  \u2009=\u20091 if stimulus was low, and 0 otherwise. The learning rate   \u03b1   was fitted as free parameter . \n\n Bayesian models  \n\nBayesian models update each trial with stimulus identity information to obtain upcoming trial probability from the posterior distribution . Using Bayes\u2019 rule, the model parameter   \u03b8   is estimated at each trial   t   provided previous observations   y   (sequence of high or low pain), given a model   M  . \n\nStimulus information can either be the frequency or transition of the binary sequence. There are \u2018fixed\u2019 models that assume no sudden jump in stimuli probabilities, and \u2018jump\u2019 models that assume the opposite. The four combinations were fitted and compared. \n\n Fixed frequency model  \n\nFor fixed models, the likelihood of parameters   \u03b8   follows a Beta distribution with parameters   N  \u2009+\u20091 and   N  \u2009+\u20091, where   N   and   N   are the numbers of high and low pain in the sequence   y  . Given that the prior is also a flat Beta distribution with parameters [1,1], the posterior can be analytically obtained with: \n\nThe likelihood of a sequence   y   given model parameters   \u03b8   can be calculated as: \n\nFinally, the posterior probability of a stimulus occurring in the next trial can be estimated with Bayes\u2019 rule: \n\nPriors \"window\" and \"decay\" were fitted as free parameters. \"Window\" is the previous   n   trials where the frequency of stimuli was estimated, and \"decay\" is the previous   n   trials where the frequency of stimuli further from current trial was discounted following an exponential decay. \n\nWhen \"window\"   w   is applied, then   N   and   N   are counted within the window of   w   trials   y  . When \"decay\"   d   is applied, an exponential decay factor   is applied to the   k   trials before their sum is calculated. Both \"window\" and \"decay\" were used simultaneously. \n\n Fixed transition model  \n\nPriors \"window\" and \"decay\" were fitted as free parameters as in the Fixed frequency model above, however, the transition probability was estimated instead of the frequency. The likelihood of a stimulus now depends on the estimated transition probability vector   \u03b8  \u2009~\u2009[  \u03b8  ,\u2009  \u03b8  ] and the previous stimulus pairs   N  \u2009~\u2009[  N  ,\u2009  N  ]. Given that both likelihood and prior can be represented using Beta distributions as before, the posterior result can be analytically obtained as: \n\n Jump frequency model  \n\nIn jump models, parameter   \u03b8   is no longer fixed, instead it can change from one trial to another with a probability of   p  . Prior   p   was fitted as a free parameter, representing the subject\u2019s assumed probability of a jump occurring during the sequence of stimuli (e.g. a high   p   assumes the sequence can reverse quickly from a low pain majority to a high pain majority). The model can be approximated as a Hidden Markov Model (HMM) in order to compute the joint distribution of   \u03b8   and observed stimuli iteratively, where the integral term captures the change in   \u03b8   from one observation   t   to the next   t  \u2009+\u20091, with probability (1\u2009\u2212\u2009  p  ) of staying the same and probability   p   of changing. This integral can be calculated numerically within a discretised grid. The posterior probability of a stimulus occurring in the next trial can then be calculated using Bayes\u2019 rule as \n\n Jump transition model  \n\nSimilarly to the jump frequency model above, prior   p   was fitted as a free parameter, but estimating transition probabilities instead of frequency. The difference is the stimulus at trial   y   now dependent on the stimulus at the previous trial, hence the addition of the term   y   in the joint distribution term, shown below. \n\n\n#### KL divergence \n  \nKullback\u2013Leibler (KL) divergence quantifies the distance between two probability distributions. In the current context, it measures the difference between the posterior probability distributions of successive trials. It is calculated as where   P   and   Q   represent the two discrete posterior probability distributions calculated in discretised grids  . KL divergence can be used to represent information gains when updating after successive trials . \n\n\n#### Subject rated probability \n  \nFor each individual subject, model-predicted probabilities   p   from the trial   k   were used as predictors in the regression: where   y   is the subject rated probabilities,   M   is the   i  th candidate model,   N   is the session number within subject,   \u03b2  ,   \u03b2  ,   \u03b2   and   \u03b8   are free parameters to be fitted and   \u03f5   is normally distributed noise added to avoid fitting errors . \n\n\n#### Model fitting \n  \nTo estimate the model-free parameters from data, Bayesian information criteria (BIC) values were calculated as: where   is the squared residual from the linear model above that relates subject ratings to model-predicted probabilities, and   n   is the number of free parameters fitted. \n\nWe use   fmincon   in MATLAB to minimise the BIC (as approximate for negative log likelihood ) for each subject/model. The procedure was repeated 100 times with different parameter initialisation, and the mean results of these repetitions were taken as the fitted parameters and minimised log likelihoods. Model\u2019s parameter recovery is reported in Supplementary Note\u00a0 . \n\n\n#### Model comparison \n  \nIn general, the best fit model was defined as the candidate model with the lowest averaged BIC. We conducted a random effect analysis with the\u00a0VBA toolbox , where fitted log likelihoods from each subject/model pair were used as model evidence. With this approach, model evidence was treated as random effects that could differ between individuals. This comparison produces model frequency (how often a given model is used by individuals), model exceedance probability (how likely it is that any given model is more frequent than all other models in the comparison set), and protected exceedance probability (corrected exceedance probability for observations due to chance) . These values are correlated and would be considered together when selecting the best fit model. \n\n\n\n### Neuroimaging data \n  \n#### Data acquisition \n  \nFirst, we collected a T1-weighted MPRAGE structural scan (voxel size 1\u2009mm isotropic) on a 3T Siemens Magnetom Skyra (Siemens Healthcare), equipped with a 32-channel head coil (Wolfson Brain Imaging Centre, Cambridge). Then we collected 5 task fMRI sessions of 246 volumes using a gradient echo planar imaging (EPI) sequence (TR\u2009=\u20092000\u2009ms, TE\u2009=\u200923\u2009ms, flip angle\u2009=\u200978\u00b0, slices per volume\u2009=\u200931, Grappa 2, voxel size 2.4 mm isotropic, A\u2009>\u2009P phase-encoding; this included four dummy volumes, in addition to those pre-discarded by the scanner). In order to correct for inhomogeneities in the static magnetic field, we imaged 4 volumes using an EPI sequence identical to that used in task fMRI, inverted in the posterior-to-anterior phase-encoding direction. Full sequence metadata are available . \n\n\n#### Preprocessing \n  \nImaging data were preprocessed using fmriprep (pypi version: 20.1.1, RRID:SCR_016216) with Freesurfer option disabled, within its Docker container. Processed functional images had first four dummy scans removed, and then smoothed with\u00a0an 8 mm Gaussian filter in SPM12. \n\n\n#### Generalised linear model analysis \n  \nNipype (pypi version: 1.5.1) was used for all fMRI processing and analysis within its published Docker container. Nipype is a python package that wraps around fMRI analysis tools including SPM12 and FLS in a Debian environment. \n\nFirst and second level GLM analyses were conducted using SPM12 through nipype. In all first level analyses, 25 regressors of no interest were included from fmriprep confounds output: CSF, white matter, global signal, dvars, std_dvars, framewise displacement, rmsd, 6 a_comp_cor with corresponding cosine components, translation in 3 axis and rotation in 3 axis. Sessions within subject are not concatenated. \n\nIn second level analyses, all first level contrasts were entered into a one-sample   t  -test, with group subject mask applied. The default FDR threshold used was 0.001 (set in Nipype threshold node height_threshold\u2009=\u20090.001). \n\nFor visualisation and cluster statistics extraction, nilearn (pypi version: 1.6.1) was used. A cluster extent of 10 voxels was applied. Visualised slice coordinates were chosen based on identified\u00a0cluster peaks. Activation clusters were overlayed on top of a subject averaged anatomical scan normalised to MNI152 space as output by fmriprep. \n\n\n#### GLM design \n  \nAll imaging results were obtained from a single GLM model. We investigated neural correlates using the winning Bayesian jump frequency model. All model predictors were generated with the group mean fitted parameters in order to minimise noise. First level regressors include the onset times for all trials, high pain trials and low pain trials (duration\u2009=\u20090). The all trial regressor was parametrically modulated by model-predicted posterior mean of high pain, the KL divergence between successive posterior distributions on jump probability, and the posterior SD of high pain. \n\nFor second level analysis, both positive and negative T-contrasts were obtained for posterior mean, KL divergence and uncertainty parametric modulators, across all the first level contrast images from all subjects. A group mean brain mask was applied to exclude activations outside the brain. Given that high and low pain are reciprocal in probabilities, a negative contrast of posterior mean of low pain would be equivalent to the posterior mean of high pain. In addition, high and low pain comparisons were done using a subtracting T-contrast between high and low pain trial regressors. We corrected for multiple comparisons with a cluster-wise FDR threshold of   p  \u2009<\u20090.001 for both parametric modulator analyses, reporting only clusters that survived this correction. \n\n\n\n### Reporting summary \n  \nFurther information on research design is available in the\u00a0  linked to this article. \n\n\n\n## Supplementary information \n  \n\n\n\n\n \n", "metadata": {"pmcid": 9633765, "text_md5": "76ace179532f9abcf383d277ac9d85e4", "field_positions": {"authors": [0, 48], "journal": [49, 59], "publication_year": [61, 65], "title": [76, 140], "keywords": [154, 179], "abstract": [192, 1496], "body": [1505, 44872]}, "batch": 1, "pmid": 36329014, "doi": "10.1038/s41467-022-34283-9", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9633765", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9633765"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9633765\">9633765</a>", "list_title": "PMC9633765  Computational and neural mechanisms of statistical pain learning"}
