{"text": "Ahrens, Merle-Marie and Awwad Shiekh Hasan, Bashar and Giordano, Bruno L. and Belin, Pascal\nFront Neurosci, 2014\n\n# Title\n\nGender differences in the temporal voice areas\n\n# Keywords\n\ngender difference\nfMRI\nvoice localizer\ntemporal voice areas\nmultivariate pattern analysis (MVPA)\nvoice perception\n\n\n# Abstract\n \nThere is not only evidence for behavioral differences in voice perception between female and male listeners, but also recent suggestions for differences in neural correlates between genders. The fMRI functional voice localizer (comprising a univariate analysis contrasting stimulation with vocal vs. non-vocal sounds) is known to give robust estimates of the temporal voice areas (TVAs). However, there is growing interest in employing multivariate analysis approaches to fMRI data (e.g., multivariate pattern analysis; MVPA). The aim of the current study was to localize voice-related areas in both female and male listeners and to investigate whether brain maps may differ depending on the gender of the listener. After a univariate analysis, a random effects analysis was performed on female (  n   = 149) and male (  n   = 123) listeners and contrasts between them were computed. In addition, MVPA with a whole-brain searchlight approach was implemented and classification maps were entered into a second-level permutation based random effects models using statistical non-parametric mapping (SnPM; Nichols and Holmes,  ). Gender differences were found only in the MVPA. Identified regions were located in the middle part of the middle temporal gyrus (bilateral) and the middle superior temporal gyrus (right hemisphere). Our results suggest differences in classifier performance between genders in response to the voice localizer with higher classification accuracy from local BOLD signal patterns in several temporal-lobe regions in female listeners. \n \n\n# Body\n \n## Introduction \n  \nPrior functional magnetic resonance imaging (fMRI) findings suggest a robust brain response to vocal vs. non-vocal sounds in many regions of the human auditory cortex in particular in the superior temporal gyrus (STG). Vocal sounds, including but not restricted to speech sounds, evoke a greater response than non-vocal sounds with bilateral activation foci located near the anterior part of the STG extending to anterior parts of the superior temporal sulcus (STS) and posterior foci located in the middle STS (Binder et al.,  ; Belin et al.,  ,  ). Using the functional voice localizer, these findings were replicated and used in various studies (Belin et al.,  ,  ; Kreifelts et al.,  ; Latinus et al.,  ; Ethofer et al.,  ). The conventional way of identifying voice sensitive regions is by applying univariate statistics, implemented using a Generalized-Linear Model (GLM), to fMRI data assuming independence among voxels. \n\nInterest has recently grown in applying multivariate approaches (e.g., Multivariate pattern analysis; MVPA). Instead of modeling individual voxels independently (univariate analysis), MVPA considers the information of distributed pattern in several voxels (e.g., Norman et al.,  ; Mur et al.,  ). Several studies used multivariate approaches to decode information reflected in brain activity patterns related to specific experimental conditions (Cox and Savoy,  ; Haynes and Rees,  ,  ; Kotz et al.,  ). MVPA is usually applied on unsmoothed data preserving high spatial frequency information. Thus, MVPA is argued to be more sensitive in detecting different cognitive states. In contrast, the conventional univariate analysis averages across voxels, thereby removing focally distributed effects (spatial smoothing). The smoothing across voxels may lead to a reduction in the information content (Kriegeskorte et al.,  ; Norman et al.,  ; Haynes et al.,  ). At present, a multivariate approach has never been employed to investigate whether it may yield a different pattern of voice-specific (voice/non-voice classification) brain regions compared to the univariate analysis. \n\nThe voice contains socially and biologically relevant information and plays a crucial role in human interaction. This information is particularly relevant for interaction between different genders (e.g., regarding emotions, identities, and attractiveness) (Belin et al.,  ,  ). Overall, research suggests that women are more sensitive than men in emotion recognition from faces and voices (Hall,  ; Hall et al.,  ; Schirmer and Kotz,  ). Women perform better in judging others' non-verbal behavior (Hall,  ) and seem to process nonverbal emotional information more automatically as compared to men (Schirmer et al.,  ). In addition, women but not men show greater limbic activity when processing emotional facial expressions (Hall et al.,  ). The exact neural mechanisms underlying voice processing in both female and male listeners still remains under debate. For instance, a study by Lattner et al. ( ) found no significant difference between the activation patterns of female and male listeners in response to voice-related information. However, there is evidence from both behavioral and neural activation studies for differences in voice perception between listeners' gender (Shaywitz et al.,  ; Schirmer et al.,  ,  ,  ; Junger et al.,  ; Skuk and Schweinberger,  ). \n\nA recent behavioral study by Skuk and Schweinberger ( ) investigated gender differences in a familiar voice identification task. They found an own-gender bias for males but not for females while females outperformed males overall. These behavioral differences (Skuk and Schweinberger,  ) may also be reflected by differences in neural activity. Previous fMRI studies investigating potential neural correlates suggested a sex difference in the functional organization of the brain for phonological processing (Shaywitz et al.,  ), in emotional prosodic and semantic processing (Schirmer et al.,  ,  ) and in response to gender-specific voice perception (Junger et al.,  ). Further evidence suggests differences between genders in vocal processing shown by an EEG study, where the processing of vocal sounds with more emotional and/or social information was more sensitive in women as compared to men (Schirmer and Kotz,  ; Schirmer et al.,  ). The above-mentioned studies mainly focus on gender differences in emotional speech processing or opposite-sex perception. However, identified brain regions are not consistent: different experimental designs and applied methods vary and make it difficult to compare between these studies (Shaywitz et al.,  ; Schirmer et al.,  ,  ,  ; Junger et al.,  ). \n\nThe current study employs a well-established experimental design of the functional \u201cvoice localizer,\u201d known to give robust estimates of the TVAs across the majority of participants. The voice localizer includes a variety of different vocal sounds, not exclusively female or male voices, but also speech and non-speech of women, men and infants and non-vocal sounds (e.g., environmental sounds). In this study, we were interested in the effect of gender on the results of the voice localizer and we asked an explorative research question of whether brain activation and/or classification accuracy maps in response to vocal (speech and non-speech) and non-vocal sounds differ between female and male listeners without prior assumptions about the strength of voice-specific activity. \n\nThe voice localizer paradigm is often used in the literature (Belin et al.,  ,  ; Kreifelts et al.,  ; Latinus et al.,  ; Ethofer et al.,  ), which makes it easier to compare among studies as well as among participants or groups. Instead of using the conventional univariate method, employing MVPA may offer a more sensitive approach in order to study potential differences between genders by means of above chance vocal/non-vocal classification accuracies in different regions of the brain. Therefore, we investigated our research question by implementing the conventional univariate analysis using GLM and MVPA based on a support-vector machine (SVM) classifier with a spherical searchlight approach. This approach enabled us to explore cortical activity over the whole-brain and to examine whether activation and/or classification maps in response to the voice localizer may significantly differ between genders. Since the effect size between genders is expected to be very small, the current study offers a substantially large sample size with   n   = 149 females and   n   = 123 males. Thus, this study provides a large sample size, a well-established experimental design and the direct comparison of two different fMRI data analysis approaches applied on the exact same data. \n\n\n## Methods \n  \n### Participants \n  \nfMRI data of 272 healthy participants, 149 female (age range: 18\u201368 years; mean \u00b1   SD   = 24.5 \u00b1 8.0) and 123 male (age range: 18\u201361 years; mean \u00b1   SD   = 24.4 \u00b1 6.5) with self-reported normal audition were analyzed. This study was conducted at the Institute of Neuroscience and Psychology (INP) in Glasgow and approved by the ethics committee of the University of Glasgow. Volunteers provided written informed consent before participating and were paid afterwards. \n\n\n### Voice localizer paradigm \n  \nSubjects were instructed to close their eyes and passively listen to a large variety of sounds. Stimuli were presented in a simple block design and divided into vocal (20 blocks) and non-vocal (20 blocks) conditions. Vocal blocks contained only sounds of human vocal origin (excluding sounds without vocal fold vibration such as whistling or whispering) and consisted of speech (e.g., words, syllables, connected speech in different languages) or non-speech (e.g., coughs, laughs, sighs and cries). The vocal stimuli consisted of recordings from 7 babies, 12 adults, 23 children, and 5 elderly people. Half of the vocal sounds (speech and non-speech) consisted of vocalizations from adults and elderly people (women and men) with comparable proportions for both genders (~24% female, ~22% male). The other half of the vocal sounds consisted of infant vocalizations (speech and non-speech) which also included baby crying/laughing. Recorded non-vocal sounds included various environmental sounds (e.g., animal vocalizations, musical instruments, nature and industrial sounds). A total number of 40 blocks were presented. Each block lasted for 8 s with an inter-block interval of 2 s. Stimuli (16bit, mono, 22050 Hz sampling rate) were normalized for RMS and are available at   (Belin et al.,  ). \n\n\n### MRI data acquisition \n  \nScanning was carried out in a 3T MR scanner (Magnetom Trio Siemens, Erlangen, Germany) and all data were acquired with the same scanner at the INP in Glasgow. Functional MRI volumes of the whole cortex were acquired using an echo-planar gradient pulse sequence (voxel size = 3 mm \u00d7 3 mm \u00d7 3 mm; Time of Repetition (TR) = 2000 ms; Echo Time (TE) = 30 ms; slice thickness = 3 mm; inter-slice gap = 0.3 mm; field of view (FoV) = 210 mm; matrix size = 70 \u00d7 70; excitation angle = 77\u00b0). A total number of 310 volumes (32 slices per volume, interleaved acquisition order) were collected with a total acquisition time of 10.28 min. Anatomical MRI volumes were acquired using a magnetization-prepared rapid gradient echo sequence (MPRAGE) (voxel size = 1 \u00d7 1 \u00d7 1 mm;   TR   = 1900 ms;   TE   = 2.52 ms; inversion time (TI) = 900 ms; slice thickness = 1 mm; FoV = 256 mm; matrix size = 256 \u00d7 265; excitation angle = 9\u00b0; 192 axial slices). \n\n\n### fMRI data analysis \n  \n#### Pre-processing \n  \nPre-processing was performed using the statistical parametric mapping software SPM8 (Department of Cognitive Neurology, London, UK.  ). After reorientation of functional and anatomical volumes to the AC/PC line (anterior- and posterior commissure), functional images were motion corrected (standard realignment). Since, subjects may have moved between anatomical and functional data acquisition, the anatomical volumes were co-registered to the mean functional image produced in the realignment above. Anatomical volumes were segmented in order to generate a binary gray matter template at threshold probability level of 0.5 for each individual participant. This template was applied during model specification in both univariate analysis und MVPA. For the univariate processing, realigned functional volumes were normalized to a standard MNI template (Montreal Neurological Institute) and spatially smoothed with a 6 mm full-width at half mean (FWHM) Gaussian Kernel. \n\n\n#### Univariate analysis \n  \nThe design matrix was defined such that each block of the experimental paradigm correlated to one condition, yielding a design matrix with 20 onsets for each condition (vocal and non-vocal). Analysis was based on the conventional general linear model (GLM) and stimuli were convolved with a boxcar hemodynamic response function provided by SPM8. Contrast images of vocal vs. non-vocal conditions were generated for each individual subject and entered into a second-level random effects analysis (RFX). To declare at the group-level whether any difference between the two conditions was significantly larger than zero, a one-sample   t  -test was applied and FWE-corrected (  p   < 0.05) brain maps were calculated. To investigate whether brain activity significantly differs between genders in response to vocal vs. non-vocal sounds, contrasts between females vs. males (male > female, female > male) were computed in a second level RFX analysis (two-sample   t  -test;   p   < 0.05 FWE-corrected). This analysis was restricted to voxels with classification accuracy significantly above theoretical chance (  p   < 0.01 uncorrected) in both females and males (see MVPA below and yellow area in   Figure 2  ). \n\n\n#### Multivariate pattern analysis \n  \nMultivariate pattern classification was performed on unsmoothed and non-normalized data using Matlab (Mathworks Inc., Natick, USA) and in-house utility scripts (INP, Voice Neurocognition Laboratory; Dr. Bashar Awwad Shiekh Hasan and Dr. Bruno L. Giordano), where the default linear support vector machine (SVM) classifier was applied. The classifier was trained and separately tested following a leave-one out cross validation strategy applied on the 40 beta parameter estimates obtained from the univariate analysis (GLM). \n\nA whole-brain searchlight decoding analysis was implemented using a sphere with a radius of 6 mm (average number of voxels in one sphere: 20.6 \u00b1 1.0   SD  ) (Kriegeskorte et al.,  ). A sphere was only considered for analysis if a minimum of 50% of its voxels were within the gray matter. The data of the voxels within a sphere were classified and the classification accuracy was stored at the central voxel, yielding a 3D brain map of classification accuracy (percentage of correct classifications) (Kriegeskorte et al.,  ; Haynes et al.,  ). To identify brain regions in which classification accuracy was significantly above chance by females and males, the theoretical chance level (50%) was subtracted, then normalized (to the MNI template) and smoothed (6 mm FWHM Gaussian Kernel). To make inference on female and male participants, classification brain maps were entered into a second-level permutation based analysis using statistical nonparametric mapping (SnPM; Statistical NonParametric Mapping; available at  ) with 10,000 permutations (see Holmes et al.,  ; Nichols and Holmes,  ). This was computed separately by gender and the resulting voxels were assessed for significance at 5% level and FWE-corrected, as determined by permutation distribution. Similarly, to assess whether classification brain maps significantly differ between genders in response to vocal/non-vocal sounds, this permutation approach was implemented between groups (female > male, male > female) with 10,000 permutations and the resulting voxels were assessed for significance at 5% level and FWE-corrected, as determined by permutation distribution (see Holmes et al.,  ; Nichols and Holmes,  ). \n\nThe between-group analysis was restricted to a mask defined by voxels with classification accuracy significantly above theoretical chance (  p   < 0.01 uncorrected) in both females and males. The resulting mask included 3783 voxels (yellow area in   Figure 2  ). The same mask was applied for both, the univariate analysis and MVPA. \n\nSeparate brain maps of vocal vs. non-vocal contrast in female and male participants as well as brain maps of contrasts between genders for both, univariate analysis and MVPA were generated using the program MRIcoGL (available at  ). \n\n\n\n\n## Results \n  \n### Univariate analysis: vocal vs. non-vocal sounds \n  \nThe univariate analysis comparing activation to vocal and non-vocal sounds showed extended areas of greater response to vocal sounds in the typical regions of the temporal voice areas (TVA), highly similar for male and female subjects (Figure  ). These regions were located bilaterally in the temporal lobes extending from posterior parts of the STS along the STG to anterior parts of the STS and also including several parts of the superior and middle temporal gyrus (STG, MTG). \n  \n Brain maps of female (red,   n   = 149) and male (blue,   n   = 123) participants  .   (A)   Univariate analysis showing bilateral activation along the superior temporal sulcus (STS) and in the inferior frontal gyrus (IFG) and corresponding contrast estimates of vocal vs. non-vocal sounds plotted for peak voxel (one-sample   t  -test, FWE-corrected,   p   < 0.05; cf. circles, note that the two peaks with highest   T  -value and largest cluster size are indicated per group).   (B)   MVPA showing comparable classification accuracy maps along STS, but not IFG and average classification accuracy \u00b1 s.e.m. at peak voxel (calculated in native space) was distinctly above chance level (0.5) for both females and males (maximum intensity projection of   t  -statistic image threshold at FWE-corrected   p   < 0.05, as determined by permutation distribution with 10,000 permutations). \n  \nSeveral hemispheric maxima of vocal vs. non-vocal response were located bilaterally along the STS in both females and males (Figure  , Table  ). Figure   shows parameter estimates of the vocal > non-vocal contrasts at the maxima of the largest cluster sizes with the highest   T  -values of each hemisphere. The brain activation differences between vocal and non-vocal response was consistent across maxima in females (MNI coordinates left:   x   = \u221257,   y   = \u221216,   z   = \u22122, cluster size 3923,   T   = 20.85; right:   x   = 60,   y   = \u221213,   z   = \u22122,   T  -value = 20.64) and in males (MNI coordinates left:   x   = \u221260,   y   = \u221222,   z   = 1, cluster size 796,   T   = 18.19; right:   x   = 60,   y   = \u221210,   z   = \u22122, cluster size 812,   T  -value = 17.46). Female listeners showed one large cluster covering the temporal lobes and subcortical parts of the brain. By contrast male listeners showed two separate voxel clusters in the left and right temporal lobes and no subcortical cluster connecting the two hemispheres (Table  ). Small bilateral clusters were found in inferior prefrontal cortex (inferior frontal gyrus, IFG) in both female and male listeners (  p   < 0.05 FWE-corrected; Figure  ). \n  \n Voice-sensitive peak voxels of female and male RFX analysis (Univariate)  . \n  \nPeak voxel coordinates in standard MNI space and corresponding t-values above for female and male 4.49 (FWE-corrected p < 0.05). \n  \n\n### MVPA analysis: vocal/non-vocal classification \n  \nThe MVPA analysis showed clusters of significantly above-chance voice/non-voice classification accuracy in the TVAs (Figure  , Table  ) (Figure  , Table  ). Hemispheric maxima of classification accuracy were at comparable locations as the peaks of voice > non-voice activation revealed by the univariate method. The classification accuracy within the peak voxel of female listeners (MNI coordinates left:   x   = \u221260,   y   = \u221216,   z   = 1, cluster size 1676,   T  -value = 20.41; right:   x   = 66,   y   = \u221231,   z   = 4, cluster size 1671,   T  -value = 21.45) as well as for male listeners (MNI coordinates left:   x   = \u221260,   y   = \u221222,   z   = 4, cluster size 984,   T  -value = 13.70; right:   x   = 63,   y   = \u221228,   z   = 4, cluster size 1211,   T  -value = 16.07) were distincly above the theoretical chance level of 0.5 (Figure  ). Overall, the maximal classification accuracy was higher in female listeners as compared to male listeners at the peak voxels (Figure  , mean \u00b1 s.e.m.: left peak in females 0.84 \u00b1 0.006, males 0.83 \u00b1 0.009; right peak in females 0.85 \u00b1 0.007, males 0.84 \u00b1 0.009. Left peak in males 0.83 \u00b1 0.009, females 0.85 \u00b1 0.006, right peak in males 0.85 \u00b1 0.009, females 0.87 \u00b1 0.007). Comparing MVPA and univariate analysis in Figures   the MVPA analysis revealed more superficial cortical regions bilateral at the temporal pole, whereas the voxel cluster of the vocal vs. non-vocal difference of the univariate analysis extend more toward the midline of the brain. \n  \n Voice-sensitive peak voxels of female and male group analysis (MVPA)  . \n  \nPeak voxel coordinates in standard MNI space and corresponding t-values above for female 4.38 and male 4.29 (FWE-corrected p < 0.05, as determined by permutation distribution with 10,000 permutations). \n  \n\n### Female vs. male contrasts \n  \nThe contrast of activation maps (univariate analysis) or classification accuracy maps (multivariate approach) from males and females revealed no significant voxels with greater parameter estimates for males > females at the chosen statistical significance threshold (  p   < 0.05, FWE-corrected) for either analysis methods. The reverse contrast (female > male), however, revealed significant voxel clusters showing greater parameter estimates for univariate analysis and higher classification accuracy for MVPA in female participants (Figure  ). \n  \n Contrast between female > male (red)  .   (A)   Univariate analysis showing significant female > male difference (two-sample   t  -test, FWE-corrected,   p   < 0.05) in the left posterior part of the superior temporal gyrus (STG) and the right anterior STG. Contrast estimates at peak voxel showing stronger activation in females (black) as compared to males (gray) in response to vocal vs. non-vocal sounds.   (B)   MVPA showing significant classification accuracy above chance level in the right middle part of the middle temporal gyrus (MTG) and the right middle STG as well as in the left middle MTG with higher average classification accuracy in females (black) than in males (gray) (maximum intensity projection of   t  -statistic image threshold at FWE-corrected   p   < 0.05, as determined by permutation distribution with 10,000 permutations). The (yellow) cluster shows the mask including voxels with significantly above chance classification accuracy in both females and males (  p   < 0.01 uncorrected). \n  \nWhen analyzed with the univariate approach (Figure  ) the contrast female > male yielded only a few significant voxels: One cluster consisted of four voxels in the left posterior part of STG and only one voxels in the right Insula (Figure  , Table  ). The corresponding contrast estimates for the reported peak voxels (MNI coordinates left:   x   = \u221248,   y   = \u221234,   z   = 16, cluster size 4,   T  -value = 4.02; right:   x   = 48,   y   = 2,   z   = \u22125, cluster size = 1,   T  -value = 4.04) showed a positive response for females in both hemispheres and for the left hemisphere in males. The Cohen's d effect size values (  d   = 0.48 and 0.49) suggested a moderate difference at the peak voxel (Table  ). Overall, females showed a stronger activation in response to vocal vs. non-vocal sounds as compared to males at both maxima (Figure  ). \n  \n Peak voxels of female > male contrast for univariate analysis and MVPA  . \n  \nPeak voxel coordinates in standard MNI space and corresponding t-values above 3.85 (univariate analysis, FWE-corrected p < 0.05) and 3.70 for MVPA (FWE-corrected p < 0.05, as determined by permutation distribution with 10,000 permutations) and Cohen's d for large cluster size. The Cohen's d of the MVPA refers to the mean difference in classification accuracy (contrast estimates of the univariate analysis respectively), divided by the pooled standard deviation for those means. \n  \nThe female > male contrast of classification accuracy maps identified significant voxel clusters in the middle part of the middle temporal gyrus (MTG) in both hemispheres, in which classification accuracy was greater for female than male participants (red clusters in Figure  ). Areas of greater classification accuracy in females were more extended in the left hemisphere with an additional smaller cluster located in the STG. The peak voxels of female > male classification accuracy difference were located in the middle part of the MTG (bilateral), and the left middle STG (MNI coordinates left:   x   = \u221269,   y   = \u2212 19,   z   = \u22128, cluster size 84,   T  -value = 5.22;   x   = \u221251,   y   = \u221222,   z   = 13, cluster size 156,   T  -value = 5.19; right:   x   = 69,   y   = \u22127,   z   = \u221211, cluster size 52,   T  -value = 4.48; cf. circle in Figure  ). The Cohen's d effect size values (  d   = 0.35, 0.35, and 0.24) suggested a small difference at the peak voxel (Table  ). Classification accuracy (computed in native space) at these coordinates was distinctly above chance (50%) for both females and males, but higher in females across peaks (Figure  ). \n\n\n\n## Discussion \n  \nThe present study aimed to investigate gender differences on voice localizer scans by employing the conventional univariate analysis as well as MVPA. Both analysis approaches revealed largely overlapping/comparable and robust estimates of the TVAs in female and male listeners. However, the MVPA was more sensitive to differences in the middle MTG of the left and right hemispheres and the middle left STG between genders as compared to univariate analysis with higher classification accuracy in women. \n\n\n## Robust TVAs \n  \nThe estimated TVAs using MVPA robustly replicated and confirmed prior fMRI findings applying the voice localizer (Belin et al.,  ,  ; Belin and Zatorre,  ; Scott and Johnsrude,  ; Von Kriegstein et al.,  ). Both analysis methods showed comparable maps of classification accuracy (MVPA) and of vocal vs. non-vocal activity difference (univariate analysis) for both female and male listeners. The average classification accuracy at the peak voxel was distinctly above chance level and higher in female as compared to male listeners. The peak voxels were at comparable locations (along middle and posterior parts of the STS) for both analysis approaches and both genders. A small difference between the MVPA and univariate analysis can be seen bilateral at the temporal pole, where the MVPA detected more vocal/non-vocal differences in superficial cortical regions as compared to the univariate analysis. In addition to the activation brain maps showing the robustly estimated TVAs (univariate analysis), the MVPA results extend previous findings by providing a corresponding classification accuracy brain map. When brain maps are considered for each analysis approach and for female and male listeners separately, our findings showed no distinct differences between genders and between univariate analysis and MVPA. Instead comparable voxel clusters of a similar size in the bilateral temporal lobes were identified, verifying the prior univariate analysis and the robustness of the TVAs (see e.g., Belin et al.,  ). \n\n\n## Gender differences \n  \nWhen data were analyzed with MVPA, differences between female and male listeners in response to vocal/non-vocal sounds were found by contrasting female > male (but not male > female). A significant difference in success of the MVPA between female and male listeners was apparent in the middle part of the MTG in both hemispheres and in the middle part of the STG in the left hemisphere. Effect sizes showed a small difference at the peak voxels. Despite the large sample size used in this study, the univariate analysis showed no major activation differences between genders. Only two small clusters with one to four voxels were significant in the posterior and anterior part of the STG. In the univariate analysis, the overall activation difference between vocal vs. non-vocal sounds was stronger in female as compared to male listeners and effect sizes showed a moderate difference at the peak voxels. \n\nThe distinct gender differences located in the middle part of MTG and middle part of STG between genders revealed by the MVPA survived our applied criteria (FWE-correction). In these regions, the classifier successfully distinguished between the vocal and non-vocal condition with better overall accuracy in females as compared to males across the peak voxels. Thus, BOLD signal in parts of auditory cortex seem to carry less information for discriminating vocal from nonvocal sounds in male than females listeners. We do not make any inference on the nature of the underlying processing differences in terms of mental states or cognitive mechanisms, but possible explanations for our findings are discussed below. \n\nMVPA may overall be more sensitive to detect small differences in the activation patterns to vocal and non-vocal sounds. Thus, differences between genders appear significant only when analyzed with MVPA (Haynes et al.,  ; Kriegeskorte et al.,  ; Norman et al.,  ). The differences in classification accuracy between female and male listeners, identified in parts of auditory cortex, may be contributed to by a different predisposition of female/male listeners to the presented vocal sound samples of the voice localizer. Previous findings suggest a sex-difference in response to infant crying and laughing. Women showed a deactivation in the anterior cingulate cortex (ACC) to both laughing and crying (independent of parental status) as compared to men (Seifritz et al.,  ). In contrast, another study showed increased activation to infant vocalization in the amygdala and ACC whereas men showed increased activation to the control stimuli (fragment recombined and edge smoothed stimuli of the original laughing/crying samples). This may reflect a tendency in women for a response preference to infant vocal expressions (Sander et al.,  ). A recent study by De Pisapia et al. ( ) found a sex-difference in response to a baby cry. Women decreased brain activity in DPFC regions and posterior cingulate cortex when they suddenly and passively heard infant cries, whereas men did not. They interpreted their findings in such a way that the female brain interrupts on-going mind-wandering during cries and the male brain continues in self-reflection (De Pisapia et al.,  ). In our study half of the vocal stimuli consisted of infant vocalizations (also emotional expressions such as laughing and crying) and our results may reflect differences in the fine-grained pattern of distributed activity in female and male listeners in response to these vocal expressions of children and babies. The outcome in this study may be affected by anatomical differences in brain structure/size between female and male listeners (Brett et al.,  ). In general individuals vary in their anatomical brain structures and undergo the experiment with different mental states which may influence their brain responses (Huettel et al.,  ). \n\nTo date, there is also evidence for differences in the vocal processing and in particular in speech perception between genders from both behavioral (Hall,  ; Skuk and Schweinberger,  ) and previous fMRI studies (Shaywitz et al.,  ; Schirmer et al.,  ,  ,  ; Junger et al.,  ). These studies found activation differences in frontal brain regions (Schirmer et al.,  ; Junger et al.,  ) and the left posterior MTG and the angular gyrus (Junger et al.,  ). The deviation of the current results in terms of identified brain regions may be due to the different experimental design and computed contrasts, the different applied criteria (e.g., mask), number of included participants and implemented analysis methods. Future studies should further aim to elucidate the relationships between behavioral and functional activation differences. However, the current study shows that the choice of fMRI analysis method (e.g., MVPA) is of relevance when considering subtle between-gender differences. \n\nRegarding the current study, it would be interesting to separate the different vocal categories in the analysis (e.g., by speaker: female/male adults vs. infants/babies) and to perform a behavioral task in order to link differences in brain activation to behavior of the listener. Furthermore, it would be interesting for future studies to take into account more specific aspects of voice quality, which were not considered in the current study. Even subtle differences in phonation (e.g., whispery voice, harshness of a voice), articulation (e.g., vowel space) and or prosody (e.g., pitch variability, loudness, tempo) are critical aspects of voice processing and could be investigated using similar methodical approaches. Apart from studying differences between women and men, also other listener characteristics, such as differences between young and elderly participants, different nationalities and/or familiarity with the presented voices/stimuli should be considered. \n\n\n## Conclusion \n  \nMale and female participants were similar in their pattern of activity differences in response to vocal vs. nonvocal sounds in the TVA of the auditory cortex. Yet, MVPA revealed several regions of significant gender differences in classification performance between female and male listeners: in these regions the distributed pattern of local activity from female participants allowed significantly better vocal/nonvocal classification than that of male participants; no region showed the opposite male > female difference. The neuronal mechanims underlying the observed differences remain unclear. \n\n### Conflict of interest statement \n  \nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. \n\n\n \n", "metadata": {"pmcid": 4115625, "text_md5": "23ec3d742a549aaf4e34f017ca99b807", "field_positions": {"authors": [0, 91], "journal": [92, 106], "publication_year": [108, 112], "title": [123, 169], "keywords": [183, 297], "abstract": [310, 1871], "body": [1880, 34166]}, "part": 1, "chapter": 2, "page": 9, "pmid": 25126055, "doi": "10.3389/fnins.2014.00228"}, "display_title": "pmcid: <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4115625>4115625</a> \u2014 Part 1 Chapter 2 Page 9", "list_title": "1.2.9  Gender differences in the temporal voice areas"}
{"text": "Ahrens, Merle-Marie and Awwad Shiekh Hasan, Bashar and Giordano, Bruno L. and Belin, Pascal\nFront Neurosci, 2014\n\n# Title\n\nGender differences in the temporal voice areas\n\n# Keywords\n\ngender difference\nfMRI\nvoice localizer\ntemporal voice areas\nmultivariate pattern analysis (MVPA)\nvoice perception\n\n\n# Abstract\n \nThere is not only evidence for behavioral differences in voice perception between female and male listeners, but also recent suggestions for differences in neural correlates between genders. The fMRI functional voice localizer (comprising a univariate analysis contrasting stimulation with vocal vs. non-vocal sounds) is known to give robust estimates of the temporal voice areas (TVAs). However, there is growing interest in employing multivariate analysis approaches to fMRI data (e.g., multivariate pattern analysis; MVPA). The aim of the current study was to localize voice-related areas in both female and male listeners and to investigate whether brain maps may differ depending on the gender of the listener. After a univariate analysis, a random effects analysis was performed on female (  n   = 149) and male (  n   = 123) listeners and contrasts between them were computed. In addition, MVPA with a whole-brain searchlight approach was implemented and classification maps were entered into a second-level permutation based random effects models using statistical non-parametric mapping (SnPM; Nichols and Holmes,  ). Gender differences were found only in the MVPA. Identified regions were located in the middle part of the middle temporal gyrus (bilateral) and the middle superior temporal gyrus (right hemisphere). Our results suggest differences in classifier performance between genders in response to the voice localizer with higher classification accuracy from local BOLD signal patterns in several temporal-lobe regions in female listeners. \n \n\n# Body\n \n## Introduction \n  \nPrior functional magnetic resonance imaging (fMRI) findings suggest a robust brain response to vocal vs. non-vocal sounds in many regions of the human auditory cortex in particular in the superior temporal gyrus (STG). Vocal sounds, including but not restricted to speech sounds, evoke a greater response than non-vocal sounds with bilateral activation foci located near the anterior part of the STG extending to anterior parts of the superior temporal sulcus (STS) and posterior foci located in the middle STS (Binder et al.,  ; Belin et al.,  ,  ). Using the functional voice localizer, these findings were replicated and used in various studies (Belin et al.,  ,  ; Kreifelts et al.,  ; Latinus et al.,  ; Ethofer et al.,  ). The conventional way of identifying voice sensitive regions is by applying univariate statistics, implemented using a Generalized-Linear Model (GLM), to fMRI data assuming independence among voxels. \n\nInterest has recently grown in applying multivariate approaches (e.g., Multivariate pattern analysis; MVPA). Instead of modeling individual voxels independently (univariate analysis), MVPA considers the information of distributed pattern in several voxels (e.g., Norman et al.,  ; Mur et al.,  ). Several studies used multivariate approaches to decode information reflected in brain activity patterns related to specific experimental conditions (Cox and Savoy,  ; Haynes and Rees,  ,  ; Kotz et al.,  ). MVPA is usually applied on unsmoothed data preserving high spatial frequency information. Thus, MVPA is argued to be more sensitive in detecting different cognitive states. In contrast, the conventional univariate analysis averages across voxels, thereby removing focally distributed effects (spatial smoothing). The smoothing across voxels may lead to a reduction in the information content (Kriegeskorte et al.,  ; Norman et al.,  ; Haynes et al.,  ). At present, a multivariate approach has never been employed to investigate whether it may yield a different pattern of voice-specific (voice/non-voice classification) brain regions compared to the univariate analysis. \n\nThe voice contains socially and biologically relevant information and plays a crucial role in human interaction. This information is particularly relevant for interaction between different genders (e.g., regarding emotions, identities, and attractiveness) (Belin et al.,  ,  ). Overall, research suggests that women are more sensitive than men in emotion recognition from faces and voices (Hall,  ; Hall et al.,  ; Schirmer and Kotz,  ). Women perform better in judging others' non-verbal behavior (Hall,  ) and seem to process nonverbal emotional information more automatically as compared to men (Schirmer et al.,  ). In addition, women but not men show greater limbic activity when processing emotional facial expressions (Hall et al.,  ). The exact neural mechanisms underlying voice processing in both female and male listeners still remains under debate. For instance, a study by Lattner et al. ( ) found no significant difference between the activation patterns of female and male listeners in response to voice-related information. However, there is evidence from both behavioral and neural activation studies for differences in voice perception between listeners' gender (Shaywitz et al.,  ; Schirmer et al.,  ,  ,  ; Junger et al.,  ; Skuk and Schweinberger,  ). \n\nA recent behavioral study by Skuk and Schweinberger ( ) investigated gender differences in a familiar voice identification task. They found an own-gender bias for males but not for females while females outperformed males overall. These behavioral differences (Skuk and Schweinberger,  ) may also be reflected by differences in neural activity. Previous fMRI studies investigating potential neural correlates suggested a sex difference in the functional organization of the brain for phonological processing (Shaywitz et al.,  ), in emotional prosodic and semantic processing (Schirmer et al.,  ,  ) and in response to gender-specific voice perception (Junger et al.,  ). Further evidence suggests differences between genders in vocal processing shown by an EEG study, where the processing of vocal sounds with more emotional and/or social information was more sensitive in women as compared to men (Schirmer and Kotz,  ; Schirmer et al.,  ). The above-mentioned studies mainly focus on gender differences in emotional speech processing or opposite-sex perception. However, identified brain regions are not consistent: different experimental designs and applied methods vary and make it difficult to compare between these studies (Shaywitz et al.,  ; Schirmer et al.,  ,  ,  ; Junger et al.,  ). \n\nThe current study employs a well-established experimental design of the functional \u201cvoice localizer,\u201d known to give robust estimates of the TVAs across the majority of participants. The voice localizer includes a variety of different vocal sounds, not exclusively female or male voices, but also speech and non-speech of women, men and infants and non-vocal sounds (e.g., environmental sounds). In this study, we were interested in the effect of gender on the results of the voice localizer and we asked an explorative research question of whether brain activation and/or classification accuracy maps in response to vocal (speech and non-speech) and non-vocal sounds differ between female and male listeners without prior assumptions about the strength of voice-specific activity. \n\nThe voice localizer paradigm is often used in the literature (Belin et al.,  ,  ; Kreifelts et al.,  ; Latinus et al.,  ; Ethofer et al.,  ), which makes it easier to compare among studies as well as among participants or groups. Instead of using the conventional univariate method, employing MVPA may offer a more sensitive approach in order to study potential differences between genders by means of above chance vocal/non-vocal classification accuracies in different regions of the brain. Therefore, we investigated our research question by implementing the conventional univariate analysis using GLM and MVPA based on a support-vector machine (SVM) classifier with a spherical searchlight approach. This approach enabled us to explore cortical activity over the whole-brain and to examine whether activation and/or classification maps in response to the voice localizer may significantly differ between genders. Since the effect size between genders is expected to be very small, the current study offers a substantially large sample size with   n   = 149 females and   n   = 123 males. Thus, this study provides a large sample size, a well-established experimental design and the direct comparison of two different fMRI data analysis approaches applied on the exact same data. \n\n\n## Methods \n  \n### Participants \n  \nfMRI data of 272 healthy participants, 149 female (age range: 18\u201368 years; mean \u00b1   SD   = 24.5 \u00b1 8.0) and 123 male (age range: 18\u201361 years; mean \u00b1   SD   = 24.4 \u00b1 6.5) with self-reported normal audition were analyzed. This study was conducted at the Institute of Neuroscience and Psychology (INP) in Glasgow and approved by the ethics committee of the University of Glasgow. Volunteers provided written informed consent before participating and were paid afterwards. \n\n\n### Voice localizer paradigm \n  \nSubjects were instructed to close their eyes and passively listen to a large variety of sounds. Stimuli were presented in a simple block design and divided into vocal (20 blocks) and non-vocal (20 blocks) conditions. Vocal blocks contained only sounds of human vocal origin (excluding sounds without vocal fold vibration such as whistling or whispering) and consisted of speech (e.g., words, syllables, connected speech in different languages) or non-speech (e.g., coughs, laughs, sighs and cries). The vocal stimuli consisted of recordings from 7 babies, 12 adults, 23 children, and 5 elderly people. Half of the vocal sounds (speech and non-speech) consisted of vocalizations from adults and elderly people (women and men) with comparable proportions for both genders (~24% female, ~22% male). The other half of the vocal sounds consisted of infant vocalizations (speech and non-speech) which also included baby crying/laughing. Recorded non-vocal sounds included various environmental sounds (e.g., animal vocalizations, musical instruments, nature and industrial sounds). A total number of 40 blocks were presented. Each block lasted for 8 s with an inter-block interval of 2 s. Stimuli (16bit, mono, 22050 Hz sampling rate) were normalized for RMS and are available at   (Belin et al.,  ). \n\n\n### MRI data acquisition \n  \nScanning was carried out in a 3T MR scanner (Magnetom Trio Siemens, Erlangen, Germany) and all data were acquired with the same scanner at the INP in Glasgow. Functional MRI volumes of the whole cortex were acquired using an echo-planar gradient pulse sequence (voxel size = 3 mm \u00d7 3 mm \u00d7 3 mm; Time of Repetition (TR) = 2000 ms; Echo Time (TE) = 30 ms; slice thickness = 3 mm; inter-slice gap = 0.3 mm; field of view (FoV) = 210 mm; matrix size = 70 \u00d7 70; excitation angle = 77\u00b0). A total number of 310 volumes (32 slices per volume, interleaved acquisition order) were collected with a total acquisition time of 10.28 min. Anatomical MRI volumes were acquired using a magnetization-prepared rapid gradient echo sequence (MPRAGE) (voxel size = 1 \u00d7 1 \u00d7 1 mm;   TR   = 1900 ms;   TE   = 2.52 ms; inversion time (TI) = 900 ms; slice thickness = 1 mm; FoV = 256 mm; matrix size = 256 \u00d7 265; excitation angle = 9\u00b0; 192 axial slices). \n\n\n### fMRI data analysis \n  \n#### Pre-processing \n  \nPre-processing was performed using the statistical parametric mapping software SPM8 (Department of Cognitive Neurology, London, UK.  ). After reorientation of functional and anatomical volumes to the AC/PC line (anterior- and posterior commissure), functional images were motion corrected (standard realignment). Since, subjects may have moved between anatomical and functional data acquisition, the anatomical volumes were co-registered to the mean functional image produced in the realignment above. Anatomical volumes were segmented in order to generate a binary gray matter template at threshold probability level of 0.5 for each individual participant. This template was applied during model specification in both univariate analysis und MVPA. For the univariate processing, realigned functional volumes were normalized to a standard MNI template (Montreal Neurological Institute) and spatially smoothed with a 6 mm full-width at half mean (FWHM) Gaussian Kernel. \n\n\n#### Univariate analysis \n  \nThe design matrix was defined such that each block of the experimental paradigm correlated to one condition, yielding a design matrix with 20 onsets for each condition (vocal and non-vocal). Analysis was based on the conventional general linear model (GLM) and stimuli were convolved with a boxcar hemodynamic response function provided by SPM8. Contrast images of vocal vs. non-vocal conditions were generated for each individual subject and entered into a second-level random effects analysis (RFX). To declare at the group-level whether any difference between the two conditions was significantly larger than zero, a one-sample   t  -test was applied and FWE-corrected (  p   < 0.05) brain maps were calculated. To investigate whether brain activity significantly differs between genders in response to vocal vs. non-vocal sounds, contrasts between females vs. males (male > female, female > male) were computed in a second level RFX analysis (two-sample   t  -test;   p   < 0.05 FWE-corrected). This analysis was restricted to voxels with classification accuracy significantly above theoretical chance (  p   < 0.01 uncorrected) in both females and males (see MVPA below and yellow area in   Figure 2  ). \n\n\n#### Multivariate pattern analysis \n  \nMultivariate pattern classification was performed on unsmoothed and non-normalized data using Matlab (Mathworks Inc., Natick, USA) and in-house utility scripts (INP, Voice Neurocognition Laboratory; Dr. Bashar Awwad Shiekh Hasan and Dr. Bruno L. Giordano), where the default linear support vector machine (SVM) classifier was applied. The classifier was trained and separately tested following a leave-one out cross validation strategy applied on the 40 beta parameter estimates obtained from the univariate analysis (GLM). \n\nA whole-brain searchlight decoding analysis was implemented using a sphere with a radius of 6 mm (average number of voxels in one sphere: 20.6 \u00b1 1.0   SD  ) (Kriegeskorte et al.,  ). A sphere was only considered for analysis if a minimum of 50% of its voxels were within the gray matter. The data of the voxels within a sphere were classified and the classification accuracy was stored at the central voxel, yielding a 3D brain map of classification accuracy (percentage of correct classifications) (Kriegeskorte et al.,  ; Haynes et al.,  ). To identify brain regions in which classification accuracy was significantly above chance by females and males, the theoretical chance level (50%) was subtracted, then normalized (to the MNI template) and smoothed (6 mm FWHM Gaussian Kernel). To make inference on female and male participants, classification brain maps were entered into a second-level permutation based analysis using statistical nonparametric mapping (SnPM; Statistical NonParametric Mapping; available at  ) with 10,000 permutations (see Holmes et al.,  ; Nichols and Holmes,  ). This was computed separately by gender and the resulting voxels were assessed for significance at 5% level and FWE-corrected, as determined by permutation distribution. Similarly, to assess whether classification brain maps significantly differ between genders in response to vocal/non-vocal sounds, this permutation approach was implemented between groups (female > male, male > female) with 10,000 permutations and the resulting voxels were assessed for significance at 5% level and FWE-corrected, as determined by permutation distribution (see Holmes et al.,  ; Nichols and Holmes,  ). \n\nThe between-group analysis was restricted to a mask defined by voxels with classification accuracy significantly above theoretical chance (  p   < 0.01 uncorrected) in both females and males. The resulting mask included 3783 voxels (yellow area in   Figure 2  ). The same mask was applied for both, the univariate analysis and MVPA. \n\nSeparate brain maps of vocal vs. non-vocal contrast in female and male participants as well as brain maps of contrasts between genders for both, univariate analysis and MVPA were generated using the program MRIcoGL (available at  ). \n\n\n\n\n## Results \n  \n### Univariate analysis: vocal vs. non-vocal sounds \n  \nThe univariate analysis comparing activation to vocal and non-vocal sounds showed extended areas of greater response to vocal sounds in the typical regions of the temporal voice areas (TVA), highly similar for male and female subjects (Figure  ). These regions were located bilaterally in the temporal lobes extending from posterior parts of the STS along the STG to anterior parts of the STS and also including several parts of the superior and middle temporal gyrus (STG, MTG). \n  \n Brain maps of female (red,   n   = 149) and male (blue,   n   = 123) participants  .   (A)   Univariate analysis showing bilateral activation along the superior temporal sulcus (STS) and in the inferior frontal gyrus (IFG) and corresponding contrast estimates of vocal vs. non-vocal sounds plotted for peak voxel (one-sample   t  -test, FWE-corrected,   p   < 0.05; cf. circles, note that the two peaks with highest   T  -value and largest cluster size are indicated per group).   (B)   MVPA showing comparable classification accuracy maps along STS, but not IFG and average classification accuracy \u00b1 s.e.m. at peak voxel (calculated in native space) was distinctly above chance level (0.5) for both females and males (maximum intensity projection of   t  -statistic image threshold at FWE-corrected   p   < 0.05, as determined by permutation distribution with 10,000 permutations). \n  \nSeveral hemispheric maxima of vocal vs. non-vocal response were located bilaterally along the STS in both females and males (Figure  , Table  ). Figure   shows parameter estimates of the vocal > non-vocal contrasts at the maxima of the largest cluster sizes with the highest   T  -values of each hemisphere. The brain activation differences between vocal and non-vocal response was consistent across maxima in females (MNI coordinates left:   x   = \u221257,   y   = \u221216,   z   = \u22122, cluster size 3923,   T   = 20.85; right:   x   = 60,   y   = \u221213,   z   = \u22122,   T  -value = 20.64) and in males (MNI coordinates left:   x   = \u221260,   y   = \u221222,   z   = 1, cluster size 796,   T   = 18.19; right:   x   = 60,   y   = \u221210,   z   = \u22122, cluster size 812,   T  -value = 17.46). Female listeners showed one large cluster covering the temporal lobes and subcortical parts of the brain. By contrast male listeners showed two separate voxel clusters in the left and right temporal lobes and no subcortical cluster connecting the two hemispheres (Table  ). Small bilateral clusters were found in inferior prefrontal cortex (inferior frontal gyrus, IFG) in both female and male listeners (  p   < 0.05 FWE-corrected; Figure  ). \n  \n Voice-sensitive peak voxels of female and male RFX analysis (Univariate)  . \n  \nPeak voxel coordinates in standard MNI space and corresponding t-values above for female and male 4.49 (FWE-corrected p < 0.05). \n  \n\n### MVPA analysis: vocal/non-vocal classification \n  \nThe MVPA analysis showed clusters of significantly above-chance voice/non-voice classification accuracy in the TVAs (Figure  , Table  ) (Figure  , Table  ). Hemispheric maxima of classification accuracy were at comparable locations as the peaks of voice > non-voice activation revealed by the univariate method. The classification accuracy within the peak voxel of female listeners (MNI coordinates left:   x   = \u221260,   y   = \u221216,   z   = 1, cluster size 1676,   T  -value = 20.41; right:   x   = 66,   y   = \u221231,   z   = 4, cluster size 1671,   T  -value = 21.45) as well as for male listeners (MNI coordinates left:   x   = \u221260,   y   = \u221222,   z   = 4, cluster size 984,   T  -value = 13.70; right:   x   = 63,   y   = \u221228,   z   = 4, cluster size 1211,   T  -value = 16.07) were distincly above the theoretical chance level of 0.5 (Figure  ). Overall, the maximal classification accuracy was higher in female listeners as compared to male listeners at the peak voxels (Figure  , mean \u00b1 s.e.m.: left peak in females 0.84 \u00b1 0.006, males 0.83 \u00b1 0.009; right peak in females 0.85 \u00b1 0.007, males 0.84 \u00b1 0.009. Left peak in males 0.83 \u00b1 0.009, females 0.85 \u00b1 0.006, right peak in males 0.85 \u00b1 0.009, females 0.87 \u00b1 0.007). Comparing MVPA and univariate analysis in Figures   the MVPA analysis revealed more superficial cortical regions bilateral at the temporal pole, whereas the voxel cluster of the vocal vs. non-vocal difference of the univariate analysis extend more toward the midline of the brain. \n  \n Voice-sensitive peak voxels of female and male group analysis (MVPA)  . \n  \nPeak voxel coordinates in standard MNI space and corresponding t-values above for female 4.38 and male 4.29 (FWE-corrected p < 0.05, as determined by permutation distribution with 10,000 permutations). \n  \n\n### Female vs. male contrasts \n  \nThe contrast of activation maps (univariate analysis) or classification accuracy maps (multivariate approach) from males and females revealed no significant voxels with greater parameter estimates for males > females at the chosen statistical significance threshold (  p   < 0.05, FWE-corrected) for either analysis methods. The reverse contrast (female > male), however, revealed significant voxel clusters showing greater parameter estimates for univariate analysis and higher classification accuracy for MVPA in female participants (Figure  ). \n  \n Contrast between female > male (red)  .   (A)   Univariate analysis showing significant female > male difference (two-sample   t  -test, FWE-corrected,   p   < 0.05) in the left posterior part of the superior temporal gyrus (STG) and the right anterior STG. Contrast estimates at peak voxel showing stronger activation in females (black) as compared to males (gray) in response to vocal vs. non-vocal sounds.   (B)   MVPA showing significant classification accuracy above chance level in the right middle part of the middle temporal gyrus (MTG) and the right middle STG as well as in the left middle MTG with higher average classification accuracy in females (black) than in males (gray) (maximum intensity projection of   t  -statistic image threshold at FWE-corrected   p   < 0.05, as determined by permutation distribution with 10,000 permutations). The (yellow) cluster shows the mask including voxels with significantly above chance classification accuracy in both females and males (  p   < 0.01 uncorrected). \n  \nWhen analyzed with the univariate approach (Figure  ) the contrast female > male yielded only a few significant voxels: One cluster consisted of four voxels in the left posterior part of STG and only one voxels in the right Insula (Figure  , Table  ). The corresponding contrast estimates for the reported peak voxels (MNI coordinates left:   x   = \u221248,   y   = \u221234,   z   = 16, cluster size 4,   T  -value = 4.02; right:   x   = 48,   y   = 2,   z   = \u22125, cluster size = 1,   T  -value = 4.04) showed a positive response for females in both hemispheres and for the left hemisphere in males. The Cohen's d effect size values (  d   = 0.48 and 0.49) suggested a moderate difference at the peak voxel (Table  ). Overall, females showed a stronger activation in response to vocal vs. non-vocal sounds as compared to males at both maxima (Figure  ). \n  \n Peak voxels of female > male contrast for univariate analysis and MVPA  . \n  \nPeak voxel coordinates in standard MNI space and corresponding t-values above 3.85 (univariate analysis, FWE-corrected p < 0.05) and 3.70 for MVPA (FWE-corrected p < 0.05, as determined by permutation distribution with 10,000 permutations) and Cohen's d for large cluster size. The Cohen's d of the MVPA refers to the mean difference in classification accuracy (contrast estimates of the univariate analysis respectively), divided by the pooled standard deviation for those means. \n  \nThe female > male contrast of classification accuracy maps identified significant voxel clusters in the middle part of the middle temporal gyrus (MTG) in both hemispheres, in which classification accuracy was greater for female than male participants (red clusters in Figure  ). Areas of greater classification accuracy in females were more extended in the left hemisphere with an additional smaller cluster located in the STG. The peak voxels of female > male classification accuracy difference were located in the middle part of the MTG (bilateral), and the left middle STG (MNI coordinates left:   x   = \u221269,   y   = \u2212 19,   z   = \u22128, cluster size 84,   T  -value = 5.22;   x   = \u221251,   y   = \u221222,   z   = 13, cluster size 156,   T  -value = 5.19; right:   x   = 69,   y   = \u22127,   z   = \u221211, cluster size 52,   T  -value = 4.48; cf. circle in Figure  ). The Cohen's d effect size values (  d   = 0.35, 0.35, and 0.24) suggested a small difference at the peak voxel (Table  ). Classification accuracy (computed in native space) at these coordinates was distinctly above chance (50%) for both females and males, but higher in females across peaks (Figure  ). \n\n\n\n## Discussion \n  \nThe present study aimed to investigate gender differences on voice localizer scans by employing the conventional univariate analysis as well as MVPA. Both analysis approaches revealed largely overlapping/comparable and robust estimates of the TVAs in female and male listeners. However, the MVPA was more sensitive to differences in the middle MTG of the left and right hemispheres and the middle left STG between genders as compared to univariate analysis with higher classification accuracy in women. \n\n\n## Robust TVAs \n  \nThe estimated TVAs using MVPA robustly replicated and confirmed prior fMRI findings applying the voice localizer (Belin et al.,  ,  ; Belin and Zatorre,  ; Scott and Johnsrude,  ; Von Kriegstein et al.,  ). Both analysis methods showed comparable maps of classification accuracy (MVPA) and of vocal vs. non-vocal activity difference (univariate analysis) for both female and male listeners. The average classification accuracy at the peak voxel was distinctly above chance level and higher in female as compared to male listeners. The peak voxels were at comparable locations (along middle and posterior parts of the STS) for both analysis approaches and both genders. A small difference between the MVPA and univariate analysis can be seen bilateral at the temporal pole, where the MVPA detected more vocal/non-vocal differences in superficial cortical regions as compared to the univariate analysis. In addition to the activation brain maps showing the robustly estimated TVAs (univariate analysis), the MVPA results extend previous findings by providing a corresponding classification accuracy brain map. When brain maps are considered for each analysis approach and for female and male listeners separately, our findings showed no distinct differences between genders and between univariate analysis and MVPA. Instead comparable voxel clusters of a similar size in the bilateral temporal lobes were identified, verifying the prior univariate analysis and the robustness of the TVAs (see e.g., Belin et al.,  ). \n\n\n## Gender differences \n  \nWhen data were analyzed with MVPA, differences between female and male listeners in response to vocal/non-vocal sounds were found by contrasting female > male (but not male > female). A significant difference in success of the MVPA between female and male listeners was apparent in the middle part of the MTG in both hemispheres and in the middle part of the STG in the left hemisphere. Effect sizes showed a small difference at the peak voxels. Despite the large sample size used in this study, the univariate analysis showed no major activation differences between genders. Only two small clusters with one to four voxels were significant in the posterior and anterior part of the STG. In the univariate analysis, the overall activation difference between vocal vs. non-vocal sounds was stronger in female as compared to male listeners and effect sizes showed a moderate difference at the peak voxels. \n\nThe distinct gender differences located in the middle part of MTG and middle part of STG between genders revealed by the MVPA survived our applied criteria (FWE-correction). In these regions, the classifier successfully distinguished between the vocal and non-vocal condition with better overall accuracy in females as compared to males across the peak voxels. Thus, BOLD signal in parts of auditory cortex seem to carry less information for discriminating vocal from nonvocal sounds in male than females listeners. We do not make any inference on the nature of the underlying processing differences in terms of mental states or cognitive mechanisms, but possible explanations for our findings are discussed below. \n\nMVPA may overall be more sensitive to detect small differences in the activation patterns to vocal and non-vocal sounds. Thus, differences between genders appear significant only when analyzed with MVPA (Haynes et al.,  ; Kriegeskorte et al.,  ; Norman et al.,  ). The differences in classification accuracy between female and male listeners, identified in parts of auditory cortex, may be contributed to by a different predisposition of female/male listeners to the presented vocal sound samples of the voice localizer. Previous findings suggest a sex-difference in response to infant crying and laughing. Women showed a deactivation in the anterior cingulate cortex (ACC) to both laughing and crying (independent of parental status) as compared to men (Seifritz et al.,  ). In contrast, another study showed increased activation to infant vocalization in the amygdala and ACC whereas men showed increased activation to the control stimuli (fragment recombined and edge smoothed stimuli of the original laughing/crying samples). This may reflect a tendency in women for a response preference to infant vocal expressions (Sander et al.,  ). A recent study by De Pisapia et al. ( ) found a sex-difference in response to a baby cry. Women decreased brain activity in DPFC regions and posterior cingulate cortex when they suddenly and passively heard infant cries, whereas men did not. They interpreted their findings in such a way that the female brain interrupts on-going mind-wandering during cries and the male brain continues in self-reflection (De Pisapia et al.,  ). In our study half of the vocal stimuli consisted of infant vocalizations (also emotional expressions such as laughing and crying) and our results may reflect differences in the fine-grained pattern of distributed activity in female and male listeners in response to these vocal expressions of children and babies. The outcome in this study may be affected by anatomical differences in brain structure/size between female and male listeners (Brett et al.,  ). In general individuals vary in their anatomical brain structures and undergo the experiment with different mental states which may influence their brain responses (Huettel et al.,  ). \n\nTo date, there is also evidence for differences in the vocal processing and in particular in speech perception between genders from both behavioral (Hall,  ; Skuk and Schweinberger,  ) and previous fMRI studies (Shaywitz et al.,  ; Schirmer et al.,  ,  ,  ; Junger et al.,  ). These studies found activation differences in frontal brain regions (Schirmer et al.,  ; Junger et al.,  ) and the left posterior MTG and the angular gyrus (Junger et al.,  ). The deviation of the current results in terms of identified brain regions may be due to the different experimental design and computed contrasts, the different applied criteria (e.g., mask), number of included participants and implemented analysis methods. Future studies should further aim to elucidate the relationships between behavioral and functional activation differences. However, the current study shows that the choice of fMRI analysis method (e.g., MVPA) is of relevance when considering subtle between-gender differences. \n\nRegarding the current study, it would be interesting to separate the different vocal categories in the analysis (e.g., by speaker: female/male adults vs. infants/babies) and to perform a behavioral task in order to link differences in brain activation to behavior of the listener. Furthermore, it would be interesting for future studies to take into account more specific aspects of voice quality, which were not considered in the current study. Even subtle differences in phonation (e.g., whispery voice, harshness of a voice), articulation (e.g., vowel space) and or prosody (e.g., pitch variability, loudness, tempo) are critical aspects of voice processing and could be investigated using similar methodical approaches. Apart from studying differences between women and men, also other listener characteristics, such as differences between young and elderly participants, different nationalities and/or familiarity with the presented voices/stimuli should be considered. \n\n\n## Conclusion \n  \nMale and female participants were similar in their pattern of activity differences in response to vocal vs. nonvocal sounds in the TVA of the auditory cortex. Yet, MVPA revealed several regions of significant gender differences in classification performance between female and male listeners: in these regions the distributed pattern of local activity from female participants allowed significantly better vocal/nonvocal classification than that of male participants; no region showed the opposite male > female difference. The neuronal mechanims underlying the observed differences remain unclear. \n\n### Conflict of interest statement \n  \nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. \n\n\n \n\n# Table(s)\n## ID: T1\n### Label: Table 1\nAnatomical location\tPeak voxel x, y, z\tt-values\tCluster size\nFEMALE LISTENERS\tFEMALE LISTENERS\tFEMALE LISTENERS\tFEMALE LISTENERS\nLeft/Right hemisphere\tLeft/Right hemisphere\tLeft/Right hemisphere\tLeft/Right hemisphere\nLeft STG, middle\t\u221257, \u221216, \u22122\t20.85\t3923\nRight STG, middle\t60, \u221213, \u22122\t20.64\t\nRight STG, middle\t63, \u221222, \u22122\t20.11\t\nLeft frontal hemisphere\tLeft frontal hemisphere\tLeft frontal hemisphere\tLeft frontal hemisphere\nIFG (pars triangularis)\t\u221248, 17, 22\t9.25\t178\nIFG (pars triangularis)\t\u221239, 29, \u22122\t8.79\t103\nPrecentral gyrus\t\u221248, \u22127, 43\t6.32\t5\nRight frontal hemisphere\tRight frontal hemisphere\tRight frontal hemisphere\tRight frontal hemisphere\nIFG (orbital)\t48, 17, \u22128\t4.87\t1\nMALE LISTENERS\tMALE LISTENERS\tMALE LISTENERS\tMALE LISTENERS\nLeft hemisphere\tLeft hemisphere\tLeft hemisphere\tLeft hemisphere\nSTG, middle\t\u221260, \u221222, 1\t18.15\t796\nSTG, middle\t\u221257, \u221213, \u22122\t17.97\t\nSTG, posterior\t\u221260, \u221237, 4\t12.73\t\nIFG (pars triangularis)\t\u221242, 29, \u22122\t7.96\t40\nIFG (pars triangularis)\t\u221242, 17, 22\t5.56\t32\nHippocampus\t\u221218, \u221210, \u221214\t4.61\t1\nRight hemisphere\tRight hemisphere\tRight hemisphere\tRight hemisphere\nSTG, middle\t60, \u221210, \u22122\t17.40\t812\nSTG, middle\t63, \u221222, \u22122\t17.11\t\nSTG, anterior\t54, 5, \u221214\t11.51\t\nIFG (pars triangularis)\t42, 32, \u22122\t6.76\t165\nIFG (pars triangularis)\t54, 23, 22\t6.62\t\nIFG (pars triangularis)\t45, 17, 22\t6.51\t\nPrecentral gyrus\t51, \u22121, 46\t7.60\t22\n### Caption\nVoice-sensitive peak voxels of female and male RFX analysis (Univariate).\n### Footer\nPeak voxel coordinates in standard MNI space and corresponding t-values above for female and male 4.49 (FWE-corrected p < 0.05).\n\n\n## ID: T2\n### Label: Table 2\nAnatomical location\tPeak voxel x, y, z\tt-values\tCluster size\nFEMALE LISTENERS\tFEMALE LISTENERS\tFEMALE LISTENERS\tFEMALE LISTENERS\nLeft hemisphere\tLeft hemisphere\tLeft hemisphere\tLeft hemisphere\nMTG, anterior\t\u221260, \u221216, 1\t20.41\t1676\nMTG, posterior\t\u221263, \u221237, 7\t18.26\t\nRight hemisphere\tRight hemisphere\tRight hemisphere\tRight hemisphere\nSTG, middle\t66, \u221231, 4\t21.45\t1671\nSTG, anterior\t60, \u22127, \u22125\t19.49\t\nMALE LISTENERS\tMALE LISTENERS\tMALE LISTENERS\tMALE LISTENERS\nLeft hemisphere\tLeft hemisphere\tLeft hemisphere\tLeft hemisphere\nMTG, middle\t\u221260, \u221222, 4\t13.70\t984\nRight hemisphere\tRight hemisphere\tRight hemisphere\tRight hemisphere\nSTG, middle\t63, \u221228, 4\t16.07\t1211\nMTG, anterior\t63, \u221210, \u22125\t14.88\t\n### Caption\nVoice-sensitive peak voxels of female and male group analysis (MVPA).\n### Footer\nPeak voxel coordinates in standard MNI space and corresponding t-values above for female 4.38 and male 4.29 (FWE-corrected p < 0.05, as determined by permutation distribution with 10,000 permutations).\n\n\n## ID: T3\n### Label: Table 3\nAnatomical location\tPeak voxel x, y, z\tt-values\tCluster size\tCohen's d at the peak voxel\nUNIVARIATE (FEMALE > MALE)\tUNIVARIATE (FEMALE > MALE)\tUNIVARIATE (FEMALE > MALE)\tUNIVARIATE (FEMALE > MALE)\tUNIVARIATE (FEMALE > MALE)\nLeft hemisphere\tLeft hemisphere\tLeft hemisphere\tLeft hemisphere\tLeft hemisphere\nSTG, posterior\t\u221248, \u221234, 16\t4.02\t4\t0.48\nRight hemisphere\tRight hemisphere\tRight hemisphere\tRight hemisphere\tRight hemisphere\nInsula\t48, 2, \u22125\t4.04\t1\t0.49\nMVPA (FEMALE > MALE)\tMVPA (FEMALE > MALE)\tMVPA (FEMALE > MALE)\tMVPA (FEMALE > MALE)\tMVPA (FEMALE > MALE)\nLeft hemisphere\tLeft hemisphere\tLeft hemisphere\tLeft hemisphere\tLeft hemisphere\nSTG, middle\t\u221269, \u221219, \u22128\t5.22\t84\t0.35\nSTG, middle\t\u221266, \u22121, \u22128\t5.02\t\t\nSTG, middle\t\u221251, \u221222, 13\t5.19\t156\t0.35\nSTG, middle\t\u221248, \u221231, 4\t4.77\t\t\nSTG, posterior\t\u221242, \u221243, 7\t4.66\t\t\nMTG, middle\t\u221257, \u221255, 16\t3.82\t2\t\nMTG, middle\t\u221269, \u221240, 1\t3.80\t2\t\nSTG, middle\t\u221269, \u221210, 10\t3.79\t2\t\nRight hemisphere\tRight hemisphere\tRight hemisphere\tRight hemisphere\tRight hemisphere\nSTG, middle\t69, \u22127, \u221211\t4.48\t52\t0.24\nMTG, middle\t66, \u221222, \u221211\t4.42\t\t\nMTG, middle\t69, \u221234, 1\t3.70\t1\t\n### Caption\nPeak voxels of female > male contrast for univariate analysis and MVPA.\n### Footer\nPeak voxel coordinates in standard MNI space and corresponding t-values above 3.85 (univariate analysis, FWE-corrected p < 0.05) and 3.70 for MVPA (FWE-corrected p < 0.05, as determined by permutation distribution with 10,000 permutations) and Cohen's d for large cluster size. The Cohen's d of the MVPA refers to the mean difference in classification accuracy (contrast estimates of the univariate analysis respectively), divided by the pooled standard deviation for those means.\n", "metadata": {"pmcid": 4115625, "text_md5": "be8e44aa602f9bfcbea77e30bd96c2e2", "field_positions": {"authors": [0, 91], "journal": [92, 106], "publication_year": [108, 112], "title": [123, 169], "keywords": [183, 297], "abstract": [310, 1871], "body": [1880, 34166], "tables": [34179, 38514]}, "batch": 1, "pmid": 25126055, "doi": "10.3389/fnins.2014.00228", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4115625", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4115625"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4115625\">4115625</a>", "list_title": "PMC4115625  Gender differences in the temporal voice areas"}
