{"text": "Murphy, Charlotte and Rueschemeyer, Shirley-Ann and Watson, David and Karapanagiotidis, Theodoros and Smallwood, Jonathan and Jefferies, Elizabeth\nNeuroimage, 2017\n\n# Title\n\nFractionating the anterior temporal lobe: MVPA reveals differential responses to input and conceptual modality\n\n# Keywords\n\nAnterior temporal lobe (ATL)\nMulti voxel pattern analysis (MVPA)\nSemantic\nHub\nSpoke\nResting-state connectivity\n\n\n# Abstract\n \nWords activate cortical regions in accordance with their modality of presentation (i.e., written vs. spoken), yet there is a long-standing debate about whether patterns of activity in any specific brain region capture modality-invariant conceptual information. Deficits in patients with semantic dementia highlight the anterior temporal lobe (ATL) as an amodal store of semantic knowledge but these studies do not permit precise localisation of this function. The current investigation used multiple imaging methods in healthy participants to examine functional dissociations within ATL. Multi-voxel pattern analysis identified spatially segregated regions: a response to input modality in anterior superior temporal gyrus (aSTG) and a response to meaning in more ventral anterior temporal lobe (vATL). This functional dissociation was supported by resting-state connectivity that found greater coupling for aSTG with primary auditory cortex and vATL with the default mode network. A meta-analytic decoding of these connectivity patterns implicated aSTG in processes closely tied to auditory processing (such as phonology and language) and vATL in meaning-based tasks (such as comprehension or social cognition). Thus we provide converging evidence for the segregation of meaning and input modality in the ATL. \n   Highlights  \n  \nMulti-voxel pattern analysis identified spatially segregated regions in the anterior temporal lobe. \n  \nAnterior superior temporal gyrus (aSTG) responded to modality input. \n  \nVentral anterior temporal lobe (vATL) responded to semantic meaning. \n  \nConverging findings from resting-state connectivity supports this functional dissociation. \n  \n \n\n# Body\n \n## Introduction \n  \nCurrent neurocognitive models propose that concepts are represented in a large-scale distributed network comprising (1) sensory and motor \u2018spoke\u2019 regions that store knowledge of physical features and (2) convergence zones that integrate across multiple modalities (e.g., visual vs. auditory) to form abstract amodal representations ( ,  ). For example, the hub and spoke model of   proposes that information from modality-specific spoke regions is integrated in an amodal \u2018hub\u2019 region within the anterior temporal lobes (ATL), allowing the conceptual similarity of items that are semantically similar yet share few surface features, such as \u2018flute\u2019 and \u2018violin\u2019, to be represented, and making it possible to map between modalities so that we can picture a flute and imagine the sound that it makes from only its name (e.g.,  ;  ;  ;  ). This hub and spoke model proposes that both the ATL and modality-specific spokes make a crucial contribution to conceptual representation, and these elements are mutually-constraining through a pattern of interactive-activation. \n\nThe spokes are hypothesized to represent the contributions of sensory and motor cortex to conceptual knowledge, as words associated with specific sensorimotor attributes activate corresponding sensorimotor cortex. For example, words denoting actions (e.g., kick) activate the motor system ( ,  ,  ), while words associated with specific smells (e.g., cinnamon) elicit activation in olfactory cortex ( ,  ). Although these neural regions are important for perception and action, they are also recruited during semantic processing to provide meaning to words ( ,  ,  ,  ,  ). \n\nThe proposal that the ATL forms a key semantic \u201chub\u201d capturing knowledge across different input modalities was initially put forward to account for the pattern of impairment in semantic dementia (SD), in which relatively focal atrophy centered on ATL leads to progressive conceptual degradation across modalities and tasks (e.g.,  ;  ). SD patients are highly consistent in the knowledge they can demonstrate when the same concepts are probed in different ways, suggesting central semantic representations degrade in this condition. Patients with SD have atrophy which increasingly affects inferior frontal and posterior temporal areas, as well as ATL, making it difficult to draw strong conclusions about the location of the \u201chub\u201d from neuropsychology alone; however, the severity of the semantic impairment correlates most strongly with the degree of hypometabolism in inferior ATL ( ). The crucial role of ATL is also supported by functional neuroimaging studies of healthy participants that show amodal conceptual processing in ATL ( ;  ). For example,   characterized the degree of modality convergence in STG, MTG, ITG and fusiform cortex comparing posterior and anterior parts of the temporal lobe. Both STG and fusiform were modality-sensitive along the temporal lobe, showing stronger activation for spoken words and pictures respectively. MTG showed a multimodal response in both anterior and posterior regions. ITG uniquely showed a pattern consistent with the increasing integration of information from different inputs, namely sensitivity to modality in posterior but not anterior regions. Moreover,   showed that, despite originating from different sensory inputs, there is considerable activation overlap for spoken and written processing in ATL regions. Thus, emerging evidence from both patients with SD and healthy participants suggests that the semantic hub may be located in ventral ATL. \n\nThese observations raise the possibility of functional dissociations with ATL.   recently observed different patterns of functional connectivity within superior and ventral regions of the ATL, with anterior STG showing stronger connectivity to language, auditory and motor regions, while ventral ATL showed connectivity to other multimodal semantic regions including inferior frontal gyrus, angular gyrus and posterior middle temporal gyrus. These parallel the pattern of white-matter connections found by   and  . Consistent with these findings it has been proposed that superior regions of the ATL are important in lexical and auditory processing, while ventral regions support conceptual processing across all sensory modalities ( ;  ;  ). Ventral and ventrolateral ATL regions have been found to respond to meaningful inputs across multiple modalities by studies employing convergent methods; including fMRI and transcranial magnetic stimulation ( ,  ,  ) and representational similarity analysis (RSA) of ECoG data ( ). \n\nThe current study used multiple imaging methodologies to simultaneously investigate the organization of knowledge in the ATL (hub) and auditory and visual regions (as potential spokes). In a functional experiment we manipulated the format in which words were presented (i.e., spoken, written) and the modality-specific features associated with the word's meaning (e.g., auditory features: \u201cloud\u201d vs. visual features: \u201cshiny\u201d). We used Multi Voxel Pattern Analysis (MVPA) to decode how these different features (modality of presentation and underlying meaning) are represented. Based on the hub and spoke model, we expected this analysis to reveal regions that are distributed across the cortex that responded to the meaning of the stimulus regardless of the input modality. In this experiment we were particularly interested in identifying regions in ATL where the meaning of words is represented that are independent of input modality. The amodal hub regions should be able to code the meaning of a stimulus regardless of the presentation format (e.g., auditory feature words should elicit similar patterns of activation even when spoken and written words are compared). In addition, this region should represent the meaning of words tied to different sensory modalities (i.e., it should represent words with auditory meanings like \u2018loud\u2019 and words with visual meanings like \u2018shiny\u2019). In contrast, the spokes should represent particular semantic features in regions of sensory cortex (i.e., words with an auditory meaning, such as loud, should be represented in auditory cortex regardless of how they are presented (written or spoken). However, spoke regions are not expected to represent meaning that is tied to a different sensory modality (i.e., auditory cortex may not contribute to semantic representation for words with a visual meaning, such as shiny). \n\nNext we used the regions identified in our MVPA analysis as regions of interest in a seed based resting state connectivity analysis to understand the neural networks in which these different regions of the ATL are embedded. We expected the amodal region of ATL to show functional connectivity with regions of cortex that are important in more abstract forms of cognition, e.g., the default mode network, rather than regions important in unimodal sensory processing, such as the auditory and visual cortex. Finally, we used the search tool Neurosynth to decode the most common interpretations of this pattern of functional connectivity in the broader neuroimaging literature. \n\n\n## Materials and methods \n  \n### Functional experiment \n  \n#### Participants \n  \nTwenty participants were recruited from the University of York. One participant's data was excluded due to excessive motion artifacts, leaving nineteen subjects in the final analysis (10 female; mean age 24.55, range 18\u201336 years). Participants were native British speakers, right handed and had normal or corrected-to-normal vision. Participants gave written informed consent to take part and were reimbursed for their time. The study was approved by the York Neuroimaging Centre Ethics Committee at the University of York. \n\n\n#### Stimuli \n  \nParticipants were presented with blocks of spoken and written items from three conditions: AUD words denoted auditory features (e.g., loud), VIS words denoted visual features (e.g., shiny) and NON stimuli were meaningless nonwords (e.g., brodic). A block consisted of a sequence of items; participants were asked to pay attention to the meaning of each item, and respond with their left index finger when an out-of-category item was presented (see  ). For VIS and AUD blocks, half of the out-of-category items were taken from the non-presented feature condition, while the other half were taken from a separate list of taste words (e.g., spicy). Participants could not predict the category of the out-of-category item and therefore had to focus on the AUD or VIS feature specified in the instructions. In the NON condition, participants were asked to respond to any item that was a word. All stimuli were presented in both spoken and a written format. Spoken words were recorded digitally and then normalized for volume and power. Written words were presented centrally as white letters on a black background. The combination of item meaning (AUD, VIS, NON) and presentation format (Spoken, Written) yielded 6 experimental conditions (Spoken-AUD, Spoken-VIS, Spoken-NON, Written-AUD, Written-VIS, Written-NON). \n\nThe selection of AUD and VIS words was validated in a behavioural study with twelve participants who did not take part in the fMRI session. Participants were asked to rate a subset of modality-specific words (n=220), according to how much each one related to four sensory categories; auditory, visual, haptic and taste. Participants also provided ratings of familiarity and emotional valence. All ratings were given on a 5-point likert-scale. We selected adjectives with strong auditory or visual associations. Each set contained 8 items which were matched for key psycholingusitic variables such as frequency and length (see  ; Wilcoxon signed rank tests revealed all   p  >.05). AUD words (such as \u2018loud\u2019) were selected if they scored significantly higher on the auditory than visual, haptic or taste modalities (all p<.001). Likewise VIS words (such as \u2018shiny\u2019) were selected if they scored significantly higher on the visual than the auditory, haptic or taste modalities (all   p  <.001). \n\nA set of 8 taste features were used in out-of-category catch trials. These items scored significantly higher on the taste modality than auditory, visual and haptic (p<.001). These items were also matched to AUD or VIS words on the variables in   (all   p  >.05). Finally, NON words were made by recombining the phonemes from the AUD and VIS conditions to create 8 pseudo-words. The non-word condition matched AUD and VIS conditions on number of letters, syllable and Levenshtein distance ( ), which quantifies the number of phoneme insertions, deletions and/or substitutions required to change one word into another, (all p>.05). The use of a small number of items is consistent with other MVPA studies into semantic representation ( ;  ). \n\nStimulus presentation was controlled by a PC running Neurobehavioural System Presentation\u00ae software (Version 0.07,  ). Stimuli were projected onto a screen viewed though a mirror mounted on the head coil. Spoken stimuli were presented binaurally using MR-compatible headphones. \n\n\n#### Task procedure \n  \nPrior to being scanned, participants were shown a printed copy of all stimuli (8 AUD, 8 VIS, 8 NON) to familiarize them with the items. They also performed a practice session consisting of 12 blocks, identical to one scanning run. \n\nIn the scanner there were 4 runs of 12 blocks. The choice of 4 functional runs is consistent with many MVPA studies that also presented trials within 4 runs that were each 5\u201310\u00a0min long ( ;  ;  ). Within each run, there were two blocks related to each of the 6 experimental conditions (spoken and written words combined with three meaning conditions: AUD, VIS and NON). These were presented in a pseudo-random order, with no immediate repetition of conditions. Blocks were separated by a jittered gap (4\u20138\u00a0s) during which a red fixation cross was presented. A block consisted of 17 stimuli: eight stimuli related to that experimental condition presented twice in a pseudo-random order, with no immediate repetition, plus one out-of-category catch trial. Written stimuli were presented for 600ms; spoken stimuli were presented on average for 633.57\u00a0ms (SD=71.57\u00a0ms). Words within each block were separated by a 500\u00a0ms inter-stimulus interval. \n\nBlock transitions were marked with a written task instruction, which indicated (i) the aspect of meaning that participants needed to focus on and (ii) the presentation format presented in parentheses. The task instructions were presented for 3500\u00a0ms (followed by 500ms fixation). A grey fixation cross against a black background was used to minimize eye movements during the duration of a block. Each block (including task instruction and jittered rest period) lasted on average 28.7\u00a0s. \n\n\n#### Acquisition \n  \nData were acquired using a GE 3\u00a0T HD Excite MRI scanner at the York Neuroimaging Centre, University of York. A Magnex head-dedicated gradient insert coil was used in conjunction with a birdcage, radio-frequency coil tuned to 127.4\u00a0MHz. A gradient-echo EPI sequence was used to collect data from 38 bottom-up axial slices aligned with the temporal lobe (TR=2\u00a0s, TE=18 ms, FOV=192\u00d7192\u00a0mm, matrix size=64\u00d764, slice thickness=3\u00a0mm, slice-gap 1mm, flip-angle=90\u00b0). Voxel size was 3\u00d73\u00d73\u00a0mm. Functional images were co-registered onto a T1-weighted anatomical image from each participant (TR=7.8\u00a0s, TE=3\u00a0ms, FOV=290\u00a0mmx290\u00a0mm, matrix size=256\u00a0mmx256\u00a0mm, voxel size=1.13\u00a0mmx1.13\u00a0mmx1\u00a0mm) using linear registration (FLIRT, FSL). \n\n\n#### Preprocessing \n  \nImaging data were preprocessed using the FSL toolbox ( ). Images were skull-stripped using a brain extraction tool (BET,  ) to remove non-brain tissue from the image. The first five volumes (10\u00a0s) of each scan were removed to minimize the effects of magnetic saturation, and slice-timing correction was applied. Motion correction (MCFLIRT,  ) was followed by temporal high-pass filtering (cutoff=0.01\u00a0Hz). Individual participant data were first registered to their high-resolution T1-anatomical image, and then into a standard space (Montreal Neurological Institute (MNI152); this process included tri-linear interpolation of voxel sizes to 2\u00d72\u00d72\u00a0mm. For univariate analyses, data were additionally smoothed (Gaussian full width half maximum 6 mm). \n\n\n#### Univariate analysis \n  \nThe condition onset and duration were taken from the first item in each block (after the initial instructions) to the end of the last item. The response to each of the 6 conditions was contrasted against rest. Box-car regressors for each condition, for each run, in the general linear model were convolved with a double gamma hemodynamic response function (FEAT, FSL). Regressors of no interest were also included to account for head motion within scans. A fixed effect design (FLAME,  ) was then conducted to average across the four runs, within each individual. Finally, individual participant data were entered into a higher-level group analysis using a mixed effects design (FLAME,  ) whole-brain analysis. \n\n\n#### Multivariate pattern analysis \n  \nParameter estimates were calculated in the same manner as for univariate analyses, for each condition and for each run: in this way, the spatial pattern information entered into the classifier from each condition represented the average response to the 8 exemplars. This method is consistent with previous literature investigating semantic representations ( ;  ;  ): it allows us to make inferences that a particular region is able to discriminate between words referring to auditory and visual features, for example, but not the meanings of these individual words. MVPA was conducted on spatially unsmoothed data to preserve local voxel information. \n\nAs we had a priori knowledge of strong selectivity for the classes in particular brain regions (ATL, primary auditory cortex and primary visual cortex), we opted for a ROI-based MVPA method rather than whole-brain analysis. This reduced the number of voxels used for classification (and therefore the number of free parameters which can lead to over-fitting; for similar approaches see   and  . The following masks were used; primary visual cortex (taken from FSL Juelich Atlas;  ), primary auditory cortex (taken from FSL Juelich Atlas;  ) and ATL (anterior to Y=\u221222;  ). The size of these masks are as follows; primary visual cortex, 12662 voxels; primary auditory cortex, 2372 voxels; ATL, 18523 voxels. \n\nTo ensure that our ROIs had sufficient signal to detect reliable fMRI activation, the temporal signal-to-noise ratio (tSNR) for each participant was calculated for the first run of the experiment by dividing the mean signal in each voxel by the standard deviation of the residual error time series in that voxel ( ). tSNR values were averaged across the voxels of each ROI. Mean tSNR values, averaged across participants, were as follows: ATL, 76.74; primary auditory cortex (PAC), 93.61; primary visual cortex (PVC), 102.96. The percentage of voxels in each ROI that had \u201cgood\u201d tSNR values (>20;  ) was above 85% for all ROIs: ATL, 86.17%; PAC, 99.87%; PVC, 94.58%. These values indicate that, although mean tSNR was lower in anterior temporal cortex than in sensory regions, the tSNR was sufficient to detect reliable fMRI activation in all ROIs ( ). Moreover, to determine whether tSNR was sufficient in each sub-region of the ATL (as signal drop out is most prominent in ventral anterior regions), the tSNR was calculated for the following regions: aSTG, 85.97; aMTG, 89.00; aITG, 69.79; anterior fusiform gyrus, 69.74; anterior parahippocampal gyrus, 67.13; temporal pole, 63.27. These values suggest that, again, although mean tSNR was lower in more ventral anterior regions, it was still sufficient to detect reliable fMRI activation ( ). \n\nFor each voxel in our three ROI masks, we computed a linear support vector machine (LIBSVM; with fixed regularization hyper-parameter C=1) and a 4-fold cross-validation (leave-one-run-out) classification, implemented in custom python scripts using the pyMVPA software package ( ). A support vector machine was chosen as this aims to combat over-fitting by limiting the complexity of the classifier ( ). The classifier was trained on three runs and tested on the independent fourth run; the testing set was then alternated for each of four iterations. Classifiers were trained and tested on individual subject data transformed into MNI standard space. The functional data were first z-scored per voxel within each run. The searchlight analysis was implemented by extracting the z-scored \u03b2-values from spheres (6mm radius) centered on each voxel in the masks. This sized sphere included\u223c1233\u00a0mm voxels ( ). Classification accuracy (proportion of correctly classified trials) for each sphere was assigned to the sphere's central voxel, in order to produce accuracy maps. The resulting accuracy maps were then smoothed with a Gaussian kernel (6mm FWHM). To determine whether accuracy maps were above chance-levels (50%), individual accuracy maps were entered into a higher-level group analysis (mixed effects, FLAME;  ), testing the accuracy values across subjects against chance for each voxel. Voxel inclusion was set at z=2.3 with a cluster significance threshold at FWE   p  <.05. \n\nThe following classification tests were performed: (1)   Semantic feature classifier  : this examined whether patterns of activity conveyed information regarding the meanings of words, by training a classifier to discriminate between words referring to auditory features (e.g. loud) and visual features (e.g., shiny). This classifier was truly format-independent in the sense that it was trained on this semantic distinction using spoken words and tested using written words (and vice versa). The advantage of performing the classification in this manner is only semantic information common to both presentation formats was informative to the classifier (see  A). The results from the two classifications were averaged to produce a single estimate of classification accuracy. (2)   Perceptual classifier  : here a classifier was trained to discriminate between spoken and written non-words and was tested on these two presentation formats for words. In this way only the presentation format that was general to both non-words and words was informative to the classifier (see  B). \n\n\n\n### Resting state fMRI \n  \n#### Participants \n  \nThis analysis was performed ona separate cohort of 42 healthy participants at York Neuroimaging Centre (13 male; mean age 20.31, range 18\u201325 years). Subjects completed a 9\u00a0minute functional connectivity MRI scan during which they were asked to rest in the scanner with their eyes open. Using these data we examined the\u00a0resting-state fMRI (rs-fMRI) connectivity of ATL regions that were informative to the semantic feature (aITG) and perceptual classifiers (aSTG) to investigate whether these regions fell within similar or distinct networks.\u00a0In addition, we investigated the rs-fMRI connectivity of semantic regions within primary sensory cortices that showed significant decoding by the semantic classifiers to examine whether these regions overlap with the connectivity maps of the ATL seeds. \n\n\n#### Acquisition \n  \nAs with the functional experiment, a\u00a0Magnex head-dedicated gradient insert coil was used in conjunction with a birdcage, radio-frequency coil tuned to 127.4\u00a0MHz. For the resting-state data, a gradient-echo EPI sequence was used to collect data from 60 axial slices with an interleaved (bottom-up) acquisition order with the following parameters: TR=3\u00a0s, TE=minimum full, volumes=180, flip angle=90\u00b0, matrix size=64\u00d764, FOV=192\u00d7192\u00a0mm, voxel size=3x3\u00d73\u00a0mm. A minimum full TE was selected to optimise image quality (as opposed to selecting a value less than minimum full which, for instance, would be beneficial for obtaining more slices per TR). Functional images were co-registered onto a T1-weighted anatomical image from each participant (TR=7.8\u00a0s, TE=3\u00a0ms, FOV=290\u00a0mmx290\u00a0mm, matrix size=256\u00a0mm\u00a0x256\u00a0mm, voxel size=1\u00a0mmx1\u00a0mmx1\u00a0mm). \n\n\n#### Pre-processing \n  \nData were preprocessed using the FSL toolbox ( ). Prior to conducting the functional connectivity analysis, the following pre-statistics processing was applied to the resting state data; motion correction using MCFLIRT to safeguard against motion-related spurious correlations ( ,  ,  ,  ); slice-timing correction using Fourier-space time-series phase-shifting; non-brain removal using BET; spatial smoothing using a Gaussian kernel of FWHM 6 mm; grand-mean intensity normalisation of the entire 4D dataset by a single multiplicative factor; high-passtemporalfiltering (Gaussian-weighted least-squares straight line fitting, with sigma=100\u00a0s); Gaussian lowpass temporal filtering, with sigma=2.8\u00a0s. \n\n\n#### Low level analysis \n  \nFor our ATL sites we created two spherical seed ROIs, 6\u00a0mm in diameter,centered on the co-ordinates of the central voxel in the highest performing spheres in our presentation and semantic searchlight analyses; left aSTG [-54 2 -10] and aITG [-50 -10 -26] respectively (see  ). For our sensory semantic regions we created two spherical seed ROIS centered on intracalcarine cortex [-18 -84 4] and planum polare [-48 -12 -4] from the best performing spheres in our semantic searchlight analysis; as these regions showed high performance accuracy on the semantic classifier   and   fall within primary sensory regions. \n\nThe time series of these regions were extracted and used as explanatory variables in a separate subject level functional connectivity analysis for each seed. Subject specific nuisance regressors were determined using a component based noise correction (CompCor) approach ( ). This method applies principal component analysis (PCA) to the fMRI signal from subject specific white matter and CSF ROIS. In total there were 11 nuisance regressors, five regressors from the CompCorr and a further 6 nuisance regressors were identified using the motion correction MCFLIRT. These principle components are then removed from the fMRI data through linear regression. The WM and CSF covariates were generated by segmenting each individual's high-resolution structural image (using FAST in FSL;  ). The default tissue probability maps, referred to as Prior Probability Maps (PPM), were registered to each individual's high-resolution structural image (T1 space) and the overlap between these PPM and the corresponding CSF and WM maps was identified. These maps were then thresholded (40% for the SCF and 66% for the WM), binarized and combined. The six motion parameters were calculated in the motion-correction step during pre-processing. Movement in each of the three Cartesian directions (x, y, z) and rotational movement around three axes (pitch, yaw, roll) were included for each individual. \n\n\n#### High level analysis \n  \nAt the group-level the data were processed using FEAT version 5.98 part of FSL (FMRIB's Software Library, ) and the analyses were carried out using FMRIB's Local Analysis of Mixed Effects (FLAME) stage 1 with automatic outlier detection. The z statistic images were then thresholded using clusters determined by z > 2.3 and a (corrected) cluster significance threshold of   p   = 0.05 ( ). No global signal regression was performed. \n\nTo investigate the differences between the connectivity maps a fixed effect design (FLAME,  ) was conducted for each participant to investigate four contrasts; (i) aSTG>aITG seed, (ii) aITG>aSTG seed, (iii) auditory semantic>visual semantic seed and (iv) visual semantic>auditory semantic seed. Individual participant data were then entered into a higher-level group analysis using a mixed effects design (FLAME,  ) whole-brain analysis. Finally, to determine whether our ATL seeds connectivity maps overlap with the connectivity maps of the sensory semantic seeds we calculated the number of overlapping voxels for our two ATL sites and the sensory semantic connectivity maps. \n\n\n\n### Resting state decoder \n  \nTo allow quantitative inferences to be drawn on the functional neural activity identified through our seed based correlational analyses we performed an automated meta-analysis using NeuroSynth ( ;  ). This software computed the spatial correlation between each ATL component mask and every other meta-analytic map (  n  =11406) for each term/concept stored in the database (e.g., semantic, language, memory, sensory). The 15 meta-analytic maps exhibiting the highest positive correlation and negative correlation for each sub-system mask were extracted, and the term corresponding to each of these meta-analyses is shown in  . The font size reflects the size of the correlation (ranging from   r  =0.10 to 0.45 for positive correlations and   r  =\u22120.05 to \u22120.2 for negative correlations, in increments of 0.05). This allows us to quantify the most likely reverse inferences that would be drawn from these functional maps by the larger neuroimaging community. \n\n\n\n## Results \n  \n### Behavioural results \n  \nAccuracy and reaction times (RT) were calculated for each participant (n=19) for the catch trials in each experimental condition. Results showed that all participants paid attention to the words as indicated by a mean accuracy above 80% for all experimental conditions (spoken AUD = 80.63% \u00b1 15.33, spoken VIS = 88.12% \u00b1 4.86, spoken NON=85.62%\u00b111.47, written AUD=83.12%\u00b119.01, written VIS=86.25%\u00b113.52, written NON=88.75%\u00b15.45). A chi-square test of independence revealed that accuracy did not significantly differ across the six experimental conditions ( (5)=6.09, p=.303) or across spoken and written input ( (1)=.301, ns). RTs differed significantly between modality-input (  t  (59)=7.36, p<.001), but not semantic-category within each modality (spoken: F(2,38)=.92, ns; written: F(2,38)=0.074, ns). In line with previous findings ( ,  ), participants were significantly faster at responding to written than spoken stimuli. Furthermore, there was no difference in RT between AUD, VIS and NON items within each presentation modality, suggesting that the experimental conditions were well matched at the behavioural level within our stimuli subset. \n\n\n### Searchlight analysis \n  \n#### Semantic feature classifier \n  \nThe format-independent searchlight classifier, trained on the distinction between visual and auditory features in one presentation modality and tested on this distinction in the other modality, was run in three separate masks (ATL; primary auditory cortex and primary visual cortex). All results reported are above chance levels (50%, cluster corrected   p  <.05). The searchlight analysis within the ATL mask revealed a left hemisphere cluster that could decode semantic information across modalities in aMTG and aITG (see  ,  ). Additionally, right hemisphere clusters were revealed in anterior parahippocampal gyrus and temporal pole (TP). The searchlight analysis within the primary auditory mask revealed a cluster in planum polare (see  ,  ). Finally, the primary visual cortex mask revealed a cluster in intracalcarine cortex that could decode semantic content (see  ,  ). \n\n\n#### Perceptual classifier \n  \nThe classifier that was trained on the distinction between spoken and written non-words and tested on the distinction between these presentation modalities for words, was also run in three separate masks (ATL; primary auditory cortex and primary visual cortex). All results reported are above chance levels (50%, cluster corrected   p  <.05). Within the ATL, anterior portions of STG, extending into temporal pole, were able to decode between presentation formats (see  ;  ). The classifier results for the primary auditory cortex mask revealed an extensive cluster of voxels that could classify perceptual information in Heschl's Gyrus, planum temporale and superior temporal gyrus (see  ;  ). The classifier results for the primary visual cortex mask revealed an extensive cluster of voxels in occipital pole (see  ;  ). \n\nTo explicitly determine whether the aITG and aSTG were differentially able to classify the modality of presentation and the meaning of the stimulus, we conducted a 2\u00d72 repeated-measures ANOVA in which we compared the prediction accuracies for each classifier output for each significant cluster. This revealed three significant effects. First, a main effect for classifier type (presentation format vs. semantic classifier; F(1,18)=36.76,   p  <.001). Second, a significant main effect of region (aSTG vs. aITG; F(1,18)=79.71,   p  <.001). Critically, we also found a significant interaction between classifier type and ATL region (F(1,18)=1087.51,   p  <.001). Post-hoc tests revealed a significant difference between aSTG and aITG for the presentation format classifier, with aSTG performing significantly better than aITG (  t  (18)=29.04,   p  <.001). There was also a significant difference between aITG and aSTG for the semantic feature classifier, with aITG performing significantly better than aSTG (  t  (18)=28.30,   p  <.001). Collectively, these analyses show a dissociation between ATL regions: aSTG classification accuracy was higher for presentation modality than word meaning, while the reverse pattern was obtained for aITG. \n\nIn addition to our ROI-based MVPA results, a whole-brain searchlight analysis was computed for both the semantic feature classifier and perceptual classifier, using the same analysis pipeline outlined for our ROI analysis. Results from the whole-brain searchlight reveal similar clusters across primary auditory cortex, primary visual cortex and anterior temporal lobe. In addition, the whole-brain analysis revealed clusters in occipital-parietal cortex and clusters extending along the temporal lobe. The unthresholded maps from the whole-brain searchlight analysis have been uploaded to the neurovault database and can be found here  .\u201d \n\n\n\n### Univariate analysis \n  \nThe searchlight results revealed that in ATL, primary auditory cortex and visual cortex, distinct regions were able to decode semantic feature type and presentation modality. As an additional complementary analysis, the percentage signal change was extracted for each condition from the pairs of clusters that were able to decode semantic feature type and modality of presentation in ATL, visual cortex and auditory cortex (generating six analyses; see  ). A 6mm sphere was centered at the peak MVPA accuracy in each of these sites (see  ). The ventral ATL region (encompassing aITG and aMTG, decoding feature type) showed deactivation across all four conditions, and the degree of deactivation was sensitive to meaning (auditory > visual features) but not input modality (spoken=written words). In contrast, aSTG (which decoded presentation modality) was sensitive to modality (spoken>written) but not meaning (auditory=visual features). Thus, univariate analyses also revealed a functional dissociation within ATL. We also examined regions that could decode modality of presentation and semantic feature type within primary auditory cortex (planum temporale and planum polare respectively) and primary visual cortex (occipital pole and intracalcarine cortex). All four sites showed strong effects of input modality in univariate analyses across both feature types. In addition, the intracalcarine cortex showed greater activity to words that denoted a visual property (e.g., bright) whereas planum polare showed greater activation to words that denoted an auditory property (e.g., loud). This effect of meaning in primary visual and auditory areas was only seen when the words were presented in the complementary input modality: primary visual cortex responded more to visual features when written words were presented, while primary auditory cortex responded more to auditory features when spoken words were presented. Thus, aITG was unique in showing a pattern across both multivariate and univariate analyses consistent with the predictions for an amodal \u2018hub\u2019: i.e., sensitivity to meaning and insensitivity to presentation modality. \n\n\n### Resting state fMRI \n  \nTo provide a better understanding of the neural architecture that supported the functional distinction between aSTG (effect of input modality) and aITG (effect of semantic feature type), we explored the connectivity of these regions in resting state fMRI (see  ) by placing spherical ROIs at peaks in the MVPA analysis. The aSTG seed showed significant positive connectivity across the entire length of STG through primary auditory cortex and into supramarginal gyrus (SMG). It coupled with posterior and anterior regions of MTG, pre- and postcentral gyrus, supplementary motor cortex and anterior cingulate gyrus and deactivation with visual regions, including lateral occipital cortex, intracalcarine cortex, occipital fusiform gyrus (OFG) and temporal occipital fusiform gyrus, as well as posterior cingulate and precuneous. In contrast, the aITG site showed connectivity with core parts of the default mode network and multimodal semantic regions, including angular gyrus, posterior parts of MTG and ITG, temporal pole extending medially to include hippocampus and anterior parahippocampal gyrus, and anterior and inferior prefrontal regions, including orbital cortex and left inferior frontal gyrus (LIFG). This seed also coupled with lateral visual regions (e.g., LOC and occipital fusiform gyrus).   presents location and size of each of these clusters. \n\nTo investigate the differences between these two ATL maps a difference analysis was performed ( B). The contrast of aSTG > aITG identified bilateral superior temporal and frontal polar regions. The contrast aITG > aSTG revealed bilateral inferior and middle portions of the temporal lobe and multimodal semantic sites including angular gyrus, pMTG and LIFG. These differences resemble resting state differences for aSTG and vATL reported by  , helping to validate the functional dissociation we observed using MVPA. \n\nTo further interrogate the assumption that aITG exhibits a connectivity profile consistent with an amodal region, whereas aSTG is connected to sensory regions, we looked at the similarity between our two ATL difference maps (see  B and C) and that of four core networks taken from  . These included two networks sensitive to sensory input (visual, somatosensory) and two networks thought to be crucial in the generation of cognitive states that do not rely on sensory inputs for their mental content (limbic and default mode network) (for a review see  ). The results, outlined in  B and  C, indicated substantial overlap between the sensory networks (namely somatosensory) and aSTG. In contrast, aITG showed substantial overlap with limbic and DMN networks. \n\n\n\n## Discussion \n  \nThe current study used multiple imaging methods to identify regions in the anterior temporal lobe (ATL) and primary sensory regions that showed the pattern expected for the semantic hub of the hub and spokes model ( ). In an fMRI study, participants listened to or viewed words that referred to either visual or auditory features (e.g.,   or  ). Multivoxel pattern analysis (MVPA) revealed a dissociation between (i) anterior inferior temporal gyrus (aITG), which could classify semantic categories relating to feature type (e.g., auditory features like \u201cloud\u201d as being different from visual features like \u201cbright\u201d) across auditory and visual inputs and (ii) anterior superior temporal gyrus (aSTG), which was sensitive to input modality across meaningful and meaningless items. This dissociation within ATL was further supported by univariate contrasts and patterns of resting state connectivity: aSTG showed a stronger response to spoken than written inputs and was functionally coupled to an auditory-motor network (somatosensory network;  ), while aITG was insensitive to input modality and showed substantial connectivity with regions in the default mode network and limbic network, plus some overlap with visual regions (see  , for similar findings). \n\nOur findings make an important contribution to our understanding of the neural basis of semantic cognition in three ways: (1) We provide evidence that conceptual knowledge, extracted from different modalities of input across many learning experiences, is represented within   ventral   portions of ATL which act as a \u2018hub\u2019 ( ,  ). (2) Across converging methods, we observe a functional dissociation between ventral and superior portions of ATL and provide evidence that these regions are situated within distinct large-scale cortical networks. (3) Responses in primary visual and auditory cortex confirm the contribution of these \u2018spoke\u2019 regions to semantic processing. \n\nAccording to the hub and spoke model ( ), conceptual knowledge depends on the co-activation of spoke regions that convey information about specific unimodal and multimodal features of concepts, and an ATL hub which integrates these features to form amodal conceptual representations that are independent of specific sensory input. Studies of patients with semantic dementia (SD) provided the original motivation for this proposal yet neuropsychological methods are not especially well-suited to the precise localization of amodal conceptual representations given the widespread atrophy in this condition. Nevertheless, the degree of semantic impairment correlates with hypometabolism in ventral rather than superior portions of ATL across patients ( ), suggesting that ventral ATL could be the critical substrate for amodal knowledge. Relevant evidence is also provided by univariate fMRI analyses of the ATL response to verbal comprehension tasks in healthy participants, which show multiple peak responses in both ventral ATL and aSTG, often to the same contrasts ( ,  ;  ). Semantic matching and naming tasks have also shown multiple peak responses in the ATL with the more superior ATL region being involved in object naming and the more ventral region in semantic matching ( ). Furthermore, the differential patterns of functional connectivity across ATL regions have been observed by both   and  . \n\nOur findings therefore add to existing knowledge by showing a dissociable response in these two regions: only the ventral ATL site showed a pattern consistent with the representation of conceptual information, since it was able to classify responses according to semantic category (i.e., feature type, not input modality). In univariate analyses, this aITG site also showed deactivation (arguably due to the use of rest rather than an active baseline;  ;  ) for both auditory and visual feature types, irrespective of whether these words were spoken or written \u2013 and the magnitude of this deactivation was greater for visual than auditory features. Finally, this site showed stronger functional connectivity at rest with the default mode and limbic systems, as expected for a region implicated in amodal conceptual processing. Therefore, our combination of functional and resting state methods provides novel converging evidence that anterior ventral temporal areas allow different sensory representations to be integrated to form \u2018amodal\u2019 conceptual representations (particularly for auditory features, see limitations below). \n\nPrevious studies have used MVPA to explore the neural basis of semantic processing, and have identified a conceptual response in ATL using classification of stimuli within a single presentation modality ( ,  ). Other studies, examining semantic cognition across modalities of presentation ( ,  ,  ), have largely not observed effects in ATL. An exception is a recent crossmodal MVPA study, investigating Dutch-English bilinguals ( ). The research tested whether patterns of activity related to the distinction between spoken nouns in one language (e.g., \u201chorse\u201d vs. \u201cduck\u201d in English) could accurately predict the same distinction in the other language (e.g., \u201cpaard\u201d vs. \u201ceend\u201d in Dutch). Consistent with our findings, the cross-language classifier revealed a significant cluster in the left ATL. This largely fell within mid-superior temporal pole rather than the more ventral region we identified in our analysis, perhaps because aSTG is an important interface between semantic processing and other aspects of language. \n\nAnalyses of resting state connectivity from the ATL regions that were able to classify input modality (aSTG) and semantic feature type (aITG) revealed that these two sites lie within distinct large-scale functional networks. A similar dissociation between the resting state connectivity of ventral ATL and anterior STG was recently reported by  , providing further evidence for the validity of the functional dissociation in ATL that we observed using MVPA. To quantify the interpretation of the functional connectivity of the aSTG and aITG connectivity maps, we performed a decoding analysis using automated fMRI meta-analytic software NeuroSynth (see  ). Meta-analytic decoding of these spatial maps revealed that our aSTG connectivity map correlated with terms related to language (e.g., sentence, comprehension) and auditory processing (e.g., speech, sound) whilst anti-correlating with other modality information (e.g., visual, spatial) and memory (e.g., working memory, episodic). In contrast, the aITG connectivity map correlated with terms related to memory (e.g., semantic, autobiographical) and social processes (e.g., theory of mind, social cognition) terms, whilst anti-correlating with modality-specific (e.g., ventral visual, motor, spatial) and executive terms (e.g., maintenance, demands). This is consistent with previous findings that relate aSTG to speech comprehension, language and sensory processing ( ,  ,  ,  ,  ,  ) and aITG to semantic processing but not sensory experience ( ,  ). Furthermore, the differences in function across temporal areas as revealed by the Neurosynth database seem to align with differences in the white-matter terminations (see  ). These findings confirmed associations between (i) the network anchored in the aSTG and auditory processing and speech perception, plus (ii) the aITG network and more abstract domains (such as social cognition, theory of mind, or mental states). \n\nThus, the putative semantic \u2018hub\u2019 in ventral ATL was functionally coupled to aspects of cortex that specialize in forms of stimulus-independent higher order cognition, including angular gyrus (AG) and posterior and anterior areas on the medial surface that correspond to the midline core of the so-called default mode network (DMN)(see also  ). This network is known to be deactivated by input ( ) and is thought to be crucial in the generation of cognitive states that do not rely on sensory information for their mental content (for a review see  ). Tasks which are associated with the default mode network include those that depend on episodic memory, semantic processing, mental state attribution as well as states of spontaneous thought studied under the rubric of mind-wandering / daydreaming ( ,  ). Although previous literature has shown that connectivity to the AG may not be due to shared semantic processing ( ). Therefore, as many cognitive states that involve the DMN are stimulus-independent in nature, their association with ventral ATL both in terms of functional connectivity and their meta-analytic decoding is consistent with the view that this region supports semantic processing across different input modalities and may form conceptual representations that are not tied to a specific input modality (see  ). In contrast, aSTG showed greater functional connectivity with auditory and motor regions and this spatial map was associated with auditory processing and language tasks, as opposed to amodal tasks, in the meta-analytic decoding. Therefore, our combination of functional and resting state methods provides novel converging evidence that anterior ventral temporal areas allow different sensory representations to be integrated to form \u2018amodal\u2019 conceptual representations. \n\nAs discussed, the hub and spoke model ( ,  ,  ) makes novel predictions about the contribution of the ATL to amodal conceptual knowledge, but it also anticipates an important role for modality-specific \u2018spoke\u2019 regions in visual and auditory cortex, in line with many influential accounts of semantic processing ( ,  ,  ,  ). Furthermore, the involvement of both hub and spoke regions in semantic representations has been shown using TMS ( ). In line with this view, MVPA revealed regions that responded to meaning in both ventral parts of ATL (putative \u2018hub\u2019) and in primary visual and auditory regions (putative \u2018spokes\u2019). In addition, even though the putative \u2018spoke\u2019 regions (i.e., voxels sensitive to meaning) were adjacent to areas that coded for input modality, the specific voxels that could classify meaning and input modality were largely different. These findings do not readily support traditional \u2018strong\u2019 embodied accounts that equate semantic representations with traces of perceptual/motor experience (for a review, see  )) since this would suggest a greater degree of overlap between the results of these two classifiers. While our data suggests that sensory systems appear to play a critical role in the representation of meaning, they also suggest that perceptual experience and imagery generated as part of semantic retrieval may be distinguishable on the basis of differences in the patterns of activity in sensory cortex. \n\nOne potential limitation of our study is that we did not observe evidence that aITG responds to both auditory and visual semantic features in the univariate contrasts: this site showed deactivation for both feature types that was greater for visual features. Thus, the strongest evidence for the aITG as an amodal hub is provided by the MVPA results and our meta-analytic decoding of this region's pattern of distinct functional connectivity, and not the univariate analyses. Our design was optimized for decoding rather than univariate effects \u2013 as we focused on obtaining the maximum number of blocks for MVPA and did not employ a high-level non-semantic baseline which would have allowed us to recover semantic activation in ATL for both auditory and visual features from a contrast ( ). Since we found that aITG responds more to auditory features (words such as \u201cloud\u201d) than visual features (words such as \u201cbright\u201d), it remains unclear whether aITG reflects the meanings of auditory features alone, or both feature types equally. Future studies might allow these possibilities to be disentangled using a high-level baseline with which both feature types can be compared (e.g.  ). \n\n\n## Conclusion \n  \nCollectively, our findings from both pattern classification and resting-state connectivity provide converging evidence that sub-regions of the ATL support different aspects of semantic processing. Anterior ITG and MTG capture meaning independent of input modality, consistent with the fact that semantic dementia patients (who have multimodal semantic impairment) have considerable atrophy in this same region of ATL ( ,  ). In contrast, aSTG exhibited a degree of modality specificity: this structure, which is known to be important for understanding speech and environmental sounds, does not fulfil the criteria for an amodal semantic hub. Finally, the current results provide evidence for modality-specific spokes regions within the vicinity of primary auditory and visual cortex (intracalcarine cortex and planum polare respectively). However, the specific voxels that could classify between each condition (presentation format and semantic feature) were largely different. These findings challenge traditional embodied accounts ( ) that attempt to equate semantic representations with traces of perceptual/motor experience, and instead support the view that the richness of semantic cognition arises at least in part from abstraction away from specific input modalities in ventral regions of the anterior temporal lobe. \n\n\n## Funding \n  \nThe research was supported by   grant BB/J006963/1. Jefferies was supported by a grant from the   (SEMBIND - 283530). The publication was part-funded by a grant from the  , \u201cProspective Psychology Stage 2: A Research Competition\u201d to Martin Seligman. The opinions expressed in this publication are those of the author(s) and do not necessarily reflect the views of the John Templeton Foundation. The authors declare no competing financial interests. \n\n \n", "metadata": {"pmcid": 5315053, "text_md5": "6e478b9d757d545e61ffa2404708add1", "field_positions": {"authors": [0, 146], "journal": [147, 157], "publication_year": [159, 163], "title": [174, 284], "keywords": [298, 409], "abstract": [422, 2101], "body": [2110, 52502]}, "batch": 1, "pmid": 27908787, "doi": "10.1016/j.neuroimage.2016.11.067", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5315053", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=5315053"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5315053\">5315053</a>", "list_title": "PMC5315053  Fractionating the anterior temporal lobe: MVPA reveals differential responses to input and conceptual modality"}
